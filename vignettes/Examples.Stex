\documentclass[nojss]{jss}
%% need no \usepackage{Sweave.sty}
\usepackage{amsmath}

%\VignetteIndexEntry{LaplacesDemon Examples}
%\VignettePackage{LaplacesDemon}
%\VignetteDepends{LaplacesDemon}

\author{Statisticat, LLC}
\title{\includegraphics[height=1in,keepaspectratio]{LDlogo} \\ \pkg{LaplacesDemon} Examples}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Statisticat LLC} %% comma-separated
\Plaintitle{LaplacesDemon Examples} %% without formatting
\Shorttitle{Examples} %% a short title (if necessary)

\Abstract{The \pkg{LaplacesDemon} package is a complete environment for Bayesian inference within \proglang{R}. Virtually any probability model may be specified. This vignette is a compendium of examples of how to specify different model forms.
}
\Keywords{Bayesian, LaplacesDemon, R}
\Plainkeywords{bayesian, laplacesdemon, r}

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{13}
%% \Issue{9}
%% \Month{September}
%% \Year{2011}
%% \Submitdate{2011-01-18}
%% \Acceptdate{2011-01-18}

\Address{
  Statisticat, LLC\\
  Farmington, CT\\
  E-mail: \email{software@bayesian-inference.com}\\
  URL: \url{http://www.bayesian-inference.com/software}
}

%% for those who use Sweave please include the following line (with % symbols):
%% need no \usepackage{Sweave.sty}

\begin{document}
\pkg{LaplacesDemon} \citep{r:laplacesdemon}, often referred to as LD, is an \proglang{R} package that is available at \url{http://www.bayesian-inference.com/software}. A formal introduction to \pkg{LaplacesDemon} is provided in an accompanying vignette entitled ``\pkg{LaplacesDemon} Tutorial'', and an introduction to Bayesian inference is provided in the ``Bayesian Inference'' vignette.

The purpose of this document is to provide users of the \pkg{LaplacesDemon} package with examples of a variety of Bayesian methods. It is also a testament to the diverse applicability of \pkg{LaplacesDemon} to Bayesian inference.

To conserve space, the examples are not worked out in detail, and only the minimum of necessary materials is provided for using the various methodologies. Necessary materials include the form expressed in notation, data (which is often simulated), the \code{Model} function, and initial values. The provided data, model specification, and initial values may be copy/pasted into an \proglang{R} file and updated with the \code{LaplacesDemon} or (usually) \code{LaplaceApproximation} functions. Although many of these examples update quickly, some examples are computationally intensive.

All examples are provided in R code, but the model specification function can be in another language. A goal is to provide these example model functions in C++ as well, and some are now available at \url{www.bayesian-inference.com/cpp/LaplacesDemonExamples.txt}.

Initial values are usually hard-coded in the examples, though the Parameter-Generating Function (PGF) is also specified. It is recommended to generate initial values with the \code{GIV} function according to the user-specified \code{PGF}.

Notation in this vignette follows these standards: Greek letters represent parameters, lower case letters represent indices, lower case bold face letters represent scalars or vectors, probability distributions are represented with calligraphic font, upper case letters represent index limits, and upper case bold face letters represent matrices. More information on notation is available at \url{http://www.bayesian-inference.com/notation}.

This vignette will grow over time as examples of more methods become included. Contributed examples are welcome. Please send contributed examples or discovered errors in a similar format in an email to \email{software@bayesian-inference.com} for review and testing. All accepted contributions are, of course, credited.

\begin{center} \Large{\textbf{Contents}} \end{center}
\begin{itemize}
\item ANCOVA \ref{ancova}
\item ANOVA, One-Way \ref{anova.one.way}
\item ANOVA, Two-Way \ref{anova.two.way}
\item Approximate Bayesian Computation (ABC) \ref{abc}
\item ARCH-M(1,1) \ref{archm}
\item Autoregression, AR(1) \ref{ar1}
\item Autoregressive Conditional Heteroskedasticity, ARCH(1,1) \ref{arch11}
\item Autoregressive Moving Average, ARMA(1,1) \ref{arma11}
\item Beta Regression \ref{beta.reg}
\item Beta-Binomial \ref{beta.binomial}
\item Binary Logit \ref{binary.logit}
\item Binary Log-Log Link Mixture \ref{binary.loglog.mixture}
\item Binary Probit \ref{binary.probit}
\item Binary Robit \ref{binary.robit}
\item Binomial Logit \ref{binomial.logit}
\item Binomial Probit \ref{binomial.probit}
\item Binomial Robit \ref{binomial.robit}
\item Change Point Regression \ref{changepoint}
\item Cluster Analysis, Confirmatory (CCA) \ref{cca}
\item Cluster Analysis, Exploratory (ECA) \ref{eca}
\item Collaborative Filtering (CF) \ref{eofa}
\item Conditional Autoregression (CAR), Poisson \ref{car.poisson}
\item Conditional Predictive Ordinate (CPO) \ref{cpo}
\item Contingency Table \ref{contingency.table}
\item Covariance Separation Strategy \ref{cov.sep.strat}
\item Discrete Choice, Conditional Logit \ref{conditional.logit}
\item Discrete Choice, Mixed Logit \ref{mixed.logit}
\item Discrete Choice, Multinomial Probit \ref{dc.mnp}
\item Distributed Lag, Koyck \ref{dl.koyck}
\item Dynamic Linear Model (DLM) \ref{dfa} \ref{ssm.lin.reg} \ref{ssm.ll} \ref{ssm.llt}
\item Exponential Smoothing \ref{exp.smo}
\item Factor Analysis, Approximate Dynamic (ADFA) \ref{adfa}
\item Factor Analysis, Confirmatory (CFA) \ref{cfa}
\item Factor Analysis, Dynamic (DFA) \ref{dfa}
\item Factor Analysis, Exploratory (EFA) \ref{efa}
\item Factor Analysis, Exploratory Ordinal (EOFA) \ref{eofa}
\item Factor Regression \ref{factor.reg}
\item Gamma Regression \ref{gamma.reg}
\item GARCH(1,1) \ref{garch}
\item GARCH-M(1,1) \ref{garchm}
\item Geographically Weighted Regression \ref{gwr}
\item Hidden Markov Model \ref{hmm}
\item Hierarchical Bayes \ref{linear.reg.hb}
\item Horseshoe Regression \ref{horseshoe}
\item Inverse Gaussian Regression \ref{ig.reg}
\item Kriging \ref{kriging}
\item Kriging, Predictive Process \ref{kriging.pp}
\item Laplace Regression \ref{laplace.reg}
\item LASSO \ref{bal}
\item Latent Dirichlet Allocation (LDA) \ref{lda}
\item Linear Regression \ref{linear.reg}
\item Linear Regression, Frequentist \ref{linear.reg.freq}
\item Linear Regression, Hierarchical Bayesian \ref{linear.reg.hb}
\item Linear Regression, Multilevel \ref{linear.reg.ml}
\item Linear Regression with Full Missingness \ref{linear.reg.full.miss}
\item Linear Regression with Missing Response \ref{linear.reg.miss.resp}
\item Linear Regression with Missing Response via ABB \ref{linear.reg.miss.resp.abb}
\item Linear Regression with Power Priors \ref{linear.reg.pp}
\item Linear Regression with Zellner's g-Prior \ref{linear.reg.g}
\item LSTAR \ref{lstar}
\item MANCOVA \ref{mancova}
\item MANOVA \ref{manova}
\item Missing Values \ref{linear.reg.full.miss} \ref{linear.reg.miss.resp} \ref{linear.reg.miss.resp.abb}
\item Mixture Model, Dirichlet Process \ref{eca}
\item Mixture Model, Finite \ref{cca} \ref{fmm}
\item Mixture Model, Infinite \ref{eca} \ref{imm}
\item Mixture Model, Poisson-Gamma \ref{poisson.gamma}
\item Model Averaging \ref{ssvs} \ref{rj}
\item Multilevel Model \ref{linear.reg.ml}
\item Multinomial Logit \ref{mnl}
\item Multinomial Logit, Nested \ref{nmnl}
\item Multinomial Probit \ref{mnp}
\item Multiple Discrete-Continuous Choice \ref{mdcc}
\item Multivariate Binary Probit \ref{multiv.bin.probit}
\item Multivariate Laplace Regression \ref{multivariate.lap.reg}
\item Multivariate Regression \ref{multivariate.reg}
\item Negative Binomial Regression \ref{negbin.reg}
\item Normal, Multilevel \ref{norm.ml}
\item Ordinal Logit \ref{ordinal.logit}
\item Ordinal Probit \ref{ordinal.probit}
\item Panel, Autoregressive Poisson \ref{panel.ap}
\item Penalized Spline Regression \ref{pspline}
\item Poisson Regression \ref{poisson.reg}
\item Poisson Regression, Overdispersed \ref{poisson.gamma} \ref{negbin.reg}
\item Poisson-Gamma Regression \ref{poisson.gamma}
\item Polynomial Regression \ref{polynomial.reg}
\item Power Priors \ref{linear.reg.pp}
\item Proportional Hazards Regression, Weibull \ref{prop.haz.weib}
\item Quantile Regression \ref{quantile.reg}
\item Revision, Normal \ref{revision.normal}
\item Ridge Regression \ref{ridge.reg}
\item Robust Regression \ref{robust.reg}
\item Seemingly Unrelated Regression (SUR) \ref{sur}
\item Simultaneous Equations \ref{simultaneous}
\item Space-Time, Dynamic \ref{spacetime.dynamic}
\item Space-Time, Nonseparable \ref{spacetime.nonsep}
\item Space-Time, Separable \ref{spacetime.sep}
\item Spatial Autoregression (SAR) \ref{sar}
\item STARMA(1,1) \ref{starma}
\item State Space Model (SSM), Dynamic Factor Analysis (DFA) \ref{dfa}
\item State Space Model (SSM), Linear Regression \ref{ssm.lin.reg}
\item State Space Model (SSM), Local Level \ref{ssm.ll}
\item State Space Model (SSM), Local Linear Trend \ref{ssm.llt}
\item State Space Model (SSM), Stochastic Volatility (SV) \ref{sv}
\item Stochastic Volatility (SV) \ref{sv}
\item Survival Model \ref{prop.haz.weib}
\item T-test \ref{anova.one.way}
\item TARCH(1) \ref{tarch}
\item Threshold Autoregression (TAR) \ref{tar}
\item Topic Model \ref{lda}
\item Time Varying AR(1) with Chebyshev Series \ref{tvarcs}
\item Variable Selection, BAL \ref{bal}
\item Variable Selection, Horseshoe \ref{horseshoe}
\item Variable Selection, RJ \ref{rj}
\item Variable Selection, SSVS \ref{ssvs}
\item Variety Model \ref{mdcc}
\item Vector Autoregression, VAR(1) \ref{var1}
\item Weighted Regression \ref{weighted.reg}
\item Zellner's g-Prior \ref{linear.reg.g}
\item Zero-Inflated Poisson (ZIP) \ref{zip}
\end{itemize}

\section{ANCOVA} \label{ancova}
This example is essentially the same as the two-way ANOVA (see section \ref{anova.two.way}), except that a covariate $\textbf{X}_{,3}$ has been added, and its parameter is $\delta$.
\subsection{Form}
$$\textbf{y}_i \sim \mathcal{N}(\mu_i, \sigma^2_1)$$
$$\mu_i = \alpha + \beta[\textbf{X}_{i,1}] + \gamma[\textbf{X}_{i,2}] + \delta \textbf{X}_{i,2}, \quad i=1,\dots,N$$
$$\epsilon_i = \textbf{y}_i - \mu_i$$
$$\alpha \sim \mathcal{N}(0, 1000)$$
$$\beta_j \sim \mathcal{N}(0, \sigma^2_2), \quad j=1,\dots,J$$
$$\beta_J = - \sum^{J-1}_{j=1} \beta_j$$
$$\gamma_k \sim \mathcal{N}(0, \sigma^2_3), \quad k=1,\dots,K$$
$$\gamma_K = - \sum^{K-1}_{k=1} \gamma_k$$
$$\delta \sim \mathcal{N}(0, 1000)$$
$$\sigma_m \sim \mathcal{HC}(25), \quad m=1,\dots,3$$
\subsection{Data}
\code{N <- 100 \\
J <- 5 \#Number of levels in factor (treatment) 1 \\
K <- 3 \#Number of levels in factor (treatment) 2 \\
X <- cbind(rcat(N,rep(1/J,J)), rcat(N,rep(1/K,K)), runif(N,-2,2)) \\
alpha <- runif(1,-1,1) \\
beta <- runif(J-1,-2,2) \\
beta <- c(beta, -sum(beta)) \\
gamma <- runif(K-1,-2,2) \\
gamma <- c(gamma, -sum(gamma)) \\
delta <- runif(1,-2,2) \\
y <- alpha + beta[X[,1]] + gamma[X[,2]] + delta*X[,3] + rnorm(N,0,0.1) \\
mon.names <- c("LP","beta[5]","gamma[3]","s.beta","s.gamma","s.epsilon") \\
parm.names <- as.parm.names(list(alpha=0, beta=rep(0,J-1), gamma=rep(0,K-1), \\
\hspace*{0.27 in} delta=0, sigma=rep(0,3))) \\
pos.alpha <- grep("alpha", parm.names) \\
pos.beta <- grep("beta", parm.names) \\
pos.gamma <- grep("gamma", parm.names) \\
pos.delta <- grep("delta", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
PGF <- function(Data) return(c(rnorm(1,0,1), rnorm(Data$J-1,0,1), \\
\hspace*{0.27 in} rnorm(Data$K-1,0,1), rnorm(1,0,1), rhalfcauchy(3,5))) \\
MyData <- list(J=J, K=K, N=N, PGF=PGF, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.alpha=pos.alpha, \\
\hspace*{0.27 in} pos.beta=pos.beta, pos.gamma=pos.gamma, pos.delta=pos.delta, \\
\hspace*{0.27 in} pos.sigma=pos.sigma, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[Data$pos.alpha] \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} beta <- c(beta, -sum(beta)) \#Sum-to-zero constraint \\
\hspace*{0.27 in} gamma <- parm[Data$pos.gamma] \\
\hspace*{0.27 in} gamma <- c(gamma, -sum(gamma)) \#Sum-to-zero constraint \\
\hspace*{0.27 in} delta <- parm[Data$pos.delta] \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnormv(alpha, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, sigma[2], log=TRUE)) \\
\hspace*{0.27 in} gamma.prior <- sum(dnorm(gamma, 0, sigma[3], log=TRUE)) \\
\hspace*{0.27 in} delta.prior <- dnormv(delta, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- alpha + beta[Data$X[,1]] + gamma[Data$X[,2]] + \\
\hspace*{0.62 in} delta*Data$X[,3] \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma[1], log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Variance Components \\
\hspace*{0.27 in} s.beta <- sd(beta) \\
\hspace*{0.27 in} s.gamma <- sd(gamma) \\
\hspace*{0.27 in} s.epsilon <- sd(Data$y - mu) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior + gamma.prior + delta.prior + \\
\hspace*{0.62 in} sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, beta[Data$J], \\
\hspace*{0.62 in} gamma[Data$K], s.beta, s.gamma, s.epsilon), \\
\hspace*{0.62 in} yhat=rnorm(length(mu), mu, sigma[1]), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(0, rep(0,(J-1)), rep(0,(K-1)), 0, rep(1,3))}

\section{ANOVA, One-Way} \label{anova.one.way}
When $J=2$, this is a Bayesian form of a t-test.
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2_1)$$
$$\mu_i = \alpha + \beta[\textbf{x}_i], \quad i=1,\dots,N$$
$$\alpha \sim \mathcal{N}(0, 1000)$$
$$\beta_j \sim \mathcal{N}(0, \sigma^2_2), \quad j=1,\dots,J$$
$$\beta_J = - \displaystyle\sum^{J-1}_{j=1} \beta_j$$
$$\sigma_{1:2} \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{N <- 100 \\
J <- 3 \\
x <- rcat(N, rep(1/J, J)) \\
alpha <- runif(1,-1,1) \\
beta <- runif(J-1,-2,2) \\
beta <- c(beta, -sum(beta)) \\
y <- alpha + beta[x] + rnorm(1,0,0.2) \\
mon.names <- c("LP","beta[3]") \\
parm.names <- as.parm.names(list(alpha=0, beta=rep(0,J-1), sigma=rep(0,2))) \\
pos.alpha <- grep("alpha", parm.names) \\
pos.beta <- grep("beta", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
PGF <- function(Data) return(c(rnorm(1,0,1), rnorm(Data$J-1,0,1), \\
\hspace*{0.27 in} rhalfcauchy(2,5))) \\
MyData <- list(J=J, N=N, PGF=PGF, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.alpha=pos.alpha, pos.beta=pos.beta, \\
\hspace*{0.27 in} pos.sigma=pos.sigma, x=x, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[Data$pos.alpha] \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} beta <- c(beta, -sum(beta)) \#Sum-to-zero constraint \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnormv(alpha, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, sigma[2], log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- alpha + beta[Data$x] \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma[1], log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,beta[Data$J]), \\
\hspace*{0.62 in} yhat=rnorm(length(mu), mu, sigma[1]), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(0, rep(0,(J-1)), rep(1,2))}

\section{ANOVA, Two-Way} \label{anova.two.way}
In this representation, $\sigma^m$ are the superpopulation variance components, \code{s.beta} and \code{s.gamma} are the finite-population within-variance components of the factors or treatments, and \code{s.epsilon} is the finite-population between-variance component.
\subsection{Form}
$$\textbf{y}_i \sim \mathcal{N}(\mu_i, \sigma^2_1)$$
$$\mu_i = \alpha + \beta[\textbf{X}_{i,1}] + \gamma[\textbf{X}_{i,2}], \quad i=1,\dots,N$$
$$\epsilon_i = \textbf{y}_i - \mu_i$$
$$\alpha \sim \mathcal{N}(0, 1000)$$
$$\beta_j \sim \mathcal{N}(0, \sigma^2_2), \quad j=1,\dots,J$$
$$\beta_J = - \sum^{J-1}_{j=1} \beta_j$$
$$\gamma_k \sim \mathcal{N}(0, \sigma^2_3), \quad k=1,\dots,K$$
$$\gamma_K = - \sum^{K-1}_{k=1} \gamma_k$$
$$\sigma_m \sim \mathcal{HC}(25), \quad m=1,\dots,3$$
\subsection{Data}
\code{N <- 100 \\
J <- 5 \#Number of levels in factor (treatment) 1 \\
K <- 3 \#Number of levels in factor (treatment) 2 \\
X <- cbind(rcat(N,rep(1/J,J)), rcat(N,rep(1/K,K))) \\
alpha <- runif(1,-1,1) \\
beta <- runif(J-1,-2,2) \\
beta <- -sum(beta) \\
gamma <- runif(K-1,-2,2) \\
gamma <- -sum(gamma) \\
y <- alpha + beta[X[,1]] + gamma[X[,2]] + rnorm(1,0,0.1) \\
mon.names <- c("LP","beta[5]","gamma[3]","s.beta","s.gamma","s.epsilon") \\
parm.names <- as.parm.names(list(alpha=0, beta=rep(0,J-1), gamma=rep(0,K-1), \\
\hspace*{0.27 in} sigma=rep(0,3))) \\
pos.alpha <- grep("alpha", parm.names) \\
pos.beta <- grep("beta", parm.names) \\
pos.gamma <- grep("gamma", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
PGF <- function(Data) return(c(rnorm(1,0,1), rnorm(Data$J-1,0,1), \\
\hspace*{0.27 in} rnorm(Data$K-1,0,1), rhalfcauchy(3,5))) \\
MyData <- list(J=J, K=K, N=N, PGF=PGF, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.alpha=pos.alpha, pos.beta=pos.beta, \\
\hspace*{0.27 in} pos.gamma=pos.gamma, pos.sigma=pos.sigma, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[Data$pos.alpha] \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} beta <- c(beta, -sum(beta)) \#Sum-to-zero constraint \\
\hspace*{0.27 in} gamma <- parm[Data$pos.gamma] \\
\hspace*{0.27 in} gamma <- c(gamma, -sum(gamma)) \#Sum-to-zero constraint \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnormv(alpha, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, sigma[2], log=TRUE)) \\
\hspace*{0.27 in} gamma.prior <- sum(dnorm(gamma, 0, sigma[3], log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- alpha + beta[Data$X[,1]] + gamma[Data$X[,2]] \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma[1], log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Variance Components \\
\hspace*{0.27 in} s.beta <- sd(beta) \\
\hspace*{0.27 in} s.gamma <- sd(gamma) \\
\hspace*{0.27 in} s.epsilon <- sd(Data$y - mu) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior + gamma.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, beta[Data$J], \\
\hspace*{0.62 in} gamma[Data$K], s.beta, s.gamma, s.epsilon), \\
\hspace*{0.62 in} yhat=rnorm(length(mu), mu, sigma[1]), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(0, rep(0,(J-1)), rep(0,(K-1)), rep(1,3))}

\section{Approximate Bayesian Computation (ABC)} \label{abc}
Approximate Bayesian Computation (ABC), also called likelihood-free estimation, is not a statistical method, but a family of numerical approximation techniques in Bayesian inference. ABC is especially useful when evaluation of the likelihood, $p(\textbf{y} | \Theta)$ is computationally prohibitive, or when suitable likelihoods are unavailable. The current example is the application of ABC in the context of linear regression. The log-likelihood is replaced with the negative sum of the distance between $\textbf{y}$ and $\textbf{y}^{rep}$ as the approximation of the log-likelihood. Distance reduces to the absolute difference. Although linear regression has an easily calculated likelihood, it is used as an example due to its generality. This example demonstrates how ABC may be estimated either with MCMC via the \code{LaplacesDemon} function or with Laplace Approximation via the \code{LaplaceApproximation} function. In this method, a tolerance (which is found often in ABC) does not need to be specified, and the logarithm of the unnormalized joint posterior density is maximized, as usual. The negative and summed distance, above, may be replaced with the negative and summed distance between summaries of the data, rather than the data itself, but this has not been desirable in testing.
\subsection{Form}
$$\textbf{y} = \mu + \epsilon$$
$$\mu = \textbf{X} \beta$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
\subsection{Data}
\code{data(demonsnacks) \\
y <- log(demonsnacks$Calories) \\
X <- cbind(1, as.matrix(log(demonsnacks[,c(1,4,10)]+1))) \\
J <- ncol(X) \\
for (j in 2:J) \{X[,j] <- CenterScale(X[,j])\} \\
mon.names <- c("LP","sigma") \\
parm.names <- as.parm.names(list(beta=rep(0,J))) \\
pos.beta <- grep("beta", parm.names) \\
PGF <- function(Data) return(rnormv(Data$J,0,1000)) \\
MyData <- list(J=J, PGF=PGF, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.beta=pos.beta, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood Approximation \\
\hspace*{0.27 in} mu <- as.vector(tcrossprod(Data$X, t(beta))) \\
\hspace*{0.27 in} epsilon <- Data$y - mu \\
\hspace*{0.27 in} sigma <- sd(epsilon) \\
\hspace*{0.27 in} LL <- -sum(abs(epsilon)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior Approximation \\
\hspace*{0.27 in} LP <- LL + beta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,sigma), \\
\hspace*{0.62 in} yhat=rnorm(length(mu), mu, sigma), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J))}

\section{ARCH-M(1,1)} \label{archm}
\subsection{Form}
$$\textbf{y}_t \sim \mathcal{N}(\mu_t, \sigma^2_t), \quad t=1,\dots,T$$
$$\textbf{y}^{new} \sim \mathcal{N}(\mu_{T+1}, \sigma^2_{new})$$
$$\mu_t = \alpha + \phi \textbf{y}_{t-1} + \delta \sigma^2_{t-1}, \quad t=1,\dots,(T+1)$$
$$\epsilon_t = \textbf{y}_t - \mu_t$$
$$\alpha \sim \mathcal{N}(0, 1000)$$
$$\phi \sim \mathcal{N}(0, 1000)$$
$$\delta \sim \mathcal{N}(0, 1000)$$
$$\sigma^2_{new} = \omega + \theta \epsilon^2_T$$
$$\sigma^2_t = \omega + \theta \epsilon^2_{t-1}$$
$$\omega \sim \mathcal{HC}(25)$$
$$\theta \sim \mathcal{U}(0, 1)$$
\subsection{Data}
\code{y <- c(0.02, -0.51, -0.30,  1.46, -1.26, -2.15, -0.91, -0.53, -1.91, \\
\hspace*{0.27 in} 2.64,  1.64,  0.15,  1.46,  1.61,  1.96, -2.67, -0.19, -3.28, \\
\hspace*{0.27 in}  1.89,  0.91, -0.71,  0.74, -0.10,  3.20, -0.80, -5.25,  1.03, \\
\hspace*{0.27 in} -0.40, -1.62, -0.80,  0.77,  0.17, -1.39, -1.28,  0.48, -1.02, \\
\hspace*{0.27 in}  0.09, -1.09,  0.86,  0.36,  1.51, -0.02,  0.47,  0.62, -1.36, \\
\hspace*{0.27 in}  1.12,  0.42, -4.39, -0.87,  0.05, -5.41, -7.38, -1.01, -1.70, \\
\hspace*{0.27 in}  0.64,  1.16,  0.87,  0.28, -1.69, -0.29,  0.13, -0.65,  0.83, \\
\hspace*{0.27 in}  0.62,  0.05, -0.14,  0.01, -0.36, -0.32, -0.80, -0.06,  0.24, \\
\hspace*{0.27 in}  0.23, -0.37,  0.00, -0.33,  0.21, -0.10, -0.10, -0.01, -0.40, \\
\hspace*{0.27 in} -0.35,  0.48, -0.28,  0.08,  0.28,  0.23,  0.27, -0.35, -0.19, \\
\hspace*{0.27 in}  0.24,  0.17, -0.02, -0.23,  0.03,  0.02, -0.17,  0.04, -0.39, \\
\hspace*{0.27 in} -0.12,  0.16,  0.17,  0.00,  0.18,  0.06, -0.36,  0.22,  0.14, \\
\hspace*{0.27 in} -0.17,  0.10, -0.01,  0.00, -0.18, -0.02,  0.07, -0.06,  0.06, \\
\hspace*{0.27 in} -0.05, -0.08, -0.07,  0.01, -0.06,  0.01,  0.01, -0.02,  0.01, \\
\hspace*{0.27 in}  0.01,  0.12, -0.03,  0.08, -0.10,  0.01, -0.03, -0.08,  0.04, \\
\hspace*{0.27 in} -0.09, -0.08,  0.01, -0.05,  0.08, -0.14,  0.06, -0.11,  0.09, \\
\hspace*{0.27 in}  0.06, -0.12, -0.01, -0.05, -0.15, -0.05, -0.03,  0.04,  0.00, \\
\hspace*{0.27 in} -0.12,  0.04, -0.06, -0.05, -0.07, -0.05, -0.14, -0.05, -0.01, \\
\hspace*{0.27 in} -0.12,  0.05,  0.06, -0.10,  0.00,  0.01,  0.00, -0.08,  0.00, \\
\hspace*{0.27 in}  0.00,  0.07, -0.01,  0.00,  0.09,  0.33,  0.13,  0.42,  0.24, \\
\hspace*{0.27 in} -0.36,  0.22, -0.09, -0.19, -0.10, -0.08, -0.07,  0.05,  0.07, \\
\hspace*{0.27 in}  0.07,  0.00, -0.04, -0.05,  0.03,  0.08,  0.26,  0.10,  0.08, \\
\hspace*{0.27 in}  0.09, -0.07, -0.33,  0.17, -0.03,  0.07, -0.04, -0.06, -0.06, \\
\hspace*{0.27 in}  0.07, -0.03,  0.00,  0.08,  0.27,  0.11,  0.11,  0.06, -0.11, \\
\hspace*{0.27 in} -0.09, -0.21,  0.24, -0.12,  0.11, -0.02, -0.03,  0.02, -0.10, \\
\hspace*{0.27 in}  0.00, -0.04,  0.01,  0.02, -0.03, -0.10, -0.09,  0.17,  0.07, \\
\hspace*{0.27 in} -0.05, -0.01, -0.05,  0.01,  0.00, -0.08, -0.05, -0.08,  0.07, \\
\hspace*{0.27 in}  0.06, -0.14,  0.02,  0.01,  0.04,  0.00, -0.13, -0.17) \\
T <- length(y) \\
mon.names <- c("LP", "ynew", "sigma2.new") \\
parm.names <- c("alpha","phi","delta","omega","theta") \\
pos.alpha <- grep("alpha", parm.names) \\
pos.phi <- grep("phi", parm.names) \\
pos.delta <- grep("delta", parm.names) \\
pos.omega <- grep("omega", parm.names) \\
pos.theta <- grep("theta", parm.names) \\
PGF <- function(Data) return(c(rnorm(3,0,1), rhalfcauchy(1,5), runif(1))) \\
MyData <- list(PGF=PGF, T=T, mon.names=mon.names, parm.names=parm.names, \\
\hspace*{0.27 in} pos.alpha=pos.alpha, pos.phi=pos.phi, pos.delta=pos.delta, \\
\hspace*{0.27 in} pos.omega=pos.omega, pos.theta=pos.theta, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[Data$pos.alpha]; phi <- parm[Data$pos.phi] \\
\hspace*{0.27 in} delta <- parm[Data$pos.delta] \\
\hspace*{0.27 in} omega <- interval(parm[Data$pos.omega], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.omega] <- omega \\
\hspace*{0.27 in} theta <- interval(parm[Data$pos.theta], 1e-10, 1-1e-5) \\
\hspace*{0.27 in} parm[Data$pos.theta] <- theta \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnormv(alpha, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} phi.prior <- dnormv(phi, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} delta.prior <- dnormv(delta, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} omega.prior <- dhalfcauchy(omega, 25, log=TRUE) \\
\hspace*{0.27 in} theta.prior <- dunif(theta, 0, 1, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- c(alpha, alpha + phi*Data$y[-Data$T]) \\
\hspace*{0.27 in} epsilon <- Data$y - mu \\
\hspace*{0.27 in} sigma2 <- c(omega, omega + theta*epsilon[-Data$T]\textasciicircum 2) \\
\hspace*{0.27 in} mu <- mu + delta*sigma2 \\
\hspace*{0.27 in} sigma2.new <- omega + theta*epsilon[Data$T]\textasciicircum 2 \\
\hspace*{0.27 in} ynew <- rnorm(1, alpha + phi*Data$y[Data$T] + delta*sigma2[Data$T], \\
\hspace*{0.62 in} sigma2.new) \\
\hspace*{0.27 in} LL <- sum(dnormv(Data$y, mu, sigma2, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + phi.prior + delta.prior + omega.prior + theta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, ynew, sigma2.new), \\
\hspace*{0.62 in} yhat=rnormv(length(mu), mu, sigma2), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,3), rep(0.5,2))}

\section{Autoregression, AR(1)} \label{ar1}
\subsection{Form}
$$\textbf{y}_t \sim \mathcal{N}(\mu_t, \sigma^2), \quad t=1,\dots,T$$
$$\textbf{y}^{new} = \alpha + \mu_{T+1}$$
$$\mu_t = \alpha + \phi \textbf{y}_{t-1}, \quad t=1,\dots,(T+1)$$
$$\alpha \sim \mathcal{N}(0, 1000)$$
$$\phi \sim \mathcal{N}(0, 1000)$$
$$\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{y <- c(0.02, -0.51, -0.30,  1.46, -1.26, -2.15, -0.91, -0.53, -1.91, \\
\hspace*{0.27 in} 2.64,  1.64,  0.15,  1.46,  1.61,  1.96, -2.67, -0.19, -3.28, \\
\hspace*{0.27 in}  1.89,  0.91, -0.71,  0.74, -0.10,  3.20, -0.80, -5.25,  1.03, \\
\hspace*{0.27 in} -0.40, -1.62, -0.80,  0.77,  0.17, -1.39, -1.28,  0.48, -1.02, \\
\hspace*{0.27 in}  0.09, -1.09,  0.86,  0.36,  1.51, -0.02,  0.47,  0.62, -1.36, \\
\hspace*{0.27 in}  1.12,  0.42, -4.39, -0.87,  0.05, -5.41, -7.38, -1.01, -1.70, \\
\hspace*{0.27 in}  0.64,  1.16,  0.87,  0.28, -1.69, -0.29,  0.13, -0.65,  0.83, \\
\hspace*{0.27 in}  0.62,  0.05, -0.14,  0.01, -0.36, -0.32, -0.80, -0.06,  0.24, \\
\hspace*{0.27 in}  0.23, -0.37,  0.00, -0.33,  0.21, -0.10, -0.10, -0.01, -0.40, \\
\hspace*{0.27 in} -0.35,  0.48, -0.28,  0.08,  0.28,  0.23,  0.27, -0.35, -0.19, \\
\hspace*{0.27 in}  0.24,  0.17, -0.02, -0.23,  0.03,  0.02, -0.17,  0.04, -0.39, \\
\hspace*{0.27 in} -0.12,  0.16,  0.17,  0.00,  0.18,  0.06, -0.36,  0.22,  0.14, \\
\hspace*{0.27 in} -0.17,  0.10, -0.01,  0.00, -0.18, -0.02,  0.07, -0.06,  0.06, \\
\hspace*{0.27 in} -0.05, -0.08, -0.07,  0.01, -0.06,  0.01,  0.01, -0.02,  0.01, \\
\hspace*{0.27 in}  0.01,  0.12, -0.03,  0.08, -0.10,  0.01, -0.03, -0.08,  0.04, \\
\hspace*{0.27 in} -0.09, -0.08,  0.01, -0.05,  0.08, -0.14,  0.06, -0.11,  0.09, \\
\hspace*{0.27 in}  0.06, -0.12, -0.01, -0.05, -0.15, -0.05, -0.03,  0.04,  0.00, \\
\hspace*{0.27 in} -0.12,  0.04, -0.06, -0.05, -0.07, -0.05, -0.14, -0.05, -0.01, \\
\hspace*{0.27 in} -0.12,  0.05,  0.06, -0.10,  0.00,  0.01,  0.00, -0.08,  0.00, \\
\hspace*{0.27 in}  0.00,  0.07, -0.01,  0.00,  0.09,  0.33,  0.13,  0.42,  0.24, \\
\hspace*{0.27 in} -0.36,  0.22, -0.09, -0.19, -0.10, -0.08, -0.07,  0.05,  0.07, \\
\hspace*{0.27 in}  0.07,  0.00, -0.04, -0.05,  0.03,  0.08,  0.26,  0.10,  0.08, \\
\hspace*{0.27 in}  0.09, -0.07, -0.33,  0.17, -0.03,  0.07, -0.04, -0.06, -0.06, \\
\hspace*{0.27 in}  0.07, -0.03,  0.00,  0.08,  0.27,  0.11,  0.11,  0.06, -0.11, \\
\hspace*{0.27 in} -0.09, -0.21,  0.24, -0.12,  0.11, -0.02, -0.03,  0.02, -0.10, \\
\hspace*{0.27 in}  0.00, -0.04,  0.01,  0.02, -0.03, -0.10, -0.09,  0.17,  0.07, \\
\hspace*{0.27 in} -0.05, -0.01, -0.05,  0.01,  0.00, -0.08, -0.05, -0.08,  0.07, \\
\hspace*{0.27 in}  0.06, -0.14,  0.02,  0.01,  0.04,  0.00, -0.13, -0.17) \\
T <- length(y) \\
mon.names <- c("LP", "ynew") \\
parm.names <- c("alpha","phi","sigma") \\
PGF <- function(Data) return(c(rnorm(2,0,1), rhalfcauchy(1,5))) \\
MyData <- list(PGF=PGF, T=T, mon.names=mon.names, parm.names=parm.names, \\
\hspace*{0.27 in} y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1]; phi <- parm[2]
\hspace*{0.27 in} sigma <- interval(parm[3], 1e-100, Inf) \\
\hspace*{0.27 in} parm[3] <- sigma \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnormv(alpha, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} phi.prior <- dnormv(phi, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- c(alpha, alpha + phi*Data$y[-Data$T]) \\
\hspace*{0.27 in} ynew <- rnorm(1, alpha + phi*Data$y[Data$T], sigma) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + phi.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,ynew), \\
\hspace*{0.62 in} yhat=rnorm(length(mu), mu, sigma), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,2), 1)}

\section{Autoregressive Conditional Heteroskedasticity, ARCH(1,1)} \label{arch11}
\subsection{Form}
$$\textbf{y}_t \sim \mathcal{N}(\mu_t, \sigma^2_t), \quad t=1,\dots,T$$
$$\textbf{y}^{new} \sim \mathcal{N}(\mu_{T+1}, \sigma^2_{new})$$
$$\mu_t = \alpha + \phi \textbf{y}_{t-1}, \quad t=1,\dots,(T+1)$$
$$\epsilon_t = \textbf{y}_t - \mu_t$$
$$\alpha \sim \mathcal{N}(0, 1000)$$
$$\phi \sim \mathcal{N}(0, 1000)$$
$$\sigma^2_{new} = \omega + \theta \epsilon^2_T$$
$$\sigma^2_t = \omega + \theta \epsilon^2_{t-1},$$
$$\omega \sim \mathcal{HC}(25)$$
$$\theta \sim \mathcal{U}(0, 1)$$
\subsection{Data}
\code{y <- c(0.02, -0.51, -0.30,  1.46, -1.26, -2.15, -0.91, -0.53, -1.91, \\
\hspace*{0.27 in} 2.64,  1.64,  0.15,  1.46,  1.61,  1.96, -2.67, -0.19, -3.28, \\
\hspace*{0.27 in}  1.89,  0.91, -0.71,  0.74, -0.10,  3.20, -0.80, -5.25,  1.03, \\
\hspace*{0.27 in} -0.40, -1.62, -0.80,  0.77,  0.17, -1.39, -1.28,  0.48, -1.02, \\
\hspace*{0.27 in}  0.09, -1.09,  0.86,  0.36,  1.51, -0.02,  0.47,  0.62, -1.36, \\
\hspace*{0.27 in}  1.12,  0.42, -4.39, -0.87,  0.05, -5.41, -7.38, -1.01, -1.70, \\
\hspace*{0.27 in}  0.64,  1.16,  0.87,  0.28, -1.69, -0.29,  0.13, -0.65,  0.83, \\
\hspace*{0.27 in}  0.62,  0.05, -0.14,  0.01, -0.36, -0.32, -0.80, -0.06,  0.24, \\
\hspace*{0.27 in}  0.23, -0.37,  0.00, -0.33,  0.21, -0.10, -0.10, -0.01, -0.40, \\
\hspace*{0.27 in} -0.35,  0.48, -0.28,  0.08,  0.28,  0.23,  0.27, -0.35, -0.19, \\
\hspace*{0.27 in}  0.24,  0.17, -0.02, -0.23,  0.03,  0.02, -0.17,  0.04, -0.39, \\
\hspace*{0.27 in} -0.12,  0.16,  0.17,  0.00,  0.18,  0.06, -0.36,  0.22,  0.14, \\
\hspace*{0.27 in} -0.17,  0.10, -0.01,  0.00, -0.18, -0.02,  0.07, -0.06,  0.06, \\
\hspace*{0.27 in} -0.05, -0.08, -0.07,  0.01, -0.06,  0.01,  0.01, -0.02,  0.01, \\
\hspace*{0.27 in}  0.01,  0.12, -0.03,  0.08, -0.10,  0.01, -0.03, -0.08,  0.04, \\
\hspace*{0.27 in} -0.09, -0.08,  0.01, -0.05,  0.08, -0.14,  0.06, -0.11,  0.09, \\
\hspace*{0.27 in}  0.06, -0.12, -0.01, -0.05, -0.15, -0.05, -0.03,  0.04,  0.00, \\
\hspace*{0.27 in} -0.12,  0.04, -0.06, -0.05, -0.07, -0.05, -0.14, -0.05, -0.01, \\
\hspace*{0.27 in} -0.12,  0.05,  0.06, -0.10,  0.00,  0.01,  0.00, -0.08,  0.00, \\
\hspace*{0.27 in}  0.00,  0.07, -0.01,  0.00,  0.09,  0.33,  0.13,  0.42,  0.24, \\
\hspace*{0.27 in} -0.36,  0.22, -0.09, -0.19, -0.10, -0.08, -0.07,  0.05,  0.07, \\
\hspace*{0.27 in}  0.07,  0.00, -0.04, -0.05,  0.03,  0.08,  0.26,  0.10,  0.08, \\
\hspace*{0.27 in}  0.09, -0.07, -0.33,  0.17, -0.03,  0.07, -0.04, -0.06, -0.06, \\
\hspace*{0.27 in}  0.07, -0.03,  0.00,  0.08,  0.27,  0.11,  0.11,  0.06, -0.11, \\
\hspace*{0.27 in} -0.09, -0.21,  0.24, -0.12,  0.11, -0.02, -0.03,  0.02, -0.10, \\
\hspace*{0.27 in}  0.00, -0.04,  0.01,  0.02, -0.03, -0.10, -0.09,  0.17,  0.07, \\
\hspace*{0.27 in} -0.05, -0.01, -0.05,  0.01,  0.00, -0.08, -0.05, -0.08,  0.07, \\
\hspace*{0.27 in}  0.06, -0.14,  0.02,  0.01,  0.04,  0.00, -0.13, -0.17) \\
T <- length(y) \\
mon.names <- c("LP", "ynew", "sigma2.new") \\
parm.names <- c("alpha","phi","omega","theta") \\
PGF <-function(Data) return(c(rnorm(2,0,1), rhalfcauchy(1,5), runif(1))) \\
MyData <- list(PGF=PGF, T=T, mon.names=mon.names, parm.names=parm.names, \\
\hspace*{0.27 in} y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1]; phi <- parm[2] \\
\hspace*{0.27 in} parm[3] <- omega <- interval(parm[3], 1e-100, Inf) \\
\hspace*{0.27 in} parm[4] <- theta <- interval(parm[4], 1e-10, 1-1e-5) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnormv(alpha, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} phi.prior <- dnormv(phi, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} omega.prior <- dhalfcauchy(omega, 25, log=TRUE) \\
\hspace*{0.27 in} theta.prior <- dunif(theta, 0, 1, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- c(alpha, alpha + phi*Data$y[-Data$T]) \\
\hspace*{0.27 in} epsilon <- Data$y - mu \\
\hspace*{0.27 in} sigma2 <- c(omega, omega + theta*epsilon[-Data$T]\textasciicircum 2) \\
\hspace*{0.27 in} sigma2.new <- omega + theta*epsilon[Data$T]\textasciicircum 2 \\
\hspace*{0.27 in} ynew <- rnormv(1, alpha + phi*Data$y[Data$T], sigma2.new) \\
\hspace*{0.27 in} LL <- sum(dnormv(Data$y, mu, sigma2, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + phi.prior + omega.prior + theta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, ynew, \\
\hspace*{0.62 in} sigma2.new), yhat=rnormv(length(mu), mu, sigma2), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,2), rep(0.5,2))}

\section{Autoregressive Moving Average, ARMA(1,1)} \label{arma11}
\subsection{Form}
$$\textbf{y}_t \sim \mathcal{N}(\mu_t, \sigma^2), \quad t=1,\dots,T$$
$$\textbf{y}^{new} = \alpha + \phi \textbf{y}_T + \theta \epsilon_T$$
$$\mu_t = \alpha + \phi \textbf{y}_{t-1} + \theta \epsilon_{t-1}$$
$$\epsilon_t = \textbf{y}_t - \mu_t$$
$$\alpha \sim \mathcal{N}(0, 1000)$$
$$\phi \sim \mathcal{N}(0, 1000)$$
$$\sigma \sim \mathcal{HC}(25)$$
$$\theta \sim \mathcal{N}(0, 1000)$$
\subsection{Data}
\code{y <- c(0.02, -0.51, -0.30,  1.46, -1.26, -2.15, -0.91, -0.53, -1.91, \\
\hspace*{0.27 in} 2.64,  1.64,  0.15,  1.46,  1.61,  1.96, -2.67, -0.19, -3.28, \\
\hspace*{0.27 in}  1.89,  0.91, -0.71,  0.74, -0.10,  3.20, -0.80, -5.25,  1.03, \\
\hspace*{0.27 in} -0.40, -1.62, -0.80,  0.77,  0.17, -1.39, -1.28,  0.48, -1.02, \\
\hspace*{0.27 in}  0.09, -1.09,  0.86,  0.36,  1.51, -0.02,  0.47,  0.62, -1.36, \\
\hspace*{0.27 in}  1.12,  0.42, -4.39, -0.87,  0.05, -5.41, -7.38, -1.01, -1.70, \\
\hspace*{0.27 in}  0.64,  1.16,  0.87,  0.28, -1.69, -0.29,  0.13, -0.65,  0.83, \\
\hspace*{0.27 in}  0.62,  0.05, -0.14,  0.01, -0.36, -0.32, -0.80, -0.06,  0.24, \\
\hspace*{0.27 in}  0.23, -0.37,  0.00, -0.33,  0.21, -0.10, -0.10, -0.01, -0.40, \\
\hspace*{0.27 in} -0.35,  0.48, -0.28,  0.08,  0.28,  0.23,  0.27, -0.35, -0.19, \\
\hspace*{0.27 in}  0.24,  0.17, -0.02, -0.23,  0.03,  0.02, -0.17,  0.04, -0.39, \\
\hspace*{0.27 in} -0.12,  0.16,  0.17,  0.00,  0.18,  0.06, -0.36,  0.22,  0.14, \\
\hspace*{0.27 in} -0.17,  0.10, -0.01,  0.00, -0.18, -0.02,  0.07, -0.06,  0.06, \\
\hspace*{0.27 in} -0.05, -0.08, -0.07,  0.01, -0.06,  0.01,  0.01, -0.02,  0.01, \\
\hspace*{0.27 in}  0.01,  0.12, -0.03,  0.08, -0.10,  0.01, -0.03, -0.08,  0.04, \\
\hspace*{0.27 in} -0.09, -0.08,  0.01, -0.05,  0.08, -0.14,  0.06, -0.11,  0.09, \\
\hspace*{0.27 in}  0.06, -0.12, -0.01, -0.05, -0.15, -0.05, -0.03,  0.04,  0.00, \\
\hspace*{0.27 in} -0.12,  0.04, -0.06, -0.05, -0.07, -0.05, -0.14, -0.05, -0.01, \\
\hspace*{0.27 in} -0.12,  0.05,  0.06, -0.10,  0.00,  0.01,  0.00, -0.08,  0.00, \\
\hspace*{0.27 in}  0.00,  0.07, -0.01,  0.00,  0.09,  0.33,  0.13,  0.42,  0.24, \\
\hspace*{0.27 in} -0.36,  0.22, -0.09, -0.19, -0.10, -0.08, -0.07,  0.05,  0.07, \\
\hspace*{0.27 in}  0.07,  0.00, -0.04, -0.05,  0.03,  0.08,  0.26,  0.10,  0.08, \\
\hspace*{0.27 in}  0.09, -0.07, -0.33,  0.17, -0.03,  0.07, -0.04, -0.06, -0.06, \\
\hspace*{0.27 in}  0.07, -0.03,  0.00,  0.08,  0.27,  0.11,  0.11,  0.06, -0.11, \\
\hspace*{0.27 in} -0.09, -0.21,  0.24, -0.12,  0.11, -0.02, -0.03,  0.02, -0.10, \\
\hspace*{0.27 in}  0.00, -0.04,  0.01,  0.02, -0.03, -0.10, -0.09,  0.17,  0.07, \\
\hspace*{0.27 in} -0.05, -0.01, -0.05,  0.01,  0.00, -0.08, -0.05, -0.08,  0.07, \\
\hspace*{0.27 in}  0.06, -0.14,  0.02,  0.01,  0.04,  0.00, -0.13, -0.17) \\
T <- length(y) \\
mon.names <- c("LP", "ynew") \\
parm.names <- c("alpha","phi","sigma","theta") \\
PGF <- function(Data) return(c(rnorm(2,0,1), rhalfcauchy(1,5), \\
\hspace*{0.27 in} rnorm(1,0,1))) \\
MyData <- list(PGF=PGF, T=T, mon.names=mon.names, parm.names=parm.names, \\
\hspace*{0.27 in} y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1]; phi <- parm[2]; theta <- parm[3] \\
\hspace*{0.27 in} parm[4] <- sigma <- interval(parm[4], 1e-100, Inf) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnormv(alpha, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} phi.prior <- dnormv(phi, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} theta.prior <- dnormv(theta, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- c(alpha, alpha + phi*Data$y[-Data$T]) \\
\hspace*{0.27 in} epsilon <- Data$y - mu \\
\hspace*{0.27 in} mu <- c(mu[1], mu[-1] + theta * epsilon[-Data$T]) \\
\hspace*{0.27 in} ynew <- rnorm(1, alpha + phi*Data$y[Data$T] + theta*epsilon[Data$T], \\
\hspace*{0.62 in} sigma) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + phi.prior + sigma.prior + theta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, ynew), \\
\hspace*{0.62 in} yhat=rnorm(length(mu), mu, sigma), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,2), 0, 1)}

\section{Beta Regression} \label{beta.reg}
\subsection{Form}
$$\textbf{y} \sim \mathcal{BETA}(a,b)$$
$$a = \mu \phi$$
$$b = (1 - \mu) \phi$$
$$\mu = \Phi(\beta_1 + \beta_2 \textbf{x}), \quad \mu \in (0, 1)$$
$$\beta_j \sim \mathcal{N}(0, 10), \quad j=1,\dots,J$$
$$\phi \sim \mathcal{HC}(25)$$
where $\Phi$ is the normal CDF.
\subsection{Data}
\code{N <- 100 \\
x <- runif(N) \\
y <- rbeta(N, (0.5-0.2*x)*3, (1-(0.5-0.2*x))*3)
mon.names <- "LP" \\
parm.names <- c("beta[1]","beta[2]","phi") \\
pos.beta <- grep("beta", parm.names) \\
pos.phi <- grep("phi", parm.names) \\
PGF <- function(Data) return(c(rnormv(2,0,10), rhalfcauchy(1,5))) \\
MyData <- list(PGF=PGF, mon.names=mon.names, parm.names=parm.names, \\
\hspace*{0.27 in} pos.beta=pos.beta, pos.phi=pos.phi, x=x, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} parm[Data$pos.phi] <- phi <- interval(parm[Data$pos.phi], 1e-100, Inf) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 10, log=TRUE)) \\
\hspace*{0.27 in} phi.prior <- dhalfcauchy(phi, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- interval(pnorm(beta[1] + beta[2]*Data$x), 0.001, 0.999, \\
\hspace*{0.62 in} reflect=FALSE) \\
\hspace*{0.27 in} a <- mu * phi \\
\hspace*{0.27 in} b <- (1 - mu) * phi \\
\hspace*{0.27 in} LL <- sum(dbeta(Data$y, a, b, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + phi.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rbeta(length(mu), a, b), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,2), 0.01)}

\section{Beta-Binomial} \label{beta.binomial}
\subsection{Form}
$$\textbf{y}_i \sim \mathcal{BIN}(\textbf{n}_i, \pi_i), \quad i=1,\dots,N$$
$$\pi_i \sim \mathcal{BETA}(\alpha, \beta) \in [0.001,0.999]$$
\subsection{Data}
\code{N <- 20 \\
n <- round(runif(N, 50, 100)) \\
y <- round(runif(N, 1, 10)) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(pi=rep(0,N))) \\
PGF <- function(Data) return(rbeta(Data$N,1,1)) \\
MyData <- list(N=N, PGF=PGF, mon.names=mon.names, n=n, \\
\hspace*{0.27 in} parm.names=parm.names, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} parm[1:Data$N] <- pi <- interval(parm[1:Data$N], 0.001, 0.999) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} pi.prior <- sum(dbeta(pi, 1, 1, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} LL <- sum(dbinom(Data$y, Data$n, pi, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + pi.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rbinom(Data$N, Data$n, pi), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0.5,N))}

\section{Binary Logit} \label{binary.logit}
\subsection{Form}
$$\textbf{y} \sim \mathcal{BERN}(\eta)$$
$$\eta = \frac{1}{1 + \exp(-\mu)}$$
$$\mu = \textbf{X} \beta$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
\subsection{Data}
\code{data(demonsnacks) \\
J <- 3 \\
y <- ifelse(demonsnacks$Calories <= 137, 0, 1) \\
X <- cbind(1, as.matrix(demonsnacks[,c(7,8)])) \\
for (j in 2:J) \{X[,j] <- CenterScale(X[,j])\} \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=rep(0,J))) \\
PGF <- function(Data) return(rnormv(Data$J,0,1000)) \\
MyData <- list(J=J, PGF=PGF, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, t(beta)) \\
\hspace*{0.27 in} eta <- invlogit(mu) \\
\hspace*{0.27 in} LL <- sum(dbern(Data$y, eta, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rbern(length(eta), eta), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- rep(0,J)}

\section{Binary Log-Log Link Mixture} \label{binary.loglog.mixture}
A weighted mixture of the log-log and complementary log-log link functions is used, where $\alpha$ is the weight. Since the log-log and complementary log-log link functions are asymmetric (as opposed to the symmetric logit and probit link functions), it may be unknown \textit{a priori} whether the log-log or complementary log-log will perform better. 
\subsection{Form}
$$\textbf{y} \sim \mathcal{BERN}(\eta)$$
$$\eta = \alpha \exp(-\exp(\mu)) + (1 - \alpha) (1 - \exp(-\exp(\mu)))$$
$$\mu = \textbf{X} \beta$$
$$\alpha \sim \mathcal{U}(0, 1)$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
\subsection{Data}
\code{data(demonsnacks) \\
J <- 3 \\
y <- ifelse(demonsnacks$Calories <= 30, 0, 1) \\
X <- cbind(1, as.matrix(demonsnacks[,c(7,8)])) \\
for (j in 2:J) \{X[,j] <- CenterScale(X[,j])\} \\
mon.names <- c("LP","alpha") \\
parm.names <- as.parm.names(list(beta=rep(0,J), logit.alpha=0)) \\
PGF <- function(Data) return(c(rnormv(Data$J,0,1000), runif(1))) \\
MyData <- list(J=J, PGF=PGF, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} parm[Data$J+1] <- alpha <- interval(parm[Data$J+1], -700, 700) \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dunif(alpha, 0, 1, log=TRUE) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, t(beta)) \\
\hspace*{0.27 in} eta <- alpha*invloglog(mu) + (1-alpha)*invcloglog(mu) \\
\hspace*{0.27 in} LL <- sum(dbern(Data$y, eta, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,alpha), \\
\hspace*{0.62 in} yhat=rbern(length(eta), eta), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), 0)}

\section{Binary Probit} \label{binary.probit}
\subsection{Form}
$$\textbf{y} \sim \mathcal{BERN}(\textbf{p})$$
$$\textbf{p} = \phi(\mu)$$
$$\mu = \textbf{X} \beta \in [-10,10]$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
where $\phi$ is the CDF of the standard normal distribution, and $J$=3.
\subsection{Data}
\code{data(demonsnacks) \\
J <- 3 \\
y <- ifelse(demonsnacks$Calories <= 137, 0, 1) \\
X <- cbind(1, as.matrix(demonsnacks[,c(7,8)])) \\
for (j in 2:J) \{X[,j] <- CenterScale(X[,j])\} \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=rep(0,J))) \\
PGF <- function(Data) return(rnormv(Data$J,0,1000)) \\
MyData <- list(J=J, PGF=PGF, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, t(beta)) \\
\hspace*{0.27 in} mu <- interval(mu, -10, 10, reflect=FALSE) \\
\hspace*{0.27 in} p <- pnorm(mu) \\
\hspace*{0.27 in} LL <- sum(dbern(Data$y, p, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rbern(length(p), p), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- rep(0,J)}

\section{Binary Robit} \label{binary.robit}
\subsection{Form}
$$\textbf{y} \sim \mathcal{BERN}(\textbf{p})$$
$$\textbf{p} = \textbf{T}_\nu(\mu)$$
$$\mu = \textbf{X} \beta \in [-10,10]$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\nu \sim \mathcal{U}(5, 10)$$
where $\textbf{T}_\nu$ is the CDF of the standard t-distribution with $\nu$ degrees of freedom.
\subsection{Data}
\code{data(demonsnacks) \\
y <- ifelse(demonsnacks$Calories <= 137, 0, 1) \\
X <- cbind(1, as.matrix(demonsnacks[,c(7,8)])) \\
J <- ncol(X) \\
for (j in 2:J) \{X[,j] <- CenterScale(X[,j])\} \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=rep(0,J), nu=0)) \\
PGF <- function(Data) return(c(rnormv(Data$J,0,1), \\
\hspace*{0.27 in} runif(1,5,10))) \\
MyData <- list(J=J, PGF=PGF, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} parm[Data$J+1] <- nu <- interval(parm[Data$J+1], 5, 10) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} nu.prior <- dunif(nu, 5, 10, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, t(beta)) \\
\hspace*{0.27 in} mu <- interval(mu, -10, 10, reflect=FALSE) \\
\hspace*{0.27 in} p <- pst(mu, nu=nu) \\
\hspace*{0.27 in} LL <- sum(dbern(Data$y, p, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + nu.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rbern(length(p), p), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), 5)}

\section{Binomial Logit} \label{binomial.logit}
\subsection{Form}
$$\textbf{y} \sim \mathcal{BIN}(\textbf{p}, \textbf{n})$$
$$\textbf{p} = \frac{1}{1 + \exp(-\mu)}$$
$$\mu = \beta_1 + \beta_2 \textbf{x}$$
$$\beta_j \sim \mathcal{N}(0,1000), \quad j=1,\dots,J$$
\subsection{Data}
\code{\#10 Trials \\
exposed <- c(100,100,100,100,100,100,100,100,100,100) \\
deaths <- c(10,20,30,40,50,60,70,80,90,100) \\
dose <- c(1,2,3,4,5,6,7,8,9,10) \\
J <- 2 \#Number of parameters \\
mon.names <- "LP" \\
parm.names <- c("beta[1]","beta[2]") \\
PGF <- function(Data) return(rnormv(Data$J,0,1000)) \\
MyData <- list(J=J, PGF=PGF, n=exposed, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, x=dose, y=deaths)
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- beta[1] + beta[2]*Data$x \\
\hspace*{0.27 in} p <- invlogit(mu) \\
\hspace*{0.27 in} LL <- sum(dbinom(Data$y, Data$n, p, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rbinom(length(p), Data$n, p), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- rep(0,J)}

\section{Binomial Probit} \label{binomial.probit}
\subsection{Form}
$$\textbf{y} \sim \mathcal{BIN}(\textbf{p}, \textbf{n})$$
$$\textbf{p} = \phi(\mu)$$
$$\mu = \beta_1 + \beta_2 \textbf{x} \in [-10,10]$$
$$\beta_j \sim \mathcal{N}(0,1000), \quad j=1,\dots,J$$
where $\phi$ is the CDF of the standard normal distribution, and $J$=2.
\subsection{Data}
\code{\#10 Trials \\
exposed <- c(100,100,100,100,100,100,100,100,100,100) \\
deaths <- c(10,20,30,40,50,60,70,80,90,100) \\
dose <- c(1,2,3,4,5,6,7,8,9,10) \\
J <- 2 \#Number of parameters \\
mon.names <- "LP" \\
parm.names <- c("beta[1]","beta[2]") \\
PGF <- function(Data) return(rnormv(Data$J,0,1000)) \\
MyData <- list(J=J, PGF=PGF, n=exposed, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, x=dose, y=deaths) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- beta[1] + beta[2]*Data$x \\
\hspace*{0.27 in} mu <- interval(mu, -10, 10, reflect=FALSE) \\
\hspace*{0.27 in} p <- pnorm(mu) \\
\hspace*{0.27 in} LL <- sum(dbinom(Data$y, Data$n, p, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rbinom(length(p), Data$n, p), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- rep(0,J)}

\section{Binomial Robit} \label{binomial.robit}
\subsection{Form}
$$\textbf{y} \sim \mathcal{BIN}(\textbf{p}, \textbf{n})$$
$$\textbf{p} = \textbf{T}_\nu(\mu)$$
$$\mu = \beta_1 + \beta_2 \textbf{x} \in [-10,10]$$
$$\beta_j \sim \mathcal{N}(0,1000), \quad j=1,\dots,J$$
$$\nu \sim \mathcal{U}(5, 10)$$ 
where $\textbf{T}_\nu$ is the CDF of the standard t-distribution with $\nu$ degrees of freedom.
\subsection{Data}
\code{\#10 Trials \\
exposed <- c(100,100,100,100,100,100,100,100,100,100) \\
deaths <- c(10,20,30,40,50,60,70,80,90,100) \\
dose <- c(1,2,3,4,5,6,7,8,9,10) \\
J <- 2 \#Number of parameters \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=rep(0,2), nu=0)) \\
PGF <- function(Data) return(c(rnormv(Data$J,0,1000), runif(1,5,10))) \\
MyData <- list(J=J, PGF=PGF, n=exposed, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, x=dose, y=deaths) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} parm[Data$J+1] <- nu <- interval(parm[Data$J+1], 5, 10) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} nu.prior <- dunif(nu, 5, 10, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- beta[1] + beta[2]*Data$x \\
\hspace*{0.27 in} mu <- interval(mu, -10, 10, reflect=FALSE) \\
\hspace*{0.27 in} p <- pst(mu, nu=nu) \\
\hspace*{0.27 in} LL <- sum(dbinom(Data$y, Data$n, p, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + nu.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rbinom(length(p), Data$n, p), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), 5)}

\section{Change Point Regression} \label{changepoint}
This example uses a popular variant of the stagnant water data set.
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2)$$
$$\mu = \alpha + \beta_1 \textbf{x} + \beta_2 (\textbf{x} - \theta)[(\textbf{x} - \theta) > 0]$$
$$\alpha \sim \mathcal{N}(0, 1000)$$
$$\beta \sim \mathcal{N}(0, 1000)$$
$$\sigma \sim \mathcal{HC}(25)$$
$$\theta \sim \mathcal{U}(-1.3, 1.1)$$
\subsection{Data}
\code{N <- 29 \\
y <- c(1.12, 1.12, 0.99, 1.03, 0.92, 0.90, 0.81, 0.83, 0.65, 0.67, 0.60, \\
\hspace*{0.27 in} 0.59, 0.51, 0.44, 0.43, 0.43, 0.33, 0.30, 0.25, 0.24, 0.13, -0.01, \\
\hspace*{0.27 in} -0.13, -0.14, -0.30, -0.33, -0.46, -0.43, -0.65) \\
x <- c(-1.39, -1.39, -1.08, -1.08, -0.94, -0.80, -0.63, -0.63, -0.25, -0.25, \\
\hspace*{0.27 in} -0.12, -0.12, 0.01, 0.11, 0.11, 0.11, 0.25, 0.25, 0.34, 0.34, 0.44, \\
\hspace*{0.27 in} 0.59, 0.70, 0.70, 0.85, 0.85, 0.99, 0.99, 1.19) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(alpha=0, beta=rep(0,2), sigma=0, theta=0)) \\
pos.alpha <- grep("alpha", parm.names) \\
pos.beta <- grep("beta", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
pos.theta <- grep("theta", parm.names) \\
PGF <- function(Data) return(c(rnorm(1), rnorm(2), runif(1), runif(1))) \\
MyData <- list(N=N, PGF=PGF, mon.names=mon.names, parm.names=parm.names, \\
\hspace*{0.27 in} pos.alpha=pos.alpha, pos.beta=pos.beta, pos.sigma=pos.sigma, \\
\hspace*{0.27 in} pos.theta=pos.theta, x=x, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
     \{ \\
     \#\#\# Parameters \\
     alpha <- parm[Data$pos.alpha] \\
     beta <- parm[Data$pos.beta] \\
     sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
     parm[Data$pos.sigma] <- sigma \\
     theta <- interval(parm[Data$pos.theta], -1.3, 1.1) \\
     parm[Data$pos.theta] <- theta \\
     \#\#\# Log(Prior Densities) \\
     alpha.prior <- dnormv(alpha, 0, 1000, log=TRUE) \\
     beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
     sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
     theta.prior <- dunif(theta, -1.3, 1.1, log=TRUE) \\
     \#\#\# Log-Likelihood \\
     mu <- alpha + beta[1]*x + beta[2]*(x - theta)*{(x - theta) > 0} \\
     LL <- sum(dnorm(Data$y, mu, sigma, log=TRUE)) \\
     \#\#\# Log-Posterior \\
     LP <- LL + alpha.prior + beta.prior + sigma.prior + theta.prior \\
     Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
          yhat=rnorm(length(mu), mu, sigma), parm=parm) \\
     return(Modelout) \\
     \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(0.2, -0.45, 0, 0.2, 0)}

\section{Cluster Analysis, Confirmatory (CCA)} \label{cca}
This is a parametric, model-based, cluster analysis, also called a finite mixture model or latent class cluster analysis, where the number of clusters $C$ is fixed. When the number of clusters is unknown, exploratory cluster analysis should be used (see section \ref{eca}). The record-level cluster membership parameter vector, $\theta$, is a vector of discrete parameters. Discrete parameters are not supported in all algorithms. The example below is updated with the Griddy-Gibbs sampler.
\subsection{Form}
$$\textbf{Y}_{i,j} \sim \mathcal{N}(\mu_{\theta[i],j}, \sigma^2_{\theta[i]}), \quad i=1,\dots,N, \quad j=1,\dots,J$$
$$\theta_i \sim \mathcal{CAT}(\pi_{1:C}), \quad i=1,\dots,N$$
$$\pi_{1:C} \sim \mathcal{D}(\alpha_{1:C})$$
$$\alpha_c = 1$$
$$\mu_{c,j} \sim \mathcal{N}(0, \nu^2_j)$$
$$\sigma_c \sim \mathcal{HC}(25)$$
$$\nu_j \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{data(demonsnacks) \\
Y <- as.matrix(log(demonsnacks + 1)) \\
N <- nrow(Y) \\
J <- ncol(Y) \\
for (j in 1:J) \{Y[,j] <- CenterScale(Y[,j])\} \\
C <- 3 \#Number of clusters \\
alpha <- rep(1,C) \#Prior probability of cluster proportion \\
mon.names <- c("LP", paste("pi[", 1:C, "]", sep="")) \\
parm.names <- as.parm.names(list(theta=rep(0,N), mu=matrix(0,C,J), \\
\hspace*{0.27 in} nu=rep(0,J), sigma=rep(0,C))) \\
pos.theta <- grep("theta", parm.names) \\
pos.mu <- grep("mu", parm.names) \\
pos.nu <- grep("nu", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
PGF <- function(Data) return(c(rcat(N, p=rep(1/C, C)), \\
\hspace*{0.27 in} rnorm(Data$C*Data$J,0,1), rhalfcauchy(Data$J+Data$C,5))) \\
MyData <- list(C=C, J=J, N=N, PGF=PGF, Y=Y, alpha=alpha, \\
\hspace*{0.27 in} mon.names=mon.names, parm.names=parm.names, pos.theta=pos.theta, \\
\hspace*{0.27 in} pos.mu=pos.mu, pos.nu=pos.nu, pos.sigma=pos.sigma) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} theta <- parm[Data$pos.theta] \\
\hspace*{0.27 in} mu <- matrix(parm[Data$pos.mu], Data$C, Data$J) \\
\hspace*{0.27 in} parm[Data$pos.nu] <- nu <- interval(parm[Data$pos.nu], 1e-100, Inf) \\
\hspace*{0.27 in} pi <- rep(0, Data$C) \\
\hspace*{0.27 in} tab <- table(theta) \\
\hspace*{0.27 in} pi[as.numeric(names(tab))] <- as.vector(tab) \\
\hspace*{0.27 in} pi <- pi / sum(pi) \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} theta.prior <- sum(dcat(theta, pi, log=TRUE)) \\
\hspace*{0.27 in} mu.prior <- sum(dnorm(mu, 0, matrix(rep(nu,Data$C), Data$C, \\
\hspace*{0.62 in} Data$J, byrow=TRUE), log=TRUE)) \\
\hspace*{0.27 in} nu.prior <- sum(dhalfcauchy(nu, 25, log=TRUE)) \\
\hspace*{0.27 in} pi.prior <- ddirichlet(pi, Data$alpha, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$Y, mu[theta,], sigma[theta], log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + theta.prior + mu.prior + nu.prior + pi.prior + \\
\hspace*{0.62 in} sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,pi), \\
\hspace*{0.62 in} yhat=rnorm(prod(dim(mu[theta,])), mu[theta,], sigma[theta]), \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rcat(N,rep(1/C,C)), rep(0,C*J), rep(1,J), rep(1,C))}

\section{Cluster Analysis, Exploratory (ECA)} \label{eca}
In ``exploratory cluster analysis'', the optimal number of clusters $C$ is unknown before the model update. This is a nonparametric, model-based, infinite mixture model that uses truncated stick-breaking within a truncated Dirichlet process. The user must specify the maximum number of clusters (mixture components), $C$ to explore, where $C$ is discrete, greater than one, and less than the number of records, $N$. The records in the $N \times J$ matrix \textbf{Y} are clustered, where $J$ is the number of predictors. The record-level cluster membership parameter vector, $\theta$, is a vector of discrete parameters. Discrete parameters are not supported in all algorithms. The example below is updated with the Griddy-Gibbs sampler.
\subsection{Form}
$$\textbf{Y}_{i,j} \sim \mathcal{N}(\mu_{\theta[i],j}, \sigma^2_{\theta[i]}), \quad i=1,\dots,N, \quad j=1,\dots,J$$
$$\theta_i \sim \mathcal{CAT}(\pi_{1:C}), \quad i=1,\dots,N$$
$$\pi_{1:C} \sim \mathrm{Stick}(\gamma)$$
$$\mu_{c,j} \sim \mathcal{N}(0, \nu^2_j)$$
$$\alpha \sim \mathcal{HC}(25)$$
$$\beta \sim \mathcal{HC}(25)$$
$$\gamma \sim \mathcal{G}(\alpha, \beta)$$
$$\sigma_c \sim \mathcal{HC}(25)$$
$$\nu_j \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{data(demonsnacks) \\
Y <- as.matrix(log(demonsnacks + 1)) \\
N <- nrow(Y) \\
J <- ncol(Y) \\
for (j in 1:J) \{Y[,j] <- CenterScale(Y[,j])\} \\
C <- 3 \#Number of clusters to explore \\
mon.names <- c("LP", paste("pi[", 1:C, "]", sep="")) \\
parm.names <- as.parm.names(list(theta=rep(0,N), mu=matrix(0,C,J), \\
\hspace*{0.27 in} nu=rep(0,J), sigma=rep(0,C), alpha=0, beta=0, gamma=0)) \\
pos.theta <- grep("theta", parm.names) \\
pos.mu <- grep("mu", parm.names) \\
pos.nu <- grep("nu", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
pos.alpha <- grep("alpha", parm.names) \\
pos.beta <- grep("beta", parm.names) \\
pos.gamma <- grep("gamma", parm.names) \\
PGF <- function(Data) return(c(rcat(Data$N, p=rep(1/Data$C,Data$C)), \\
\hspace*{0.27 in} rnorm(Data$C*Data$J,0,1), rhalfcauchy(Data$J+Data$C,5), \\
\hspace*{0.27 in} rhalfcauchy(2,5), rgamma(1,rhalfcauchy(2,5)))) \\
MyData <- list(C=C, J=J, N=N, PGF=PGF, Y=Y, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.theta=pos.theta, pos.mu=pos.mu, \\
\hspace*{0.27 in} pos.nu=pos.nu, pos.sigma=pos.sigma, pos.alpha=pos.alpha, \\
\hspace*{0.27 in} pos.beta=pos.beta, pos.gamma=pos.gamma) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperhyperparameters \\
\hspace*{0.27 in} alpha <- interval(parm[Data$pos.alpha], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.alpha] <- alpha \\
\hspace*{0.27 in} beta <- interval(parm[Data$pos.beta], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.beta] <- beta \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} gamma <- interval(parm[Data$pos.gamma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.nu] <- nu <- interval(parm[Data$pos.nu], 1e-100, Inf) \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} theta <- parm[Data$pos.theta] \\
\hspace*{0.27 in} mu <- matrix(parm[Data$pos.mu], Data$C, Data$J) \\
\hspace*{0.27 in} pi <- rStick(Data$C-1, gamma) \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} \#\#\# Log(Hyperhyperprior Densities) \\
\hspace*{0.27 in} alpha.prior <- dhalfcauchy(alpha, 25, log=TRUE) \\
\hspace*{0.27 in} beta.prior <- dhalfcauchy(beta, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log(Hyperprior Densities) \\
\hspace*{0.27 in} gamma.prior <- dgamma(gamma, alpha, beta, log=TRUE) \\
\hspace*{0.27 in} nu.prior <- sum(dhalfcauchy(nu, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} theta.prior <- sum(dcat(theta, pi, log=TRUE)) \\
\hspace*{0.27 in} mu.prior <- sum(dnorm(mu, 0, matrix(rep(nu,Data$C), Data$C, \\
\hspace*{0.62 in} Data$J, byrow=TRUE), log=TRUE)) \\
\hspace*{0.27 in} pi.prior <- dStick(pi, gamma, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$Y, mu[theta,], sigma[theta], log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + theta.prior + mu.prior + nu.prior + pi.prior + \\
\hspace*{0.62 in} alpha.prior + beta.prior + gamma.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,pi), \\
\hspace*{0.62 in} yhat=rnorm(prod(dim(mu[theta,])), mu[theta,], sigma[theta]), \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rcat(N,p=rep(1/C,C)), rep(0,C*J), rep(1,J), rep(1,C), \\
\hspace*{0.27 in} rep(1,3))}

\section{Conditional Autoregression (CAR), Poisson} \label{car.poisson}
This CAR example is a slightly modified form of example 7.3 (Model A) in \citet{congdon03}. The Scottish lip cancer data also appears in the WinBUGS \citep{spiegelhalter03} examples and is a widely analyzed example. The data $\textbf{y}$ consists of counts for $i=1,\dots,56$ counties in Scotland. A single predictor $\textbf{x}$ is provided. The errors, $\epsilon$, are allowed to include spatial effects as smoothing by spatial effects from areal neighbors. The vector $\epsilon_\mu$ is the mean of each area's error, and is a weighted average of errors in contiguous areas. Areal neighbors are indicated in adjacency matrix $\textbf{A}$.
\subsection{Form}
$$\textbf{y} \sim \mathcal{P}(\lambda)$$
$$\lambda = \exp(\log(\textbf{E}) + \beta_1 + \beta_2 \textbf{x} + \epsilon)$$
$$\epsilon \sim \mathcal{N}(\epsilon_\mu, \sigma^2)$$
$$\epsilon_{\mu[i]} = \rho \sum^J_{j=1} \textbf{A}_{i,j} \epsilon_j, \quad i=1,\dots,N$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\rho \sim \mathcal{U}(-1,1)$$
$$\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{N <- 56 \#Number of areas \\
NN <- 264 \#Number of adjacent areas \\
y <- c(9,39,11,9,15,8,26,7,6,20,13,5,3,8,17,9,2,7,9,7,16,31,11,7,19,15,7, \\
\hspace*{0.27 in} 10,16,11,5,3,7,8,11,9,11,8,6,4,10,8,2,6,19,3,2,3,28,6,1,1,1,1,0,0) \\
E <- c( 1.4,8.7,3.0,2.5,4.3,2.4,8.1,2.3,2.0,6.6,4.4,1.8,1.1,3.3,7.8,4.6, \\
\hspace*{0.27 in} 1.1,4.2,5.5,4.4,10.5,22.7,8.8,5.6,15.5,12.5,6.0,9.0,14.4,10.2,4.8, \\
\hspace*{0.27 in} 2.9,7.0,8.5,12.3,10.1,12.7,9.4,7.2,5.3,18.8,15.8,4.3,14.6,50.7,8.2, \\
\hspace*{0.27 in} 5.6,9.3,88.7,19.6,3.4,3.6,5.7,7.0,4.2,1.8) \#Expected \\
x <- c(16,16,10,24,10,24,10,7,7,16,7,16,10,24,7,16,10,7,7,10,7,16,10,7,1,1, \\
\hspace*{0.27 in} 7,7,10,10,7,24,10,7,7,0,10,1,16,0,1,16,16,0,1,7,1,1,0,1,1,0,1,1,16,10) \\
A <- matrix(0, N, N) \\
A[1,c(5,9,11,19)] <- 1 \#Area 1 is adjacent to areas 5, 9, 11, and 19 \\
A[2,c(7,10)] <- 1 \#Area 2 is adjacent to areas 7 and 10 \\
A[3,c(6,12)] <- 1; A[4,c(18,20,28)] <- 1; A[5,c(1,11,12,13,19)] <- 1 \\
A[6,c(3,8)] <- 1; A[7,c(2,10,13,16,17)] <- 1; A[8,6] <- 1 \\
A[9,c(1,11,17,19,23,29)] <- 1; A[10,c(2,7,16,22)] <- 1 \\
A[11,c(1,5,9,12)] <- 1; A[12,c(3,5,11)] <- 1; A[13,c(5,7,17,19)] <- 1 \\
A[14,c(31,32,35)] <- 1; A[15,c(25,29,50)] <- 1 \\
A[16,c(7,10,17,21,22,29)] <- 1; A[17,c(7,9,13,16,19,29)] <- 1 \\
A[18,c(4,20,28,33,55,56)] <- 1; A[19,c(1,5,9,13,17)] <- 1 \\
A[20,c(4,18,55)] <- 1; A[21,c(16,29,50)] <- 1; A[22,c(10,16)] <- 1 \\
A[23,c(9,29,34,36,37,39)] <- 1; A[24,c(27,30,31,44,47,48,55,56)] <- 1 \\
A[25,c(15,26,29)] <- 1; A[26,c(25,29,42,43)] <- 1 \\
A[27,c(24,31,32,55)] <- 1; A[28,c(4,18,33,45)] <- 1 \\
A[29,c(9,15,16,17,21,23,25,26,34,43,50)] <- 1 \\
A[30,c(24,38,42,44,45,56)] <- 1; A[31,c(14,24,27,32,35,46,47)] <- 1 \\
A[32,c(14,27,31,35)] <- 1; A[33,c(18,28,45,56)] <- 1 \\
A[34,c(23,29,39,40,42,43,51,52,54)] <- 1; A[35,c(14,31,32,37,46)] <- 1 \\
A[36,c(23,37,39,41)] <- 1; A[37,c(23,35,36,41,46)] <- 1 \\
A[38,c(30,42,44,49,51,54)] <- 1; A[39,c(23,34,36,40,41)] <- 1 \\
A[40,c(34,39,41,49,52)] <- 1; A[41,c(36,37,39,40,46,49,53)] <- 1 \\
A[42,c(26,30,34,38,43,51)] <- 1; A[43,c(26,29,34,42)] <- 1 \\
A[44,c(24,30,38,48,49)] <- 1; A[45,c(28,30,33,56)] <- 1 \\
A[46,c(31,35,37,41,47,53)] <- 1; A[47,c(24,31,46,48,49,53)] <- 1 \\
A[48,c(24,44,47,49)] <- 1; A[49,c(38,40,41,44,47,48,52,53,54)] <- 1 \\
A[50,c(15,21,29)] <- 1; A[51,c(34,38,42,54)] <- 1 \\
A[52,c(34,40,49,54)] <- 1; A[53,c(41,46,47,49)] <- 1 \\
A[54,c(34,38,49,51,52)] <- 1; A[55,c(18,20,24,27,56)] <- 1 \\
A[56,c(18,24,30,33,45,55)] <- 1 \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=rep(0,2), epsilon=rep(0,N), rho=0, \\
\hspace*{0.27 in} sigma=0)) \\
pos.beta <- grep("beta", parm.names) \\
pos.epsilon <- grep("epsilon", parm.names) \\
pos.rho <- grep("rho", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
PGF <- function(Data) return(c(rnormv(2,0,1000), rnorm(Data$N,0,1), \\
\hspace*{0.27 in} runif(1,-1,1), rhalfcauchy(1,5))) \\
MyData <- list(A=A, E=E, N=N, PGF=PGF, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.beta=pos.beta, pos.epsilon=pos.epsilon, \\
\hspace*{0.27 in} pos.rho=pos.rho, pos.sigma=pos.sigma, x=x, y=y)
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} epsilon <- parm[Data$pos.epsilon] \\
\hspace*{0.27 in} parm[Data$pos.rho] <- rho <- interval(parm[Data$pos.rho], -1, 1) \\
\hspace*{0.27 in} epsilon.mu <- rho * rowSums(epsilon * Data$A) \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} epsilon.prior <- sum(dnorm(epsilon, epsilon.mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} rho.prior <- dunif(rho, -1, 1, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} lambda <- exp(log(Data$E) + beta[1] + beta[2]*Data$x/10 + epsilon) \\
\hspace*{0.27 in} LL <- sum(dpois(Data$y, lambda, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + epsilon.prior + rho.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rpois(length(lambda), lambda), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,2), rep(0,N), 0, 1)}

\section{Conditional Predictive Ordinate} \label{cpo}
For a more complete introduction to the conditional predictive ordinate (CPO), see the vignette entitled ``Bayesian Inference''. Following is a brief guide to the applied use of CPO.

To include CPO in any model that is to be updated with MCMC, calculate and monitor the record-level inverse of the likelihood, $\mathrm{InvL}_i$ for records $i=1,\dots,N$. $\mathrm{CPO}_i$ is the inverse of the posterior mean of $\mathrm{InvL}_i$. The inverse $\mathrm{CPO}_i$, or $\mathrm{ICPO}_i$, is the posterior mean of $\mathrm{InvL}_i$. ICPOs larger than 40 can be considered as possible outliers, and higher than 70 as extreme values.

Here, CPO is added to the linear regression example in section \ref{linear.reg}. In this data, record 6 is a possible outlier, and record 8 is an extreme value.
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2)$$
$$\mu = \textbf{X}\beta$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{data(demonsnacks) \\
y <- log(demonsnacks$Calories) \\
X <- cbind(1, as.matrix(log(demonsnacks[,c(1,4,10)]+1))) \\
J <- ncol(X) \\
for (j in 2:J) \{X[,j] <- CenterScale(X[,j])\} \\
mon.names <- c("LP",as.parm.names(list(InvL=rep(0,N)))) \\
parm.names <- as.parm.names(list(beta=rep(0,J), sigma=0)) \\
pos.beta <- grep("beta", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
PGF <- function(Data) return(c(rnormv(Data$J,0,1000), rhalfcauchy(1,5))) \\
MyData <- list(J=J, PGF=PGF, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.beta=pos.beta, pos.sigma=pos.sigma, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, t(beta)) \\
\hspace*{0.27 in} LL <- dnorm(Data$y, mu, sigma, log=TRUE) \\
\hspace*{0.27 in} InvL <- 1 / exp(LL) \\
\hspace*{0.27 in} LL <- sum(LL) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,InvL), \\
\hspace*{0.62 in} yhat=rnorm(length(mu), mu, sigma), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), 1)}

\section{Contingency Table} \label{contingency.table}
The two-way contingency table, matrix $\textbf{Y}$, can easily be extended to more dimensions. Contingency table $\textbf{Y}$ has J rows and K columns. The cell counts are fit with Poisson regression, according to intercept $\alpha$, main effects $\beta_j$ for each row, main effects $\gamma_k$ for each column, and interaction effects $\delta_{j,k}$ for dependence effects. An omnibus (all cells) test of independence is done by estimating two models (one with $\delta$, and one without), and a large enough Bayes Factor indicates a violation of independence when the model with $\delta$ fits better than the model without $\delta$. In an ANOVA-like style, main effects contrasts can be used to distinguish rows or groups of rows from each other, as well as with columns. Likewise, interaction effects contrasts can be used to test independence in groups of $\delta_{j,k}$ elements. Finally, single-cell interactions can be used to indicate violations of independence for a given cell, such as when zero is not within its 95\% probability interval.
\subsection{Form}
$$\textbf{Y}_{j,k} \sim \mathcal{P}(\lambda_{j,k}), \quad j=1,\dots,J, \quad k=1,\dots,K$$
$$\lambda_{j,k} = \exp(\alpha + \beta_j + \gamma_k + \delta_{j,k}), \quad j=1,\dots,J, \quad k=1,\dots,K$$
$$\alpha \sim \mathcal{N}(0, 1000)$$
$$\beta_j \sim \mathcal{N}(0, \beta^2_\sigma), \quad j=1,\dots,J$$
$$\beta_J = - \displaystyle\sum^{J-1}_{j=1} \beta_j$$
$$\beta_\sigma \sim \mathcal{HC}(25)$$
$$\gamma_k \sim \mathcal{N}(0, \gamma^2_\sigma), \quad k=1,\dots,K$$
$$\gamma_K = - \displaystyle\sum^{K-1}_{k=1} \gamma_k$$
$$\gamma_\sigma \sim \mathcal{HC}(25)$$
$$\delta_{j,k} \sim \mathcal{N}(0, \delta^2_\sigma)$$
$$\delta_{J,K} = - \displaystyle\sum \delta_{-J,-K}$$
$$\delta_\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{J <- 3 \#Rows \\
K <- 3 \#Columns \\
alpha <- runif(1,10,20) \\
beta <- rnorm(J-1, 0, 2); beta <- c(beta, -sum(beta)) \\
gamma <- rnorm(K-1, 0, 2); gamma <- c(gamma, -sum(gamma)) \\
delta <- rnorm(J*K-1, 0, 0.5); delta <- c(delta, -sum(delta)) \\
Y <- matrix(alpha + matrix(beta, J, K) + matrix(gamma, J, K, byrow=TRUE) + \\
\hspace*{0.27 in} matrix(delta, J, K), J, K) \\
Y <- round(Y) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(alpha=0, beta=rep(0,J-1), \\
\hspace*{0.27 in} gamma=rep(0,K-1), delta=rep(0,J*K-1), b.sigma=0, g.sigma=0, \\
\hspace*{0.27 in} d.sigma=0)) \\
pos.alpha <- grep("alpha", parm.names) \\
pos.beta <- grep("beta", parm.names) \\
pos.gamma <- grep("gamma", parm.names) \\
pos.delta <- grep("delta", parm.names) \\
pos.b.sigma <- grep("b.sigma", parm.names) \\
pos.g.sigma <- grep("g.sigma", parm.names) \\
pos.d.sigma <- grep("d.sigma", parm.names) \\
PGF <- function(Data) return(c(rnorm(1,log(mean(Y)),1), rnorm(Data$J-1), \\
\hspace*{0.27 in} rnorm(Data$K-1), rnorm(Data$J*Data$K-1), rhalfcauchy(3,5))) \\
MyData <- list(J=J, K=K, PGF=PGF, Y=Y, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.alpha=pos.alpha, pos.beta=pos.beta, \\
\hspace*{0.27 in} pos.gamma=pos.gamma, pos.delta=pos.delta, pos.b.sigma=pos.b.sigma, \\
\hspace*{0.27 in} pos.g.sigma=pos.g.sigma, pos.d.sigma=pos.d.sigma) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} beta.sigma <- interval(parm[Data$pos.b.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.b.sigma] <- beta.sigma \\
\hspace*{0.27 in} gamma.sigma <- interval(parm[Data$pos.g.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.g.sigma] <- gamma.sigma \\
\hspace*{0.27 in} delta.sigma <- interval(parm[Data$pos.d.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.d.sigma] <- delta.sigma \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[Data$pos.alpha] \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} beta <- c(beta, -sum(beta)) \\
\hspace*{0.27 in} gamma <- parm[Data$pos.gamma] \\
\hspace*{0.27 in} gamma <- c(gamma, -sum(gamma)) \\
\hspace*{0.27 in} delta <- parm[Data$pos.delta] \\
\hspace*{0.27 in} delta <- c(delta, -sum(delta)) \\
\hspace*{0.27 in} delta <- matrix(delta, Data$J, Data$K) \\
\hspace*{0.27 in} \#\#\# Log(Hyperprior Densities) \\
\hspace*{0.27 in} beta.sigma.prior <- dhalfcauchy(beta.sigma, 25, log=TRUE) \\
\hspace*{0.27 in} gamma.sigma.prior <- dhalfcauchy(gamma.sigma, 25, log=TRUE) \\
\hspace*{0.27 in} delta.sigma.prior <- dhalfcauchy(delta.sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnormv(alpha, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, beta.sigma, log=TRUE)) \\
\hspace*{0.27 in} gamma.prior <- sum(dnorm(gamma, 0, gamma.sigma, log=TRUE)) \\
\hspace*{0.27 in} delta.prior <- sum(dnorm(delta, 0, delta.sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} beta <- matrix(beta, Data$J, Data$K) \\
\hspace*{0.27 in} gamma <- matrix(gamma, Data$J, Data$K, byrow=TRUE) \\
\hspace*{0.27 in} lambda <- exp(alpha + beta + gamma + delta) \\
\hspace*{0.27 in} LL <- sum(dpois(Data$Y, lambda, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior + beta.sigma.prior + \\
\hspace*{0.62 in} gamma.prior + gamma.sigma.prior + delta.prior + \\
\hspace*{0.62 in} delta.sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rpois(length(lambda), lambda), \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(log(mean(Y)), rep(0,J-1), rep(0,K-1), rep(0,J*K-1), \\
\hspace*{0.27 in} rep(1,3)) \\}

\section{Covariance Separation Strategy} \label{cov.sep.strat}
A Seemingly Unrelated Regression (SUR) model is used to provide an example of a flexible way to estimate covariance or precision matrices with the ``separation strategy'' decomposition of \citet{barnard00}. For more information on SUR models, see section \ref{sur}.

The most common way of specifying a covariance matrix, such as for the multivariate normal distribution, may be with the conjugate inverse Wishart distribution. Alternatively, the conjugate Wishart distribution is often used for a precision matrix. The Wishart and inverse Wishart distributions, however, do not always perform well, due to only one parameter for variability, and usually in the case of small sample sizes or when its dimension approaches the sample size. There are several alternatives. This example decomposes a covariance matrix into a standard deviation vector and a correlation matrix, each of which are easy to understand (as opposed to setting priors on eigenvalues). A precision matrix may be decomposed similarly, though the separated components are interpreted differently.

\citet{barnard00} prefer to update the covariance separation strategy with Gibbs sampling rather than Metropolis-Hastings, though the form presented here works well in testing with Adaptive MCMC.
\subsection{Form}
$$\textbf{Y}_{t,j} \sim \mathcal{N}_J(\mu_{t,j}, \Sigma), \quad t=1,\dots,T; \quad j=1,\dots,J$$
$$\mu_{t,1} = \alpha_1 + \alpha_2 \textbf{X}_{t-1,1} +  \alpha_3 \textbf{X}_{t-1,2}, \quad t=2,\dots,T$$
$$\mu_{t,2} = \beta_1 + \beta_2 \textbf{X}_{t-1,3} +  \beta_3 \textbf{X}_{t-1,4}, \quad t=2,\dots,T$$
$$\Sigma = \textbf{S} \textbf{R} \textbf{S}$$
$$\alpha_k \sim \mathcal{N}(0, 1000), \quad k=1,\dots,K$$
$$\beta_k \sim \mathcal{N}(0, 1000), \quad k=1,\dots,K$$
$$\textbf{R}_{i,j} \sim \mathcal{N}(\rho_\mu, \rho^2_\sigma), \quad \textbf{R}_{i,j} \in [-1,1], \quad i=1,\dots,J$$
$$\textbf{S} = \sigma \textbf{I}_J$$
$$\rho_\mu \sim \mathcal{N}(0, 2), \quad \in [-1, 1]$$
$$\rho_\sigma \sim \mathcal{HC}(25), \quad \in (0, 1000]$$
$$\sigma_j \sim \mathcal{N}(\sigma_\mu, \sigma_\sigma)$$
$$\sigma_\mu \sim \mathcal{HN}(1000), \quad \in (0, 1000]$$
$$\sigma_\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{T <- 20 \#Time-periods \\
year <- c(1935,1936,1937,1938,1939,1940,1941,1942,1943,1944,1945,1946, \\
\hspace*{0.27 in} 1947,1948,1949,1950,1951,1952,1953,1954) \\
IG <- c(33.1,45.0,77.2,44.6,48.1,74.4,113.0,91.9,61.3,56.8,93.6,159.9, \\
\hspace*{0.27 in} 147.2,146.3,98.3,93.5,135.2,157.3,179.5,189.6) \\
VG <- c(1170.6,2015.8,2803.3,2039.7,2256.2,2132.2,1834.1,1588.0,1749.4, \\
\hspace*{0.27 in} 1687.2,2007.7,2208.3,1656.7,1604.4,1431.8,1610.5,1819.4,2079.7, \\
\hspace*{0.27 in} 2371.6,2759.9) \\
CG <- c(97.8,104.4,118.0,156.2,172.6,186.6,220.9,287.8,319.9,321.3,319.6, \\
\hspace*{0.27 in} 346.0,456.4,543.4,618.3,647.4,671.3,726.1,800.3,888.9) \\
IW <- c(12.93,25.90,35.05,22.89,18.84,28.57,48.51,43.34,37.02,37.81, \\
\hspace*{0.27 in} 39.27,53.46,55.56,49.56,32.04,32.24,54.38,71.78,90.08,68.60) \\
VW <- c(191.5,516.0,729.0,560.4,519.9,628.5,537.1,561.2,617.2,626.7, \\
\hspace*{0.27 in} 737.2,760.5,581.4,662.3,583.8,635.2,723.8,864.1,1193.5,1188.9) \\
CW <- c(1.8,0.8,7.4,18.1,23.5,26.5,36.2,60.8,84.4,91.2,92.4,86.0,111.1, \\
\hspace*{0.27 in} 130.6,141.8,136.7,129.7,145.5,174.8,213.5) \\
J <- 2 \#Number of dependent variables \\
Y <- matrix(c(IG,IW), T, J) \\
R <- diag(J) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(alpha=rep(0,3), beta=rep(0,3), \\
\hspace*{0.27 in} R=diag(J), rho.mu=0, rho.sigma=0, log.sigma=rep(0,J), sigma.mu=0, \\
\hspace*{0.27 in} log.sig.sigma=0), uppertri=c(0,0,1,0,0,0,0,0)) \\
pos.alpha <- grep("alpha", parm.names) \\
pos.beta <- grep("beta", parm.names) \\
pos.R <- grep("R", parm.names) \\
pos.rho.mu <- grep("rho.mu", parm.names) \\
pos.rho.sigma <- grep("rho.sigma", parm.names) \\
pos.log.sigma <- grep("log.sigma", parm.names) \\
pos.sigma.mu <- grep("sigma.mu", parm.names) \\
pos.log.sig.sigma <- grep("log.sig.sigma", parm.names) \\
PGF <- function(Data) return(c(rnormv(3,0,10), rnormv(3,0,10), \\
\hspace*{0.27 in} runif(length(upper.triangle(diag(Data$J), diag=TRUE)), -1, 1), \\
\hspace*{0.27 in} rtrunc(1, "norm", a=-1, b=1, mean=0, sd=2), \\
\hspace*{0.27 in} log(rhalfcauchy(Data$J+1,5)), rhalfnorm(1, 10), \\
\hspace*{0.27 in} log(rhalfcauchy(1,5)))) \\
MyData <- list(J=J, PGF=PGF, T=T, Y=Y, CG=CG, CW=CW, IG=IG, IW=IW, VG=VG, \\
\hspace*{0.27 in} VW=VW, mon.names=mon.names, parm.names=parm.names, ) \\
\hspace*{0.27 in} pos.alpha=pos.alpha, pos.beta=pos.beta, pos.R=pos.R, \\
\hspace*{0.27 in} pos.rho.mu=pos.rho.mu, pos.rho.sigma=pos.rho.sigma, \\
\hspace*{0.27 in} pos.log.sigma=pos.log.sigma, pos.sigma.mu=pos.sigma.mu, \\
\hspace*{0.27 in} pos.log.sig.sigma=pos.log.sig.sigma) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} rho.mu <- interval(parm[Data$pos.rho.mu], -1, 1) \\
\hspace*{0.27 in} parm[Data$pos.rho.mu] <- rho.mu \\
\hspace*{0.27 in} rho.sigma <- interval(parm[Data$pos.rho.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.rho.sigma] <- rho.sigma \\
\hspace*{0.27 in} sigma.mu <- interval(parm[Data$pos.sigma.mu], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma.mu] <- sigma.mu \\
\hspace*{0.27 in} sigma.sigma <- sigma.sigma <- exp(parm[Data$pos.log.sig.sigma]) \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[Data$pos.alpha] \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} R <- as.parm.matrix(R, Data$J, parm, Data, a=-1, b=1) \\
\hspace*{0.27 in} parm[Data$pos.R] <- upper.triangle(R, diag=TRUE) \\
\hspace*{0.27 in} sigma <- exp(parm[Data$pos.log.sigma]) \\
\hspace*{0.27 in} S <- diag(sigma) \\
\hspace*{0.27 in} Sigma <- as.symmetric.matrix(S \%*\% R \%*\% S) \\
\hspace*{0.27 in} \#\#\# Log(Hyperprior Densities) \\
\hspace*{0.27 in} rho.mu.prior <- dtrunc(rho.mu, "norm", a=-1, b=1, mean=0, sd=2, \\
\hspace*{0.62 in} log=TRUE) \\
\hspace*{0.27 in} rho.sigma.prior <- dhalfcauchy(rho.sigma, 25, log=TRUE) \\
\hspace*{0.27 in} sigma.mu.prior <- dhalfnorm(sigma.mu, 1000, log=TRUE) \\
\hspace*{0.27 in} sigma.sigma.prior <- dhalfcauchy(sigma.sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- sum(dnormv(alpha, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} R.prior <- sum(dtrunc(upper.triangle(R, diag=TRUE), "norm", \\
\hspace*{0.62 in} a=-1, b=1, mean=rho.mu, sd=rho.sigma, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dnorm(sigma, sigma.mu, sigma.sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- Data$Y \\
\hspace*{0.27 in} mu[-1,1] <- alpha[1] + alpha[2]*Data$CG[-Data$T] + \\
\hspace*{0.62 in} alpha[3]*Data$VG[-Data$T] \\
\hspace*{0.27 in} mu[-1,2] <- beta[1] + beta[2]*Data$CW[-Data$T] + \\
\hspace*{0.62 in} beta[3]*Data$VW[-Data$T] \\
\hspace*{0.27 in} LL <- sum(dmvn(Data$Y[-1,], mu[-1,], Sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior + R.prior + rho.mu.prior + \\
\hspace*{0.62 in} rho.sigma.prior + sigma.prior + sigma.mu.prior + \\
\hspace*{0.62 in} sigma.sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rmvn(nrow(mu), mu, Sigma), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,3), rep(0,3), upper.triangle(R, diag=TRUE),
\hspace*{0.27 in} rep(0,2), rep(0,J), rep(1,2))}

\section{Discrete Choice, Conditional Logit} \label{conditional.logit}
\subsection{Form}
$$\textbf{y}_i \sim \mathcal{CAT}(\textbf{p}_{i,1:J}), \quad i=1,\dots,N, \quad j=1,\dots,J$$
$$\textbf{p}_{i,j} = \frac{\phi_{i,j}}{\sum^J_{j=1} \phi_{i,j}}$$
$$\phi = \exp(\mu)$$
$$\mu_{i,j} = \beta_{j,1:K} \textbf{X}_{i,1:K} + \gamma \textbf{Z}_{i,1:C} \in [-700,700], \quad j=1,\dots,(J-1)$$
$$\mu_{i,J} = \gamma \textbf{Z}_{i,1:C}$$
$$\beta_{j,k} \sim \mathcal{N}(0, 1000), \quad j=1,\dots,(J-1)$$
$$\gamma_c \sim \mathcal{N}(0, 1000)$$
\subsection{Data}
\code{y <- x01 <- x02 <- z01 <- z02 <- c(1:300) \\
y[1:100] <- 1 \\
y[101:200] <- 2 \\
y[201:300] <- 3 \\
x01[1:100] <- rnorm(100, 25, 2.5) \\
x01[101:200] <- rnorm(100, 40, 4.0) \\
x01[201:300] <- rnorm(100, 35, 3.5) \\
x02[1:100] <- rnorm(100, 2.51, 0.25) \\
x02[101:200] <- rnorm(100, 2.01, 0.20) \\
x02[201:300] <- rnorm(100, 2.70, 0.27) \\
z01[1:100] <- 1 \\
z01[101:200] <- 2 \\
z01[201:300] <- 3 \\
z02[1:100] <- 40 \\
z02[101:200] <- 50 \\
z02[201:300] <- 100 \\
N <- length(y) \\
J <- 3 \#Number of categories in y \\
K <- 3 \#Number of individual attributes (including the intercept) \\
C <- 2 \#Number of choice-based attributes (intercept is not included) \\
X <- matrix(c(rep(1,N),x01,x02),N,K) \#Design matrix of individual attrib. \\
Z <- matrix(c(z01,z02),N,C) \#Design matrix of choice-based attributes \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=matrix(0,J-1,K), gamma=rep(0,C))) \\
pos.beta <- grep("beta", parm.names) \\
pos.gamma <- grep("gamma", parm.names) \\
PGF <- function(Data) return(c(rnormv((Data$J-1)*Data$K,0,10), \\
\hspace*{0.27 in} rnormv(Data$C,0,10))) \\
MyData <- list(C=C, J=J, K=K, N=N, PGF=PGF, X=X, Z=Z, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.beta=pos.beta, pos.gamma=pos.gamma, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- matrix(parm[Data$pos.beta], Data$J-1, Data$K) \\
\hspace*{0.27 in} gamma <- parm[Data$pos.gamma] \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} gamma.prior <- sum(dnormv(gamma, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- matrix(rep(tcrossprod(gamma, Data$Z), Data$J), Data$N, Data$J) \\
\hspace*{0.27 in} mu[,-Data$J] <- mu[,-Data$J] + tcrossprod(Data$X, beta) \\
\hspace*{0.27 in} mu <- interval(mu, -700, 700, reflect=FALSE) \\
\hspace*{0.27 in} phi <- exp(mu) \\
\hspace*{0.27 in} p <- phi / rowSums(phi) \\
\hspace*{0.27 in} LL <- sum(dcat(Data$y, p, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + gamma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=rcat(nrow(p), p), \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,(J-1)*K), rep(0,C))}

\section{Discrete Choice, Mixed Logit} \label{mixed.logit}
\subsection{Form}
$$\textbf{y}_i \sim \mathcal{CAT}(\textbf{p}_{i,1:J}), \quad i=1,\dots,N$$
$$\textbf{p}_{i,j} = \frac{\phi_{i,j}}{\sum^J_{j=1} \phi_{i,j}}$$
$$\phi = \exp(\mu)$$
$$\mu_{i,j} = \beta_{j,1:K} \textbf{X}_{i,1:K} + \gamma \textbf{Z}_{i,1:C} \in [-700,700], \quad j=1,\dots,(J-1)$$
$$\mu_{i,J} = \gamma \textbf{Z}_{i,1:C}$$
$$\beta_{j,k} \sim \mathcal{N}(0, 1000), \quad j=1,\dots,(J-1)$$
$$\gamma_c \sim \mathcal{N}(\zeta_{\mu[c]}, \zeta^2_{\sigma[c]})$$
$$\zeta_{\mu[c]} \sim \mathcal{N}(0, 1000)$$
$$\zeta_{\sigma[c]} \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{y <- x01 <- x02 <- z01 <- z02 <- c(1:300) \\
y[1:100] <- 1 \\
y[101:200] <- 2 \\
y[201:300] <- 3 \\
x01[1:100] <- rnorm(100, 25, 2.5) \\
x01[101:200] <- rnorm(100, 40, 4.0) \\
x01[201:300] <- rnorm(100, 35, 3.5) \\
x02[1:100] <- rnorm(100, 2.51, 0.25) \\
x02[101:200] <- rnorm(100, 2.01, 0.20) \\
x02[201:300] <- rnorm(100, 2.70, 0.27) \\
z01[1:100] <- 1 \\
z01[101:200] <- 2 \\
z01[201:300] <- 3 \\
z02[1:100] <- 40 \\
z02[101:200] <- 50 \\
z02[201:300] <- 100 \\
N <- length(y) \\
J <- 3 \#Number of categories in y \\
K <- 3 \#Number of individual attributes (including the intercept) \\
C <- 2 \#Number of choice-based attributes (intercept is not included) \\
X <- matrix(c(rep(1,N),x01,x02),N,K) \#Design matrix of individual attrib. \\
Z <- matrix(c(z01,z02),N,C) \#Design matrix of choice-based attributes \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=matrix(0,J-1,K), gamma=rep(0,C), \\
\hspace*{0.27 in} zeta.mu=rep(0,C), zeta.sigma=rep(0,C))) \\
pos.beta <- grep("beta", parm.names) \\
pos.gamma <- grep("gamma", parm.names) \\
pos.zeta.mu <- grep("zeta.mu", parm.names) \\
pos.zeta.sigma <- grep("zeta.sigma", parm.names) \\
PGF <- function(Data) return(c(rnormv((Data$J-1)*Data$K,0,1000), \\
\hspace*{0.27 in} rnorm(Data$N*Data$C, \\
\hspace*{0.27 in} matrix(rnormv(Data$C,0,1000), Data$N, Data$C, byrow=TRUE), \\
\hspace*{0.27 in} matrix(rhalfcauchy(Data$C,5), Data$N, Data$C, byrow=TRUE)), \\
\hspace*{0.27 in} rnormv(Data$C,0,1000), rhalfcauchy(Data$C,5))) \\
MyData <- list(C=C, J=J, K=K, N=N, X=X, Z=Z, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.beta=pos.beta, pos.gamma=pos.gamma, \\
\hspace*{0.27 in} pos.zeta.mu=pos.zeta.mu, pos.zeta.sigma=pos.zeta.sigma, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- matrix(parm[Data$pos.beta], Data$J-1, Data$K) \\
\hspace*{0.27 in} gamma <- parm[Data$pos.gamma] \\
\hspace*{0.27 in} zeta.mu <- parm[Data$pos.zeta.mu] \\
\hspace*{0.27 in} zeta.sigma <- interval(parm[Data$pos.zeta.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.zeta.sigma] <- zeta.sigma \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} gamma.prior <- sum(dnorm(gamma, matrix(zeta.mu, Data$N, Data$C, \\
\hspace*{0.62 in} byrow=TRUE), matrix(zeta.sigma, Data$N, Data$C, byrow=TRUE), \\
\hspace*{0.27 in} log=TRUE)) \\
\hspace*{0.27 in} zeta.mu.prior <- sum(dnormv(zeta.mu, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} zeta.sigma.prior <- sum(dhalfcauchy(zeta.sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- matrix(rep(rowSums(gamma * Data$Z),Data$J), Data$N, Data$J) \\
\hspace*{0.27 in} mu[,-Data$J] <- tcrossprod(Data$X, beta) + gamma * Data$Z \\
\hspace*{0.27 in} mu <- interval(mu, -700, 700, reflect=FALSE) \\
\hspace*{0.27 in} phi <- exp(mu) \\
\hspace*{0.27 in} p <- phi / rowSums(phi) \\
\hspace*{0.27 in} LL <- sum(dcat(Data$y, p, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + gamma.prior + zeta.mu.prior + zeta.sigma.prior\\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=rcat(nrow(p), p), \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,(J-1)*K), rep(0,N*C), rep(0,C), rep(1,C))}

\section{Discrete Choice, Multinomial Probit} \label{dc.mnp}
\subsection{Form}
$$\textbf{W}_{i,1:(J-1)} \sim \mathcal{N}_{J-1}(\mu_{i,1:(J-1)}, \Sigma), \quad i=1,\dots,N$$
\[\textbf{W}_{i,j} \in \left\{
\begin{array}{l l}
 $[0,10]$ & \quad \mbox{if $\textbf{y}_i = j$}\\
 $[-10,0]$ \\ \end{array} \right. \]
$$\mu_{1:N,j} = \textbf{X} \beta_{j,1:K} + \textbf{Z} \gamma$$
$$\Sigma = \textbf{U}^T \textbf{U}$$
$$\beta_{j,k} \sim \mathcal{N}(0, 10), \quad j=1,\dots,(J-1), \quad k=1,\dots,K$$
$$\gamma_c \sim \mathcal{N}(0, 10), \quad c=1,\dots,C$$
$$\textbf{U}_{j,k} \sim \mathcal{N}(0,1), \quad j=1,\dots,(J-1), \quad k=1,\dots,(J-1), \quad j \ge k, \quad j \ne k = 1$$
\subsection{Data}
\code{N <- 50 \\
J <- 5 \#Categories of y \\
K <- 8 \#Number of columns in design matrix X \\
C <- 2 \#Number of choice-based attributes \\
X <- matrix(runif(N*K,-2,2), N, K) \\
X[,1] <- 1 \\
beta <- matrix(runif((J-1)*K), J-1, K) \\
gamma <- runif(C) \\
Z <- matrix(runif(N*C), N, C) \#Design matrix of choice-based attributes \\
Z[,1] <- 1 \\
mu <- tcrossprod(X, beta) + as.vector(tcrossprod(Z, t(gamma))) \\
S <- diag(J-1) \\
u <- c(0, rnorm((J-2) + (factorial(J-1) / \\
\hspace*{0.27 in} (factorial(J-1-2)*factorial(2))),0,1)) \\
U <- diag(J-1) \\
U[upper.tri(U, diag=TRUE)] <- u \\
diag(U) <- exp(diag(U)) \\
Sigma <- t(U) \%*\% U \\
Sigma[1,] <- Sigma[,1] <- U[1,] \\
mu <- tcrossprod(X, beta) \\
W <- rmvn(N, mu, Sigma) + matrix(rnorm(N*(J-1),0,0.1), N, J-1) \\
y <- max.col(cbind(W,0)) \\
table(y) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=matrix(0,(J-1),K), gamma=rep(0,C), \\
\hspace*{0.27 in} U=U, W=matrix(0,N,J-1)), uppertri=c(0,0,1,0)) \\
parm.names <- parm.names[-which(parm.names == "U[1,1]")] \\
pos.beta <- grep("beta", parm.names) \\
pos.gamma <- grep("gamma", parm.names) \\
pos.U <- grep("U", parm.names) \\
pos.W <- grep("W", parm.names) \\
PGF <- function(Data) \{ \\
\hspace*{0.27 in} beta <- rnormv((Data$J-1)*Data$K,0,1) \\
\hspace*{0.27 in} gamma <- rnormv(Data$C,0,1) \\
\hspace*{0.27 in} U <- rnorm((Data$J-2) + (factorial(Data$J-1) / \\
\hspace*{0.62 in} (factorial(Data$J-1-2)*factorial(2))),0,1) \\
\hspace*{0.27 in} W <- matrix(runif(Data$N*(Data$J-1),-10,0), Data$N, Data$J-1) \\
\hspace*{0.27 in} Y <- as.indicator.matrix(Data$y) \\
\hspace*{0.27 in} W <- ifelse(Y[,-Data$J] == 1, abs(W), W) \\
\hspace*{0.27 in} return(c(beta, gamma, U, as.vector(W)))\} \\
MyData <- list(C=C, J=J, K=K, N=N, PGF=PGF, S=S, X=X, Z=Z, \\
\hspace*{0.27 in} mon.names=mon.names, parm.names=parm.names, pos.beta=pos.beta, \\
\hspace*{0.27 in} pos.gamma=pos.gamma, pos.U=pos.U, pos.W=pos.W, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- matrix(parm[Data$pos.beta], Data$J-1, Data$K) \\
\hspace*{0.27 in} gamma <- parm[Data$pos.gamma] \\
\hspace*{0.27 in} u <- c(0, parm[Data$pos.U]) \\
\hspace*{0.27 in} U <- diag(Data$J-1) \\
\hspace*{0.27 in} U[upper.tri(U, diag=TRUE)] <- u \\
\hspace*{0.27 in} diag(U) <- exp(diag(U)) \\
\hspace*{0.27 in} Sigma <- t(U) \%*\% U \\
\hspace*{0.27 in} Sigma[1,] <- Sigma[,1] <- U[1,] \\
\hspace*{0.27 in} W <- matrix(parm[Data$pos.W], Data$N, Data$J-1) \\
\hspace*{0.27 in} Y <- as.indicator.matrix(Data$y) \\
\hspace*{0.27 in} temp <- which(Y[,-c(Data$J)] == 1) \\
\hspace*{0.27 in} W[temp] <- interval(W[temp], 0, 10) \\
\hspace*{0.27 in} temp <- which(Y[,-c(Data$J)] == 0) \\
\hspace*{0.27 in} W[temp] <- interval(W[temp], -10, 0) \\
\hspace*{0.27 in} parm[Data$pos.W] <- as.vector(W) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 10, log=TRUE)) \\
\hspace*{0.27 in} gamma.prior <- sum(dnormv(gamma, 0, 10, log=TRUE)) \\
\hspace*{0.27 in} U.prior <- sum(dnorm(u[-length(u)], 0, 1, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, beta) + \\
\hspace*{0.62 in} as.vector(tcrossprod(Data$Z, t(gamma))) \\
\hspace*{0.27 in} \#eta <- exp(cbind(mu,0)) \\
\hspace*{0.27 in} \#p <- eta / rowSums(eta) \\
\hspace*{0.27 in} LL <- sum(dmvn(W, mu, Sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + gamma.prior + U.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=max.col(cbind(rmvn(nrow(mu), mu, Sigma),0)), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- GIV(Model, MyData, PGF=TRUE)}

\section{Distributed Lag, Koyck} \label{dl.koyck}
This example applies Koyck or geometric distributed lags to $k=1,\dots,K$ discrete events in covariate $\textbf{x}$, transforming the covariate into a $N$ x $K$ matrix $\textbf{X}$ and creates a $N$ x $K$ lag matrix $\textbf{L}$.
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2)$$
$$\mu_t = \alpha + \phi \textbf{y}_{t-1} + \sum^K_{k=1} \textbf{X}_{t,k} \beta \lambda^{\textbf{L}[t,k]}, \quad k=1,\dots,K, \quad t=2,\dots,T$$
$$\mu_1 = \alpha + \sum^K_{k=1} \textbf{X}_{1,k} \beta \lambda^{\textbf{L}[1,k]}, \quad k=1,\dots,K$$
$$\alpha \sim \mathcal{N}(0, 1000)$$
$$\beta \sim \mathcal{N}(0, 1000)$$
$$\lambda \sim \mathcal{U}(0, 1)$$
$$\phi \sim \mathcal{N}(0, 1000)$$
$$\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{y <- c(0.02, -0.51, -0.30,  1.46, -1.26, -2.15, -0.91, -0.53, -1.91, \\
\hspace*{0.27 in}  2.64,  1.64,  0.15,  1.46,  1.61,  1.96, -2.67, -0.19, -3.28, \\
\hspace*{0.27 in}  1.89,  0.91, -0.71,  0.74, -0.10,  3.20, -0.80, -5.25,  1.03, \\
\hspace*{0.27 in} -0.40, -1.62, -0.80,  0.77,  0.17, -1.39, -1.28,  0.48, -1.02, \\
\hspace*{0.27 in}  0.09, -1.09,  0.86,  0.36,  1.51, -0.02,  0.47,  0.62, -1.36, \\
\hspace*{0.27 in}  1.12,  0.42, -4.39, -0.87,  0.05, -5.41, -7.38, -1.01, -1.70, \\
\hspace*{0.27 in}  0.64,  1.16,  0.87,  0.28, -1.69, -0.29,  0.13, -0.65,  0.83, \\
\hspace*{0.27 in}  0.62,  0.05, -0.14,  0.01, -0.36, -0.32, -0.80, -0.06,  0.24, \\
\hspace*{0.27 in}  0.23, -0.37,  0.00, -0.33,  0.21, -0.10, -0.10, -0.01, -0.40, \\
\hspace*{0.27 in} -0.35,  0.48, -0.28,  0.08,  0.28,  0.23,  0.27, -0.35, -0.19, \\
\hspace*{0.27 in}  0.24,  0.17, -0.02, -0.23,  0.03,  0.02, -0.17,  0.04, -0.39, \\
\hspace*{0.27 in} -0.12,  0.16,  0.17,  0.00,  0.18,  0.06, -0.36,  0.22,  0.14, \\
\hspace*{0.27 in} -0.17,  0.10, -0.01,  0.00, -0.18, -0.02,  0.07, -0.06,  0.06, \\
\hspace*{0.27 in} -0.05, -0.08, -0.07,  0.01, -0.06,  0.01,  0.01, -0.02,  0.01, \\
\hspace*{0.27 in}  0.01,  0.12, -0.03,  0.08, -0.10,  0.01, -0.03, -0.08,  0.04, \\
\hspace*{0.27 in} -0.09, -0.08,  0.01, -0.05,  0.08, -0.14,  0.06, -0.11,  0.09, \\
\hspace*{0.27 in}  0.06, -0.12, -0.01, -0.05, -0.15, -0.05, -0.03,  0.04,  0.00, \\
\hspace*{0.27 in} -0.12,  0.04, -0.06, -0.05, -0.07, -0.05, -0.14, -0.05, -0.01, \\
\hspace*{0.27 in} -0.12,  0.05,  0.06, -0.10,  0.00,  0.01,  0.00, -0.08,  0.00, \\
\hspace*{0.27 in}  0.00,  0.07, -0.01,  0.00,  0.09,  0.33,  0.13,  0.42,  0.24, \\
\hspace*{0.27 in} -0.36,  0.22, -0.09, -0.19, -0.10, -0.08, -0.07,  0.05,  0.07, \\
\hspace*{0.27 in}  0.07,  0.00, -0.04, -0.05,  0.03,  0.08,  0.26,  0.10,  0.08, \\
\hspace*{0.27 in}  0.09, -0.07, -0.33,  0.17, -0.03,  0.07, -0.04, -0.06, -0.06, \\
\hspace*{0.27 in}  0.07, -0.03,  0.00,  0.08,  0.27,  0.11,  0.11,  0.06, -0.11, \\
\hspace*{0.27 in} -0.09, -0.21,  0.24, -0.12,  0.11, -0.02, -0.03,  0.02, -0.10, \\
\hspace*{0.27 in}  0.00, -0.04,  0.01,  0.02, -0.03, -0.10, -0.09,  0.17,  0.07, \\
\hspace*{0.27 in} -0.05, -0.01, -0.05,  0.01,  0.00, -0.08, -0.05, -0.08,  0.07, \\
\hspace*{0.27 in}  0.06, -0.14,  0.02,  0.01,  0.04,  0.00, -0.13, -0.17) \\
x <- c(0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, \\
\hspace*{0.27 in} 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \\
\hspace*{0.27 in} 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \\
\hspace*{0.27 in} 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \\
\hspace*{0.27 in} 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \\
\hspace*{0.27 in} 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \\
\hspace*{0.27 in} 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \\
\hspace*{0.27 in} 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \\
\hspace*{0.27 in} 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \\
\hspace*{0.27 in} 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \\
\hspace*{0.27 in} 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0) \\
T <- length(y) \\
K <- length(which(x != 0)) \\
L <- X <- matrix(0, T, K) \\
for (i in 1:K) \{ \\
\hspace*{0.27 in} X[which(x != 0)[i]:T,i] <- x[which(x != 0)[i]] \\
\hspace*{0.27 in} L[(which(x != 0)[i]):T,i] <- 0:(T - which(x != 0)[i])\} \\
mon.names <- "LP" \\
parm.names <- c("alpha","beta","lambda","phi","sigma") \\
PGF <- function(Data) return(c(rnormv(2,0,1000), runif(1), \\
\hspace*{0.27 in} rnormv(1,0,1000), rhalfcauchy(1,5))) \\
MyData <- list(L=L, PGF=PGF, T=T, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1]; beta <- parm[2] \\
\hspace*{0.27 in} parm[3] <- lambda <- interval(parm[3], 0, 1) \\
\hspace*{0.27 in} phi <- parm[4] \\
\hspace*{0.27 in} parm[5] <- sigma <- interval(parm[5], 1e-100, Inf) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnormv(alpha, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} beta.prior <- dnormv(beta, 0, 1000, log=TRUE) \\ 
\hspace*{0.27 in} lambda.prior <- dunif(lambda, 0, 1, log=TRUE) \\
\hspace*{0.27 in} phi.prior <- dnormv(phi, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- c(alpha, alpha + phi*Data$y[-Data$T]) + \\
\hspace*{0.62 in} rowSums(Data$X * beta * lambda\textasciicircum Data$L) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior + lambda.prior + phi.prior + \\
\hspace*{0.62 in} sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rnorm(length(mu), mu, sigma), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,2), 0.5, 0, 1)}

\section{Exponential Smoothing} \label{exp.smo}
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2)$$
$$\mu_t = \alpha \textbf{y}_{t-1} + (1 - \alpha) \mu_{t-1}, \quad t=2,\dots,T$$
$$\alpha \sim \mathcal{U}(0,1)$$
$$\sigma \sim \mathcal{HC}$$
\subsection{Data}
\code{T <- 10 \\
y <- rep(0,T) \\
y[1] <- 0 \\
for (t in 2:T) \{y[t] <- y[t-1] + rnorm(1,0,0.1)\} \\
mon.names <- "LP" \\
parm.names <- c("alpha","sigma") \\
PGF <- function(Data) return(c(runif(1), rhalfcauchy(1,5))) \\
MyData <- list(PGF=PGF, T=T, mon.names=mon.names, parm.names=parm.names, \\
\hspace*{0.27 in} y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} parm[1] <- alpha <- interval(parm[1], 0, 1) \\
\hspace*{0.27 in} parm[2] <- sigma <- interval(parm[2], 1e-100, Inf) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dunif(alpha, 0, 1, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- y \\
\hspace*{0.27 in} mu[-1] <- alpha*Data$y[-1] \\
\hspace*{0.27 in} mu[-1] <- mu[-1] + (1 - alpha) * mu[-Data$T] \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y[-1], mu[-Data$T], sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rnorm(length(mu), mu, sigma), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(0.5, 1)}

\section{Factor Analysis, Approximate Dynamic} \label{adfa}
The Approximate Dynamic Factor Analysis (ADFA) model has many names, including the approximate factor model and approximate dynamic factor model. An ADFA is a Dynamic Factor Analysis (DFA) in which the factor scores of the dynamic factors are approximated with principal components. This is a combination of principal components and common factor analysis, in which the factor loadings of common factors are estimated from the data and factor scores are estimated from principal components. This is a two-stage model: principal components are estimated in the first stage and a decision is made regarding how many principal components to retain, and ADFA is estimated in the second stage. For more information on DFA, see section \ref{dfa}.
\subsection{Form}
$$\textbf{Y}_{t,j} \sim \mathcal{N}(\mu_{t,j}, \sigma^2_j), \quad t=2,\dots,T, \quad j=1,\dots,J$$
$$\mu_{t,} = \textbf{F}_{t-1,} \Lambda$$
$$\Lambda_{p,j} \sim \mathcal{N}(0, 1000), \quad p=1,\dots,P, \quad j=1,\dots,J$$
$$\sigma_j \sim \mathcal{HC}(25), \quad j=1,\dots,J$$
\subsection{Data}
\code{T <- 10 \#Number of time-periods \\
J <- 20 \#Number of variables \\
P <- 5 \#Number of approximate dynamic factors \\
Lambda <- matrix(runif(J*P,-1,1), P, J) \\
Sigma <- matrix(runif(P*P), P, P); diag(Sigma) <- runif(P)*5 \\
Sigma <- as.symmetric.matrix(Sigma); Sigma <- as.positive.definite(Sigma) \\
F <- rmvn(T, rep(0,P), Sigma) \\
Y <- tcrossprod(F, t(Lambda)) \\
PCA <- prcomp(Y, scale=TRUE) \\
F <- PCA$x[,1:P] \\
mon.names <- c("LP", paste("ynew[", 1:J, "]", sep="")) \\
parm.names <- as.parm.names(list(Lambda=matrix(0,P,J), sigma=rep(0,J))) \\
pos.Lambda <- grep("Lambda", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
PGF <- function(Data) return(c(rnormv(Data$P*Data$J,0,1000), \\
\hspace*{0.27 in} rhalfcauchy(Data$J,5))) \\
MyData <- list(F=F, J=J, P=P, PGF=PGF, T=T, Y=Y, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.Lambda=pos.Lambda, pos.sigma=pos.sigma) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} Lambda <- matrix(parm[Data$pos.Lambda], Data$P, Data$J) \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} Lambda.prior <- sum(dnormv(Lambda, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- rbind(rep(0, Data$J), tcrossprod(F[-Data$T,], t(Lambda))) \\
\hspace*{0.27 in} Sigma <- matrix(sigma, Data$T, Data$J, byrow=TRUE) \\
\hspace*{0.27 in} ynew <- rnorm(Data$J, tcrossprod(F[Data$T,], t(Lambda)), sigma) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$Y, mu, Sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + Lambda.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,ynew), \\
\hspace*{0.62 in} yhat=rnorm(prod(dim(mu)), mu, Sigma), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,P*J), rep(1,J))}

\section{Factor Analysis, Confirmatory} \label{cfa}
Factor scores are in matrix \textbf{F}, factor loadings for each variable are in vector $\lambda$, and $\textbf{f}$ is a vector that indicates which variable loads on which factor.
\subsection{Form}
$$\textbf{Y}_{i,m} \sim \mathcal{N}(\mu_{i,m}, \sigma^2_m), \quad i=1,\dots,N, \quad m=1,\dots,M$$
$$\mu = \alpha^T + \textbf{F}_{1:N,\textbf{f}} \lambda^T$$
$$\textbf{F}_{i,1:P} \sim \mathcal{N}_P(\gamma, \Omega^{-1}), \quad i=1,\dots,N$$
$$\alpha_m \sim \mathcal{N}(0, 1000), \quad m=1,\dots,M$$
$$\lambda_m \sim \mathcal{N}(0, 1000), \quad m=1,\dots,M$$
$$\sigma_m \sim \mathcal{HC}(25), \quad m=1,\dots,M$$
$$\Omega \sim \mathcal{W}_N(\textbf{S}), \quad \textbf{S} = \textbf{I}_P$$ 
\subsection{Data}
\code{data(swiss) \\
Y <- cbind(swiss$Agriculture, swiss$Examination, swiss$Education, \\
\hspace*{0.27 in} swiss$Catholic, swiss$Infant.Mortality) \\
M <- ncol(Y) \#Number of variables \\
N <- nrow(Y) \#Number of records \\
P <- 3 \#Number of factors \\
f <- c(1,3,2,2,1) \#Indicator f for the factor for each variable m \\
gamma <- rep(0,P) \\
S <- diag(P) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(F=matrix(0,N,P), lambda=rep(0,M), \\
\hspace*{0.27 in} U=diag(P), alpha=rep(0,M), sigma=rep(0,M)), uppertri=c(0,0,1,0,0)) \\
pos.F <- grep("F", parm.names) \\
pos.lambda <- grep("lambda", parm.names) \\
pos.alpha <- grep("alpha", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
PGF <- function(Data) return(c(rmvnpc(Data$N, Data$gamma, \\
\hspace*{0.27 in} rwishartc(Data$N,Data$S)), rnormv(Data$M,0,1000), \\
\hspace*{0.27 in} upper.triangle(rwishartc(Data$N,Data$S), diag=TRUE), \\
\hspace*{0.27 in} rnormv(Data$M,0,1000), rhalfcauchy(Data$M,5))) \\
MyData <- list(M=M, N=N, P=P, PGF=PGF, S=S, Y=Y, f=f, gamma=gamma, \\
\hspace*{0.27 in} mon.names=mon.names, parm.names=parm.names, pos.F=pos.F, \\
\hspace*{0.27 in} pos.lambda=pos.lambda, pos.alpha=pos.alpha, pos.sigma=pos.sigma) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[Data$pos.alpha] \\
\hspace*{0.27 in} lambda <- parm[Data$pos.lambda] \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} F <- matrix(parm[Data$pos.F], Data$N, Data$P) \\
\hspace*{0.27 in} U <- as.parm.matrix(U, Data$P, parm, Data, chol=TRUE) \\
\hspace*{0.27 in} diag(U) <- exp(diag(U)) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- sum(dnormv(alpha, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} lambda.prior <- sum(dnormv(lambda, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} U.prior <- dwishartc(U, Data$N, Data$S, log=TRUE) \\
\hspace*{0.27 in} F.prior <- sum(dmvnpc(F, Data$gamma, U, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- matrix(alpha, Data$N, Data$M, byrow=TRUE) + F[,Data$f] * \\
\hspace*{0.62 in} matrix(lambda, Data$N, Data$M, byrow=TRUE) \\
\hspace*{0.27 in} Sigma <- matrix(sigma, Data$N, Data$M, byrow=TRUE) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$Y, mu, Sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + lambda.prior + sigma.prior + F.prior + \\
\hspace*{0.62 in} U.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rnorm(prod(dim(mu)), mu, Sigma), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,N*P), rep(0,M), upper.triangle(S, diag=TRUE), \\
\hspace*{0.27 in} rep(0,M), rep(1,M))}

\section{Factor Analysis, Dynamic} \label{dfa}
The factor scores in \textbf{F} are dynamic with respect to time, and are estimated as in a state space model (SSM) or dynamic linear model (DLM) with constant variance in the state vector. For more information on SSMs, see section \ref{ssm.lin.reg}. For more information on exploratory factor analysis, see section \ref{efa}.
\subsection{Form}
$$\textbf{Y}_{t,j} \sim \mathcal{N}(\mu_{t,j}, \sigma^2_j), \quad t=2,\dots,T, \quad j=1,\dots,J$$
$$\mu_{2:T,} = \textbf{F}_{1:(T-1),} \Lambda$$
$$\textbf{F}_{1,1:P} \sim \mathcal{N}_P(0, \Omega^{-1})$$
$$\textbf{F}_{t,1:P} \sim \mathcal{N}_P(\textbf{F}_{t-1,1:P}, \Omega^{-1}), \quad t=2,\dots,T$$
$$\Lambda_{p,j} \sim \mathcal{N}(0, 1000), \quad p=1,\dots,P, \quad j=1,\dots,J$$
$$\Omega \sim \mathcal{W}_N(\textbf{S}), \quad \textbf{S} = \textbf{I}_P$$
$$\sigma_j \sim \mathcal{HC}(25), \quad j=1,\dots,J$$
\subsection{Data}
\code{T <- 10 \#Number of time-periods \\
J <- 20 \#Number of time-series \\
P <- 3 \#Number of dynamic factors \\
Lambda <- matrix(runif(J*P,-1,1), P, J) \\
Sigma <- matrix(runif(P*P), P, P); diag(Sigma) <- runif(P)*5 \\
Sigma <- as.symmetric.matrix(Sigma); Sigma <- as.positive.definite(Sigma) \\
F <- rmvn(T, rep(0,P), Sigma) \\
Y <- tcrossprod(F, t(Lambda)) \\
S <- diag(P) \\
mon.names <- c("LP", paste("ynew[", 1:J, "]", sep="")) \\
parm.names <- as.parm.names(list(F=matrix(0,T,P), U=diag(P), \\
\hspace*{0.27 in} Lambda=matrix(0,P,J), sigma=rep(0,J)), uppertri=c(0,1,0,0)) \\
pos.F <- grep("F", parm.names) \\
pos.Lambda <- grep("Lambda", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
PGF <- function(Data) return(c(rmvnpc(Data$T, rep(0,Data$P), \\
\hspace*{0.27 in} rwishartc(Data$P+1,Data$S)), \\
\hspace*{0.27 in} upper.triangle(rwishartc(Data$P+1,Data$S), diag=TRUE), \\
\hspace*{0.27 in} rnormv(Data$P*Data$J,0,1000), rhalfcauchy(Data$J,5)))) \\
MyData <- list(J=J, P=P, PGF=PGF, S=S, T=T, Y=Y, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.F=pos.F, pos.Lambda=pos.Lambda, \\
\hspace*{0.27 in} pos.sigma=pos.sigma) \\
Dyn <- matrix(".", T, P) \\
for (t in 1:T) \{for (p in 1:P) \{ \\
\hspace*{0.27 in} Dyn[t,p] <- paste("F[",t,",",p,"]", sep="")\}\} \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} F <- matrix(parm[Data$pos.F], Data$T, Data$P) \\
\hspace*{0.27 in} U <- as.parm.matrix(U, Data$P, parm, Data, chol=TRUE) \\
\hspace*{0.27 in} diag(U) <- exp(diag(U) \\
\hspace*{0.27 in} Lambda <- matrix(parm[Data$pos.Lambda], Data$P, Data$J) \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} F.prior <- sum(dmvnpc(F, rbind(rep(0, Data$P), F[-Data$T,]), U, \\
\hspace*{0.62 in} log=TRUE)) \\
\hspace*{0.27 in} U.prior <- dwishartc(U, Data$P+1, Data$S, log=TRUE) \\
\hspace*{0.27 in} Lambda.prior <- sum(dnormv(Lambda, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- rbind(rep(0, Data$J), tcrossprod(F[-Data$T,], t(Lambda))) \\
\hspace*{0.27 in} Sigma <- matrix(sigma, Data$T, Data$J, byrow=TRUE) \\
\hspace*{0.27 in} ynew <- rnorm(Data$J, tcrossprod(F[Data$T,], t(Lambda)), sigma) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$Y, mu, Sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + F.prior + U.prior + Lambda.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,ynew), \\
\hspace*{0.62 in} yhat=rnorm(prod(dim(mu)), mu, Sigma), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,T*P), S[upper.tri(S, diag=TRUE)], rep(0,P*J), \\
\hspace*{0.27 in} rep(1,J)) \\
}

\section{Factor Analysis, Exploratory} \label{efa}
Factor scores are in matrix \textbf{F} and factor loadings are in matrix $\Lambda$.
\subsection{Form}
$$\textbf{Y}_{i,m} \sim \mathcal{N}(\mu_{i,m}, \sigma^2_m), \quad i=1,\dots,N, \quad m=1,\dots,M$$
$$\mu = \alpha^T + \textbf{F} \Lambda$$
$$\textbf{F}_{i,1:P} \sim \mathcal{N}_P(\gamma, \Omega^{-1}), \quad i=1,\dots,N$$
$$\alpha_m \sim \mathcal{N}(0, 1000), \quad m=1,\dots,M$$
$$\gamma_p = 0, \quad p=1,\dots,P$$
$$\Lambda_{p,m} \sim \mathcal{N}(0, 1000), \quad p=1,\dots,P, \quad m=1,\dots,M$$
$$\Omega \sim \mathcal{W}_N(\textbf{S}), \quad \textbf{S} = \textbf{I}_P$$
$$\sigma_m \sim \mathcal{HC}(25), \quad m=1,\dots,M$$
\subsection{Data}
\code{M <- 10 \#Number of variables \\
N <- 20 \#Number of records \\
P <- 3 \#Number of factors \\
alpha <- runif(M)*10 \\
Lambda <- matrix(runif(M*P,-1,1), P, M) \\
U <- diag(P)
U[upper.tri(U, diag=TRUE)] <- runif(length(upper.triangle(U, diag=TRUE)))
F <- rmvnc(N, rep(0,P), U) \\
Y <- matrix(alpha, N, M, byrow=TRUE) + tcrossprod(F, t(Lambda)) \\
gamma <- rep(0,P) \\
S <- diag(P) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(F=matrix(0,N,P), Lambda=matrix(0,P,M), \\
\hspace*{0.27 in} U=diag(P), alpha=rep(0,M), sigma=rep(0,M)), uppertri=c(0,0,1,0,0)) \\
pos.F <- grep("F", parm.names) \\
pos.Lambda <- grep("Lambda", parm.names) \\
pos.alpha <- grep("alpha", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
PGF <- function(Data) return(c(rmvnpc(Data$N, Data$gamma, \\
\hspace*{0.27 in} rwishartc(Data$N, Data$S)), rnormv(Data$P*Data$M,0,1000), \\
\hspace*{0.27 in} upper.triangle(rwishartc(Data$N, Data$S), diag=TRUE), \\
\hspace*{0.27 in} rnormv(Data$M,0,1000), rhalfcauchy(Data$M,5))) \\
MyData <- list(M=M, N=N, P=P, PGF=PGF, S=S, Y=Y, gamma=gamma, \\
\hspace*{0.27 in} mon.names=mon.names, parm.names=parm.names, pos.F=pos.F, \\
\hspace*{0.27 in} pos.Lambda=pos.Lambda, pos.alpha=pos.alpha, pos.sigma=pos.sigma) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} F <- matrix(parm[Data$pos.F], Data$N, Data$P) \\
\hspace*{0.27 in} Lambda <- matrix(parm[Data$pos.Lambda], Data$P, Data$M) \\
\hspace*{0.27 in} U <- as.parm.matrix(U, Data$P, parm, Data, chol=TRUE) \\
\hspace*{0.27 in} diag(U) <- exp(diag(U)) \\
\hspace*{0.27 in} alpha <- parm[Data$pos.alpha] \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} F.prior <- sum(dmvnpc(F, Data$gamma, U, log=TRUE)) \\
\hspace*{0.27 in} Lambda.prior <- sum(dnormv(Lambda, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} U.prior <- dwishartc(U, Data$N, Data$S, log=TRUE) \\
\hspace*{0.27 in} alpha.prior <- sum(dnormv(alpha, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- matrix(alpha, Data$N, Data$M, byrow=TRUE) + \\
\hspace*{0.62 in} tcrossprod(F, t(Lambda)) \\
\hspace*{0.27 in} Sigma <- matrix(sigma, Data$N, Data$M, byrow=TRUE) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$Y, mu, Sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + F.prior + Lambda.prior + U.prior + alpha.prior + \\
\hspace*{0.62 in} sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.27 in} yhat=rnorm(prod(dim(mu)), mu, Sigma), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,N*P), rep(0,P*M), upper.triangle(S, diag=TRUE), \\
\hspace*{0.27 in} rep(0,M), rep(1,M))}

\section{Factor Analysis, Exploratory Ordinal} \label{eofa}
This exploratory ordinal factor analysis (EOFA) model form is also suitable for collaborative filtering, and automatically handles missing values.
\subsection{Form}
$$\textbf{Y}_{i,m} \sim \mathcal{CAT}(\textbf{P}_{i,m,1:K}), \quad i=1,\dots,N, \quad m=1,\dots,M$$
$$\textbf{P}_{,,K} = 1 - Q_{,,(K-1)}$$
$$\textbf{P}_{,,k} = |Q_{,,k} - Q_{,,(k-1)}|, \quad k=2,\dots,(K-1)$$
$$\textbf{P}_{,,1} = Q_{,,1}$$
$$Q = \phi(\mu)$$
$$\mu_{,,k} = \alpha_k - \textbf{F} \Lambda, \quad k=1,\dots,(K-1)$$
$$\textbf{F}_{i,1:P} \sim \mathcal{N}_P(\gamma, \Omega^{-1}), \quad i=1,\dots,N$$
$$\gamma_p = 0, \quad p=1,\dots,P$$
$$\Lambda_{p,m} \sim \mathcal{N}(0, 1000), \quad p=1,\dots,P, \quad m=1,\dots,M$$
$$\Omega \sim \mathcal{W}_N(\textbf{S}), \quad \textbf{S} = \textbf{I}_P$$
$$\alpha_k \sim \mathcal{N}(0, 1) \in [(k-1),k] \in [-5,5], \quad k=2,\dots,(K-1)$$
\subsection{Data}
\code{M <- 10 \#Number of variables \\
N <- 20 \#Number of records \\
K <- 3 \#Number of discrete values \\
P <- 3 \#Number of factors \\
alpha <- sort(rnorm(K-1)) \\
Lambda <- matrix(runif(M*P,-1,1), P, M) \\
U <- diag(P) \\
U[upper.tri(U, diag=TRUE)] <- runif(length(upper.triangle(U, diag=TRUE))) \\
F <- rmvnc(N, rep(0,P), U) \\
mu <- aperm(array(alpha, dim=c(K-1, M, N)), perm=c(3,2,1)) \\
mu <- mu - array(tcrossprod(F, t(Lambda)), dim=c(N, M, K-1)) \\
Pr <- Q <- pnorm(mu) \\
Pr[ , , -1] <- abs(Q[ , , -1] - Q[ , , -(K-1)]) \\
Pr <- array(Pr, dim=c(N, M, K)) \\
Pr[ , , K] <- 1 - Q[ , , (K-1)] \\
dim(Pr) <- c(N*M, K) \\
Y <- matrix(rcat(nrow(Pr), Pr), N, M) \#Make sure Y has all values \\
gamma <- rep(0,P) \\
S <- diag(P) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(F=matrix(0,N,P), Lambda=matrix(0,P,M), \\
\hspace*{0.27 in} U=diag(P), alpha=rep(0,K-1)), uppertri=c(0,0,1,0)) \\
pos.F <- grep("F", parm.names) \\
pos.Lambda <- grep("Lambda", parm.names) \\
pos.alpha <- grep("alpha", parm.names) \\
PGF <- function(Data) return(c(rmvnpc(Data$N, Data$gamma, \\
\hspace*{0.27 in} rwishartc(Data$N, Data$S)), rnorm(Data$P*Data$M), \\
\hspace*{0.27 in} upper.triangle(rwishartc(Data$N, Data$S), diag=TRUE), \\
\hspace*{0.27 in} sort(rnorm(Data$K-1)))) \\
MyData <- list(K=K, M=M, N=N, P=P, PGF=PGF, S=S, Y=Y, gamma=gamma, \\
\hspace*{0.27 in} mon.names=mon.names, parm.names=parm.names, pos.F=pos.F, \\
\hspace*{0.27 in} pos.Lambda=pos.Lambda, pos.alpha=pos.alpha) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} F <- matrix(parm[Data$pos.F], Data$N, Data$P) \\
\hspace*{0.27 in} Lambda <- matrix(parm[Data$pos.Lambda], Data$P, Data$M) \\
\hspace*{0.27 in} U <- as.parm.matrix(U, Data$P, parm, Data, chol=TRUE) \\
\hspace*{0.27 in} diag(U) <- exp(diag(U)) \\
\hspace*{0.27 in} alpha <- sort(interval(parm[Data$pos.alpha], -5, 5)) \\
\hspace*{0.27 in} parm[Data$pos.alpha] <- alpha \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} F.prior <- sum(dmvnpc(F, Data$gamma, U, log=TRUE)) \\
\hspace*{0.27 in} Lambda.prior <- sum(dnormv(Lambda, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} U.prior <- dwishartc(U, Data$N, Data$S, log=TRUE) \\
\hspace*{0.27 in} alpha.prior <- sum(dnormv(alpha, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- aperm(array(alpha, dim=c(Data$K-1, Data$M, Data$N)), \\
\hspace*{0.62 in} perm=c(3,2,1)) \\
\hspace*{0.27 in} mu <- mu - array(tcrossprod(F, t(Lambda)), \\
\hspace*{0.62 in} dim=c(Data$N, Data$M, Data$K-1)) \\
\hspace*{0.27 in} P <- Q <- pnorm(mu) \\
\hspace*{0.27 in} P[ , , -1] <- abs(Q[ , , -1] - Q[ , , -(Data$K-1)]) \\
\hspace*{0.27 in} P <- array(P, dim=c(Data$N, Data$M, Data$K)) \\
\hspace*{0.27 in} P[ , , Data$K] <- 1 - Q[ , , (Data$K-1)] \\
\hspace*{0.27 in} y <- as.vector(Data$Y) \\
\hspace*{0.27 in} dim(P) <- c(Data$N*Data$M, Data$K) \\
\hspace*{0.27 in} LL <- sum(dcat(y, P, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + F.prior + Lambda.prior + U.prior + alpha.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=matrix(rcat(nrow(P), P), Data$N, Data$M), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,N*P), rep(0,P*M), upper.triangle(S, diag=TRUE), \\
\hspace*{0.27 in} seq(from=-1, to=1, len=K-1))
}

\section{Factor Regression} \label{factor.reg}
This example of factor regression is constrained to the case where the number of factors is equal to the number of independent variables (IVs) less the intercept. The purpose of this form of factor regression is to orthogonalize the IVs with respect to $\textbf{y}$, rather than variable reduction. This method is the combination of confirmatory factor analysis in section \ref{cfa} and linear regression in section \ref{linear.reg}.
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\nu, \sigma^2_{J+1})$$
$$\nu = \mu \beta$$
$$\mu_{i,1} = 1$$
$$\mu_{i,j+1} = \mu_{i,j}, \quad j=1,\dots,J$$
$$\textbf{X}_{i,j} \sim \mathcal{N}(\mu_{i,j}, \sigma^2_j), \quad i=1,\dots,N, \quad j=2,\dots,J$$
$$\mu_{i,j} = \alpha_j + \lambda_j \textbf{F}_{i,j}, \quad i=1,\dots,N, \quad j=2,\dots,J$$
$$\textbf{F}_{i,1:J} \sim \mathcal{N}_{J-1}(0, \Omega^{-1}), \quad i=1,\dots,N$$
$$\alpha_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,(J-1)$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\lambda_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,(J-1)$$
$$\sigma_j \sim \mathcal{HC}(25), \quad j=1,\dots,(J+1)$$
$$\Omega \sim \mathcal{W}_N(\textbf{S}), \quad \textbf{S} = \textbf{I}_J$$
\subsection{Data}
\code{data(demonsnacks) \\
N <- nrow(demonsnacks) \\
y <- log(demonsnacks$Calories) \\
X <- as.matrix(log(demonsnacks[,c(1,4,10)]+1)) \\
J <- ncol(X) \\
for (j in 1:J) \{X[,j] <- CenterScale(X[,j])\} \\
S <- diag(J) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(alpha=rep(0,J), beta=rep(0,J+1), \\
\hspace*{0.27 in} lambda=rep(0,J), sigma=rep(0,J+1), F=matrix(0,N,J), U=diag(J)), \\
\hspace*{0.27 in} uppertri=c(0,0,0,0,0,1)) \\
pos.alpha <- grep("alpha", parm.names) \\
pos.beta <- grep("beta", parm.names) \\
pos.lambda <- grep("lambda", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
pos.F <- grep("F", parm.names) \\
PGF <- function(Data) return(c(rnormv(Data$J,0,1000), \\
\hspace*{0.27 in} rnormv(Data$J+1,0,1000), rnormv(Data$J,0,1000), \\
\hspace*{0.27 in} rhalfcauchy(Data$J+1,5), rmvnpc(Data$N, rep(0,Data$J), Data$S),
\hspace*{0.27 in} upper.triangle(rwishartc(Data$J+1,Data$S), diag=TRUE))) \\
MyData <- list(J=J, N=N, PGF=PGF, S=S, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.alpha=pos.alpha, pos.beta=pos.beta, \\
\hspace*{0.27 in} pos.lambda=pos.lambda, pos.sigma=pos.sigma, pos.F=pos.F, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[Data$pos.alpha] \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} lambda <- parm[Data$pos.lambda] \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} F <- matrix(Data$pos.F], Data$N, Data$J) \\
\hspace*{0.27 in} U <- as.parm.matrix(U, Data$J, parm, Data, chol=TRUE) \\
\hspace*{0.27 in} diag(U) <- exp(diag(U)) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- sum(dnormv(alpha, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} lambda.prior <- sum(dnormv(lambda, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} F.prior <- sum(dmvnpc(F, rep(0,Data$J), U, log=TRUE)) \\
\hspace*{0.27 in} U.prior <- dwishartc(U, Data$J+1, Data$S, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- matrix(alpha, Data$N, Data$J, byrow=TRUE) + F * \\
\hspace*{0.62 in} matrix(lambda, Data$N, Data$J, byrow=TRUE) \\
\hspace*{0.27 in} nu <- tcrossprod(beta, cbind(rep(1,Data$N),mu)) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$X, mu, matrix(sigma[1:Data$J], Data$N, Data$J, \\
\hspace*{0.62 in} byrow=TRUE)), dnorm(Data$y, nu, sigma[Data$J+1], log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior + lambda.prior + sigma.prior + \\
\hspace*{0.62 in} F.prior + U.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rnorm(Data$N, nu, sigma[Data$J+1]), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), rep(0,J+1), rep(0,J), rep(0,J+1),
\hspace*{0.27 in} rep(0,N*J), S[upper.tri(S, diag=TRUE)])}

\section{Gamma Regression} \label{gamma.reg}
\subsection{Form}
$$\textbf{y} \sim \mathcal{G}(\lambda \tau, \tau)$$
$$\lambda = \exp(\textbf{X} \beta)$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\tau \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{N <- 20 \\
J <- 3 \\
X <- matrix(runif(N*J,-2,2),N,J); X[,1] <- 1 \\
beta <- runif(J,-2,2) \\
y <- round(exp(tcrossprod(X, t(beta)))) + 0.1 \#Must be > 0 \\
mon.names <- c("LP","sigma2") \\
parm.names <- as.parm.names(list(beta=rep(0,J), tau=0)) \\
pos.beta <- grep("beta", parm.names) \\
pos.tau <- grep("tau", parm.names) \\
PGF <- function(Data) return(c(rnormv(Data$J,0,1000), rhalfcauchy(1,5))) \\
MyData <- list(J=J, N=N, PGF=PGF, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.beta=pos.beta, pos.tau=pos.tau, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} tau <- interval(parm[Data$pos.tau], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.tau] <- tau \\
\hspace*{0.27 in} sigma2 <- 1/tau \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} tau.prior <- dhalfcauchy(tau, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} lambda <- exp(tcrossprod(Data$X, t(beta))) \\
\hspace*{0.27 in} LL <- sum(dgamma(Data$y, tau*lambda, tau, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + tau.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,sigma2), \\
\hspace*{0.62 in} yhat=rgamma(nrow(lambda), tau*lambda, tau), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), 1)}

\section{GARCH(1,1)} \label{garch}
\subsection{Form}
$$\textbf{y}_t \sim \mathcal{N}(\mu_t, \sigma^2_t), \quad t=1,\dots,T$$
$$\textbf{y}^{new} \sim \mathcal{N}(\mu_{T+1}, \sigma^2_{new})$$
$$\mu_t = \alpha + \phi \textbf{y}_{t-1}, \quad t=1,\dots,(T+1)$$
$$\epsilon_t = \textbf{y}_t - \mu_t$$
$$\alpha \sim \mathcal{N}(0, 1000)$$
$$\phi \sim \mathcal{N}(0, 1000)$$
$$\sigma^2_{new} = \theta_1 + \theta_2 \epsilon^2_T + \theta_3 \sigma^2_T$$
$$\sigma^2_t = \theta_1 + \theta_2 \epsilon^2_{t-1} + \theta_3 \sigma^2_{t-1}$$
$$\theta_k = \frac{1}{1 + \exp(-\theta_k)}, \quad k=1,\dots,3$$
$$\theta_k \sim \mathcal{N}(0, 1000) \in [-10,10], \quad k=1,\dots,3$$
\subsection{Data}
\code{y <- c(0.02, -0.51, -0.30,  1.46, -1.26, -2.15, -0.91, -0.53, -1.91, \\
\hspace*{0.27 in} 2.64,  1.64,  0.15,  1.46,  1.61,  1.96, -2.67, -0.19, -3.28, \\
\hspace*{0.27 in}  1.89,  0.91, -0.71,  0.74, -0.10,  3.20, -0.80, -5.25,  1.03, \\
\hspace*{0.27 in} -0.40, -1.62, -0.80,  0.77,  0.17, -1.39, -1.28,  0.48, -1.02, \\
\hspace*{0.27 in}  0.09, -1.09,  0.86,  0.36,  1.51, -0.02,  0.47,  0.62, -1.36, \\
\hspace*{0.27 in}  1.12,  0.42, -4.39, -0.87,  0.05, -5.41, -7.38, -1.01, -1.70, \\
\hspace*{0.27 in}  0.64,  1.16,  0.87,  0.28, -1.69, -0.29,  0.13, -0.65,  0.83, \\
\hspace*{0.27 in}  0.62,  0.05, -0.14,  0.01, -0.36, -0.32, -0.80, -0.06,  0.24, \\
\hspace*{0.27 in}  0.23, -0.37,  0.00, -0.33,  0.21, -0.10, -0.10, -0.01, -0.40, \\
\hspace*{0.27 in} -0.35,  0.48, -0.28,  0.08,  0.28,  0.23,  0.27, -0.35, -0.19, \\
\hspace*{0.27 in}  0.24,  0.17, -0.02, -0.23,  0.03,  0.02, -0.17,  0.04, -0.39, \\
\hspace*{0.27 in} -0.12,  0.16,  0.17,  0.00,  0.18,  0.06, -0.36,  0.22,  0.14, \\
\hspace*{0.27 in} -0.17,  0.10, -0.01,  0.00, -0.18, -0.02,  0.07, -0.06,  0.06, \\
\hspace*{0.27 in} -0.05, -0.08, -0.07,  0.01, -0.06,  0.01,  0.01, -0.02,  0.01, \\
\hspace*{0.27 in}  0.01,  0.12, -0.03,  0.08, -0.10,  0.01, -0.03, -0.08,  0.04, \\
\hspace*{0.27 in} -0.09, -0.08,  0.01, -0.05,  0.08, -0.14,  0.06, -0.11,  0.09, \\
\hspace*{0.27 in}  0.06, -0.12, -0.01, -0.05, -0.15, -0.05, -0.03,  0.04,  0.00, \\
\hspace*{0.27 in} -0.12,  0.04, -0.06, -0.05, -0.07, -0.05, -0.14, -0.05, -0.01, \\
\hspace*{0.27 in} -0.12,  0.05,  0.06, -0.10,  0.00,  0.01,  0.00, -0.08,  0.00, \\
\hspace*{0.27 in}  0.00,  0.07, -0.01,  0.00,  0.09,  0.33,  0.13,  0.42,  0.24, \\
\hspace*{0.27 in} -0.36,  0.22, -0.09, -0.19, -0.10, -0.08, -0.07,  0.05,  0.07, \\
\hspace*{0.27 in}  0.07,  0.00, -0.04, -0.05,  0.03,  0.08,  0.26,  0.10,  0.08, \\
\hspace*{0.27 in}  0.09, -0.07, -0.33,  0.17, -0.03,  0.07, -0.04, -0.06, -0.06, \\
\hspace*{0.27 in}  0.07, -0.03,  0.00,  0.08,  0.27,  0.11,  0.11,  0.06, -0.11, \\
\hspace*{0.27 in} -0.09, -0.21,  0.24, -0.12,  0.11, -0.02, -0.03,  0.02, -0.10, \\
\hspace*{0.27 in}  0.00, -0.04,  0.01,  0.02, -0.03, -0.10, -0.09,  0.17,  0.07, \\
\hspace*{0.27 in} -0.05, -0.01, -0.05,  0.01,  0.00, -0.08, -0.05, -0.08,  0.07, \\
\hspace*{0.27 in}  0.06, -0.14,  0.02,  0.01,  0.04,  0.00, -0.13, -0.17) \\
T <- length(y) \\
mon.names <- c("LP", "ynew", "sigma2.new") \\
parm.names <- c("alpha","phi","omega","theta[1]","theta[2]") \\
PGF <- function(Data) return(c(rnormv(2,0,1000), rhalfcauchy(1,5),  \\
\hspace*{0.27 in} runif(2))) \\
MyData <- list(PGF=PGF, T=T, mon.names=mon.names, parm.names=parm.names, \\
\hspace*{0.27 in} y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1]; phi <- parm[2] \\
\hspace*{0.27 in} parm[3] <- omega <- interval(parm[3], 1e-100, Inf) \\
\hspace*{0.27 in} theta <- interval(parm[4:5], 1e-5, 1-1e-5) \\
\hspace*{0.27 in} if(sum(theta) >= 1) theta[2] <- 1 - 1e-5 - theta[1] \\
\hspace*{0.27 in} parm[4:5] <- theta
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnormv(alpha, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} phi.prior <- dnormv(phi, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} omega.prior <- dhalfcauchy(omega, 25, log=TRUE) \\
\hspace*{0.27 in} theta.prior <- sum(dunif(theta, 0, 1, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- c(alpha, alpha + phi*Data$y[-Data$T]) \\
\hspace*{0.27 in} epsilon <- Data$y - mu \\
\hspace*{0.27 in} sigma2 <- c(omega, omega + theta[1]*epsilon[-Data$T]\textasciicircum 2) \\
\hspace*{0.27 in} sigma2[-1] <- sigma2[-1] + theta[2]*sigma2[-Data$T] \\
\hspace*{0.27 in} sigma2.new <- omega + theta[1]*epsilon[Data$T]\textasciicircum 2 + \\
\hspace*{0.62 in} theta[2]*sigma2[Data$T] \\
\hspace*{0.27 in} ynew <- rnormv(1, alpha + phi*Data$y[Data$T], sigma2.new) \\
\hspace*{0.27 in} LL <- sum(dnormv(Data$y, mu, sigma2, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + phi.prior + omega.prior + theta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, ynew, sigma2.new), \\
\hspace*{0.62 in} yhat=rnormv(length(mu), mu, sigma2), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,2), rep(0.4,3))}

\section{GARCH-M(1,1)} \label{garchm}
\subsection{Form}
$$\textbf{y}_t \sim \mathcal{N}(\mu_t, \sigma^2_t), \quad t=1,\dots,T$$
$$\textbf{y}^{new} \sim \mathcal{N}(\mu_{T+1}, \sigma^2_{new})$$
$$\mu_t = \alpha + \phi \textbf{y}_{t-1} + \delta \sigma^2_{t-1}, \quad t=1,\dots,(T+1)$$
$$\epsilon_t = \textbf{y}_t - \mu_t$$
$$\alpha \sim \mathcal{N}(0, 1000)$$
$$\phi \sim \mathcal{N}(0, 1000)$$
$$\sigma^2_{new} = \omega + \theta_1 \epsilon^2_T + \theta_2 \sigma^2_T$$
$$\sigma^2_t = \omega + \theta_1 \epsilon^2_{t-1} + \theta_2 \sigma^2_{t-1}$$
$$\omega \sim \mathcal{HC}(25)$$
$$\theta_k \sim \mathcal{U}(0, 1), \quad k=1,\dots,2$$
\subsection{Data}
\code{y <- c(0.02, -0.51, -0.30,  1.46, -1.26, -2.15, -0.91, -0.53, -1.91, \\
\hspace*{0.27 in} 2.64,  1.64,  0.15,  1.46,  1.61,  1.96, -2.67, -0.19, -3.28, \\
\hspace*{0.27 in}  1.89,  0.91, -0.71,  0.74, -0.10,  3.20, -0.80, -5.25,  1.03, \\
\hspace*{0.27 in} -0.40, -1.62, -0.80,  0.77,  0.17, -1.39, -1.28,  0.48, -1.02, \\
\hspace*{0.27 in}  0.09, -1.09,  0.86,  0.36,  1.51, -0.02,  0.47,  0.62, -1.36, \\
\hspace*{0.27 in}  1.12,  0.42, -4.39, -0.87,  0.05, -5.41, -7.38, -1.01, -1.70, \\
\hspace*{0.27 in}  0.64,  1.16,  0.87,  0.28, -1.69, -0.29,  0.13, -0.65,  0.83, \\
\hspace*{0.27 in}  0.62,  0.05, -0.14,  0.01, -0.36, -0.32, -0.80, -0.06,  0.24, \\
\hspace*{0.27 in}  0.23, -0.37,  0.00, -0.33,  0.21, -0.10, -0.10, -0.01, -0.40, \\
\hspace*{0.27 in} -0.35,  0.48, -0.28,  0.08,  0.28,  0.23,  0.27, -0.35, -0.19, \\
\hspace*{0.27 in}  0.24,  0.17, -0.02, -0.23,  0.03,  0.02, -0.17,  0.04, -0.39, \\
\hspace*{0.27 in} -0.12,  0.16,  0.17,  0.00,  0.18,  0.06, -0.36,  0.22,  0.14, \\
\hspace*{0.27 in} -0.17,  0.10, -0.01,  0.00, -0.18, -0.02,  0.07, -0.06,  0.06, \\
\hspace*{0.27 in} -0.05, -0.08, -0.07,  0.01, -0.06,  0.01,  0.01, -0.02,  0.01, \\
\hspace*{0.27 in}  0.01,  0.12, -0.03,  0.08, -0.10,  0.01, -0.03, -0.08,  0.04, \\
\hspace*{0.27 in} -0.09, -0.08,  0.01, -0.05,  0.08, -0.14,  0.06, -0.11,  0.09, \\
\hspace*{0.27 in}  0.06, -0.12, -0.01, -0.05, -0.15, -0.05, -0.03,  0.04,  0.00, \\
\hspace*{0.27 in} -0.12,  0.04, -0.06, -0.05, -0.07, -0.05, -0.14, -0.05, -0.01, \\
\hspace*{0.27 in} -0.12,  0.05,  0.06, -0.10,  0.00,  0.01,  0.00, -0.08,  0.00, \\
\hspace*{0.27 in}  0.00,  0.07, -0.01,  0.00,  0.09,  0.33,  0.13,  0.42,  0.24, \\
\hspace*{0.27 in} -0.36,  0.22, -0.09, -0.19, -0.10, -0.08, -0.07,  0.05,  0.07, \\
\hspace*{0.27 in}  0.07,  0.00, -0.04, -0.05,  0.03,  0.08,  0.26,  0.10,  0.08, \\
\hspace*{0.27 in}  0.09, -0.07, -0.33,  0.17, -0.03,  0.07, -0.04, -0.06, -0.06, \\
\hspace*{0.27 in}  0.07, -0.03,  0.00,  0.08,  0.27,  0.11,  0.11,  0.06, -0.11, \\
\hspace*{0.27 in} -0.09, -0.21,  0.24, -0.12,  0.11, -0.02, -0.03,  0.02, -0.10, \\
\hspace*{0.27 in}  0.00, -0.04,  0.01,  0.02, -0.03, -0.10, -0.09,  0.17,  0.07, \\
\hspace*{0.27 in} -0.05, -0.01, -0.05,  0.01,  0.00, -0.08, -0.05, -0.08,  0.07, \\
\hspace*{0.27 in}  0.06, -0.14,  0.02,  0.01,  0.04,  0.00, -0.13, -0.17) \\
T <- length(y) \\
mon.names <- c("LP", "ynew", "sigma2.new") \\
parm.names <- c("alpha","phi","delta","omega","theta[1]", "theta[2]") \\
PGF <- function(Data) return(c(rnormv(3,0,1000), rhalfcauchy(1,5), \\
\hspace*{0.27 in} runif(2))) \\
MyData <- list(PGF=PGF, T=T, mon.names=mon.names, parm.names=parm.names, \\
\hspace*{0.27 in} y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1]; phi <- parm[2]; delta <- parm[3] \\
\hspace*{0.27 in} parm[4] <- omega <- interval(parm[4], 1e-100, Inf) \\
\hspace*{0.27 in} parm[5:6] <- theta <- interval(parm[5:6], 1e-10, 1-1e-5) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnormv(alpha, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} phi.prior <- dnormv(phi, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} delta.prior <- dnormv(delta, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} omega.prior <- dhalfcauchy(omega, 25, log=TRUE) \\
\hspace*{0.27 in} theta.prior <- sum(dunif(theta, 0, 1, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- c(alpha, alpha + phi*Data$y[-Data$T]) \\
\hspace*{0.27 in} epsilon <- Data$y - mu \\
\hspace*{0.27 in} sigma2 <- c(omega, omega + theta[1]*epsilon[-Data$T]\textasciicircum 2) \\
\hspace*{0.27 in} sigma2[-1] <- sigma2[-1] + theta[2]*sigma2[-Data$T] \\
\hspace*{0.27 in} sigma2.new <- omega + theta[1]*epsilon[Data$T]\textasciicircum 2 + \\
\hspace*{0.62 in} theta[2]*sigma2[Data$T] \\
\hspace*{0.27 in} mu <- mu + delta*sigma2 \\
\hspace*{0.27 in} ynew <- rnormv(1, alpha + phi*Data$y[Data$T] + delta*sigma2[Data$T], \\
\hspace*{0.62 in} sigma2.new) \\
\hspace*{0.27 in} LL <- sum(dnormv(Data$y, mu, sigma2, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + phi.prior + delta.prior + omega.prior + \\
\hspace*{0.62 in} theta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, ynew, sigma2.new), \\
\hspace*{0.62 in} yhat=rnormv(length(mu), mu, sigma2), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,3), rep(0.3,3))}

\section{Geographically Weighted Regression} \label{gwr}
\subsection{Form}
$$\textbf{y}_{i,k} \sim \mathcal{N}(\mu_{i,k}, \tau^{-1}_{i,k}), \quad i=1,\dots,N, \quad k=1,\dots,N$$
$$\mu_{i,1:N} = \textbf{X} \beta_{i,1:J}$$
$$\tau = \frac{1}{\sigma^2} \textbf{w} \nu$$
$$\textbf{w} = \frac{\exp(-0.5 \textbf{Z}^2)}{\textbf{h}}$$
$$\alpha \sim \mathcal{U}(1.5, 100)$$
$$\beta_{i,j} \sim \mathcal{N}(0, 1000), \quad i=1,\dots,N, \quad j=1,\dots,J$$
$$\textbf{h} \sim \mathcal{N}(0.1, 1000) \in [0.1, \infty]$$
$$\nu_{i,k} \sim \mathcal{G}(\alpha, 2), \quad i=1,\dots,N, \quad k=1,\dots,N$$
$$\sigma_i \sim \mathcal{HC}(25), \quad i=1,\dots,N$$
\subsection{Data}
\code{crime <-   c(18.802, 32.388, 38.426,  0.178, 15.726, 30.627, 50.732, \\
\hspace*{0.27 in} 26.067, 48.585, 34.001, 36.869, 20.049, 19.146, 18.905, 27.823, \\
\hspace*{0.27 in} 16.241,  0.224, 30.516, 33.705, 40.970, 52.794, 41.968, 39.175, \\
\hspace*{0.27 in} 53.711, 25.962, 22.541, 26.645, 29.028, 36.664, 42.445, 56.920, \\
\hspace*{0.27 in} 61.299, 60.750, 68.892, 38.298, 54.839, 56.706, 62.275, 46.716, \\
\hspace*{0.27 in} 57.066, 54.522, 43.962, 40.074, 23.974, 17.677, 14.306, 19.101, \\
\hspace*{0.27 in} 16.531, 16.492) \\
income <-  c(21.232,  4.477, 11.337,  8.438, 19.531, 15.956, 11.252, \\
\hspace*{0.27 in} 16.029,  9.873, 13.598,  9.798, 21.155, 18.942, 22.207, 18.950, \\
\hspace*{0.27 in} 29.833, 31.070, 17.586, 11.709,  8.085, 10.822,  9.918, 12.814, \\
\hspace*{0.27 in} 11.107, 16.961, 18.796, 11.813, 14.135, 13.380, 17.017,  7.856, \\
\hspace*{0.27 in}  8.461,  8.681, 13.906, 14.236,  7.625, 10.048,  7.467,  9.549, \\
\hspace*{0.27 in}  9.963, 11.618, 13.185, 10.655, 14.948, 16.940, 18.739, 18.477, \\
\hspace*{0.27 in} 18.324, 25.873) \\
housing <- c(44.567, 33.200, 37.125, 75.000, 80.467, 26.350, 23.225, \\
\hspace*{0.27 in} 28.750, 18.000, 96.400, 41.750, 47.733, 40.300, 42.100, 42.500, \\
\hspace*{0.27 in} 61.950, 81.267, 52.600, 30.450, 20.300, 34.100, 23.600, 27.000, \\
\hspace*{0.27 in} 22.700, 33.500, 35.800, 26.800, 27.733, 25.700, 43.300, 22.850, \\
\hspace*{0.27 in} 17.900, 32.500, 22.500, 53.200, 18.800, 19.900, 19.700, 41.700, \\
\hspace*{0.27 in} 42.900, 30.600, 60.000, 19.975, 28.450, 31.800, 36.300, 39.600, \\
\hspace*{0.27 in} 76.100, 44.333) \\
easting <- c(35.62, 36.50, 36.71, 33.36, 38.80, 39.82, 40.01, 43.75, \\
\hspace*{0.27 in} 39.61, 47.61, 48.58, 49.61, 50.11, 51.24, 50.89, 48.44, 46.73, \\
\hspace*{0.27 in} 43.44, 43.37, 41.13, 43.95, 44.10, 43.70, 41.04, 43.23, 42.67, \\
\hspace*{0.27 in} 41.21, 39.32, 41.09, 38.3,  41.31, 39.36, 39.72, 38.29, 36.60, \\
\hspace*{0.27 in} 37.60, 37.13, 37.85, 35.95, 35.72, 35.76, 36.15, 34.08, 30.32, \\
\hspace*{0.27 in} 27.94, 27.27, 24.25, 25.47, 29.02) \\
northing <- c(42.38, 40.52, 38.71, 38.41, 44.07, 41.18, 38.00, 39.28, \\
\hspace*{0.27 in} 34.91, 36.42, 34.46, 32.65, 29.91, 27.80, 25.24, 27.93, 31.91, \\
\hspace*{0.27 in} 35.92, 33.46, 33.14, 31.61, 30.40, 29.18, 28.78, 27.31, 24.96, \\
\hspace*{0.27 in} 25.90, 25.85, 27.49, 28.82, 30.90, 32.88, 30.64, 30.35, 32.09, \\
\hspace*{0.27 in} 34.08, 36.12, 36.30, 36.40, 35.60, 34.66, 33.92, 30.42, 28.26, \\
\hspace*{0.27 in} 29.85, 28.21, 26.69, 25.71, 26.58) \\
N <- length(crime) \\
J <- 3 \#Number of predictors, including the intercept \\
X <- matrix(c(rep(1,N), income, housing),N,J) \\
D <- as.matrix(dist(cbind(northing,easting), diag=TRUE, upper=TRUE)) \\
Z <- D / sd(as.vector(D)) \\
y <- matrix(0,N,N); for (i in 1:N) \{for (k in 1:N) \{y[i,k] <- crime[k]\}\} \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(alpha=0, beta=matrix(0,N,J), H=0, \\
\hspace*{0.27 in} nu=matrix(0,N,N), sigma=rep(0,N))) \\
pos.alpha <- grep("alpha", parm.names) \\
pos.beta <- grep("beta", parm.names) \\
pos.H <- grep("H", parm.names) \\
pos.nu <- grep("nu", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
PGF <- function(Data) return(c(runif(1,1.5,100), \\
\hspace*{0.27 in} rnormv(Data$N*Data$J,0,1000), runif(1, 0.1, 1000), \\
\hspace*{0.27 in} rgamma(Data$N*Data$N,runif(1,1.5,100),2), rhalfcauchy(Data$N,5))) \\
MyData <- list(J=J, N=N, PGF=PGF, X=X, Z=Z, latitude=northing, \\
\hspace*{0.27 in} longitude=easting, mon.names=mon.names, parm.names=parm.names, \\
\hspace*{0.27 in} pos.alpha=pos.alpha, pos.beta=pos.beta, pos.H=pos.H, pos.nu=pos.nu, \\
\hspace*{0.27 in} pos.sigma=pos.sigma, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- interval(parm[Data$pos.alpha], 1.5, 100) \\
\hspace*{0.27 in} parm[Data$pos.alpha] <- alpha \\
\hspace*{0.27 in} beta <- matrix(parm[Data$pos.beta], Data$N, Data$J) \\
\hspace*{0.27 in} parm[Data$pos.H] <- H <- interval(parm[Data$pos.H], 0.1, Inf) \\
\hspace*{0.27 in} parm[Data$pos.nu] <- nu <- interval(parm[Data$pos.nu], 1e-100, Inf) \\
\hspace*{0.27 in} nu <- matrix(nu, Data$N, Data$N)
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dunif(alpha, 1.5, 100, log=TRUE) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} h.prior <- dhalfnorm(H-0.1, 1000, log=TRUE) \\
\hspace*{0.27 in} nu.prior <- sum(dgamma(nu, alpha, 2, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} w <- exp(-0.5 * Data$Z\textasciicircum 2) / H \\
\hspace*{0.27 in} tau <- (1/sigma\textasciicircum 2) * w * nu \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, beta) \\
\hspace*{0.27 in} LL <- sum(dnormp(Data$y, mu, tau, log=TRUE)) \\
\hspace*{0.27 in} \#WSE <- w * nu * (Data$y - mu)\textasciicircum 2; w.y <- w * nu * Data$y \\
\hspace*{0.27 in} \#WMSE <- rowMeans(WSE); y.w <- rowSums(w.y) / rowSums(w) \\
\hspace*{0.27 in} \#LAR2 <- 1 - WMSE / sd(y.w)\textasciicircum 2 \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior + h.prior + nu.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rnormp(prod(dim(mu)), mu, tau), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(runif(1,1.5,100), rep(0,N*J), 1, rep(1,N*N), rep(1,N))}

\section{Hidden Markov Model} \label{hmm}
\subsection{Form}
This introductory hidden Markov model (HMM) includes $N$ discrete states. Discrete draws of the states of $\theta$ are updated with the Griddy-Gibbs sampler.
$$\textbf{y}_t \sim \mathcal{N}(\mu_\theta, \sigma^2_\theta), \quad t=1,\dots,T$$
$$\mu \sim \mathcal{N}(\mu_0, \lambda \sigma^2)$$
$$\sigma^2 \sim \mathcal{G}^{-1}(\nu, \sigma^2_0)$$
$$\theta \sim \mathcal{CAT}(\phi)$$
$$\phi_{1:N} \sim \mathcal{D}(\alpha_{1:N})$$
$$\lambda \sim \mathcal{HC}(25)$$
$$\mu_0 \sim \mathcal{N}(0, 1000)$$
$$\nu \sim \mathcal{HC}(25)$$
$$\sigma^2_0 \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{T <- 10 \#Number of time-periods \\
N <- 2 \#Number of discrete (hidden) states \\
y <- cumsum(rnorm(T)) \\
alpha <- rep(1,N) \#Concentration hyperparameter \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(lambda=rep(0,N), mu0=rep(0,N), \\
\hspace*{0.27 in} mu1=rep(0,N), nu=rep(0,N), phi=rep(0,N), sigma0=rep(0,N), \\
\hspace*{0.27 in} sigma2=rep(0,N), theta=rep(0,T))) \\
pos.lambda <- grep("lambda", parm.names) \\
pos.mu0 <- grep("mu0", parm.names) \\
pos.mu1 <- grep("mu1", parm.names) \\
pos.nu <- grep("nu", parm.names) \\
pos.phi <- grep("phi", parm.names) \\
pos.sigma0 <- grep("sigma0", parm.names) \\
pos.sigma2 <- grep("sigma2", parm.names) \\
pos.theta <- grep("theta", parm.names) \\
PGF <- function(Data) \{
\hspace*{0.27 in} phi <- runif(Data$N) \\
\hspace*{0.27 in} phi <- phi / sum(phi) \\
\hspace*{0.27 in} return(c(runif(Data$N), rnorm(Data$N), \\
\hspace*{0.27 in} rnorm(Data$N), runif(Data$N), phi, runif(Data$N), runif(Data$N), \\
\hspace*{0.27 in} rcat(Data$T, rep(1/Data$N,Data$N))))\} \\
MyData <- list(N=N, PGF=PGF, T=T, alpha=alpha, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.lambda=pos.lambda, pos.mu0=pos.mu0, \\
\hspace*{0.27 in} pos.mu1=pos.mu1, pos.nu=pos.nu, pos.phi=pos.phi, \\
\hspace*{0.27 in} pos.sigma0=pos.sigma0, pos.sigma2=pos.sigma2, pos.theta=pos.theta, \\
\hspace*{0.27 in} y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} lambda <- interval(parm[Data$pos.lambda], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.lambda] <- lambda \\
\hspace*{0.27 in} mu0 <- parm[Data$pos.mu0] \\
\hspace*{0.27 in} mu <- parm[Data$pos.mu1] \\
\hspace*{0.27 in} parm[Data$pos.mu1] <- mu <- mu[order(mu)] \\
\hspace*{0.27 in} parm[Data$pos.nu] <- nu <- interval(parm[Data$pos.nu], 1e-100, Inf) \\
\hspace*{0.27 in} phi <- abs(parm[Data$pos.phi]) \\
\hspace*{0.27 in} parm[Data$pos.phi] <- phi <- phi / sum(phi) \\
\hspace*{0.27 in} sigma0 <- interval(parm[Data$pos.sigma0], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma0] <- sigma0 \\
\hspace*{0.27 in} sigma2 <- interval(parm[Data$pos.sigma2], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma2] <- sigma2 \\
\hspace*{0.27 in} theta <- parm[Data$pos.theta] \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} lambda.prior <- sum(dhalfcauchy(lambda, 25, log=TRUE)) \\
\hspace*{0.27 in} mu0.prior <- sum(dnormv(mu0, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} mu.prior <- sum(dnormv(mu, mu0, lambda*sigma2, log=TRUE)) \\
\hspace*{0.27 in} nu.prior <- sum(dhalfcauchy(nu, 25, log=TRUE)) \\
\hspace*{0.27 in} phi.prior <- sum(ddirichlet(phi, Data$alpha, log=TRUE)) \\
\hspace*{0.27 in} sigma0.prior <- sum(dhalfcauchy(sigma0, 25, log=TRUE)) \\
\hspace*{0.27 in} sigma2.prior <- sum(dinvgamma(sigma2, nu, sigma0, log=TRUE)) \\
\hspace*{0.27 in} theta.prior <- sum(dcat(theta, phi, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} LL <- sum(dnormv(Data$y, mu[theta], sigma2[theta], log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + lambda.prior + mu0.prior + mu.prior + nu.prior + \\
\hspace*{0.62 in} phi.prior + sigma0.prior + sigma2.prior + theta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rnormv(length(theta), mu[theta], sigma2[theta]), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(runif(N), rnorm(N), rnorm(N), runif(N), runif(N),\\
\hspace*{0.27 in} runif(N), runif(N), rcat(T, rep(1/N,N)))}

\section{Inverse Gaussian Regression} \label{ig.reg}
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}^{-1}(\mu, \lambda)$$
$$\mu = \exp(\textbf{X}\beta) + C$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\lambda \sim \mathcal{HC}(25)$$
where $C$ is a small constant, such as 1.0E-10. 
\subsection{Data}
\code{N <- 100 \\
J <- 3 \#Number of predictors, including the intercept \\
X <- matrix(1,N,J) \\
for (j in 2:J) \{X[,j] <- rnorm(N,runif(1,-3,3),runif(1,0.1,1))\} \\
beta.orig <- runif(J,-3,3) \\
e <- rnorm(N,0,0.1) \\
y <- exp(tcrossprod(X, t(beta.orig)) + e) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=rep(0,J), lambda=0)) \\
pos.beta <- grep("beta", parm.names) \\
pos.lambda <- grep("lambda", parm.names) \\
PGF <- function(Data) return(c(rnormv(Data$J,0,1000), rhalfcauchy(1,5))) \\
MyData <- list(J=J, PGF=PGF, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.beta=pos.beta, pos.lambda=pos.lambda, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} lambda <- interval(parm[Data$pos.lambda], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.lambda] <- lambda \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} lambda.prior <- dhalfcauchy(lambda, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- exp(tcrossprod(Data$X, t(beta))) + 1.0E-10 \\
\hspace*{0.27 in} LL <- sum(dinvgaussian(Data$y, mu, lambda, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + lambda.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rinvgaussian(length(mu), mu, lambda), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), 1)}

\section{Kriging} \label{kriging}
This is an example of universal kriging of $\textbf{y}$ given $\textbf{X}$, regression effects $\beta$, and spatial effects $\zeta$. Euclidean distance between spatial coordinates (longitude and latitude) is used for each of $i=1,\dots,N$ records of $\textbf{y}$. An additional record is created from the same data-generating process to compare the accuracy of interpolation. For the spatial component, $\phi$ is the rate of spatial decay and $\kappa$ is the scale. $\kappa$ is often difficult to identify, so it is set to 1 (Gaussian), but may be allowed to vary up to 2 (Exponential). In practice, $\phi$ is also often difficult to identify. While $\Sigma$ is spatial covariance, spatial correlation is $\rho = \exp(-\phi \textbf{D})$. To extend this to a large data set, consider the predictive process kriging example in section \ref{kriging.pp}.
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2_1)$$
$$ \mu = \textbf{X} \beta + \zeta$$
$$ \textbf{y}^{new} = \textbf{X} \beta + \sum^N_{i=1} \left ( \frac{\rho_i}{\sum \rho} \zeta_i \right )$$
$$ \rho = \exp(-\phi \textbf{D}^{new})^\kappa$$
$$ \zeta \sim \mathcal{N}_N(\zeta_\mu, \Sigma)$$
$$ \Sigma = \sigma^2_2 \exp(-\phi \textbf{D})^\kappa$$
$$ \beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,2$$
$$ \sigma_j \sim \mathcal{HC}(25) \in [0.1,10], \quad j=1,\dots,2$$
$$ \phi \sim \mathcal{U}(1, 5)$$
$$ \zeta_\mu = 0$$
$$ \kappa = 1$$
\subsection{Data}
\code{N <- 20 \\
longitude <- runif(N+1,0,100) \\
latitude <- runif(N+1,0,100) \\
D <- as.matrix(dist(cbind(longitude,latitude), diag=TRUE, upper=TRUE)) \\
Sigma <- 10000 * exp(-1.5 * D) \\
zeta <- as.vector(apply(rmvn(1000, rep(0,N+1), Sigma), 2, mean)) \\
beta <- c(50,2) \\
X <- matrix(runif((N+1)*2,-2,2),(N+1),2); X[,1] <- 1 \\
mu <- as.vector(tcrossprod(X, t(beta))) \\
y <- mu + zeta \\
longitude.new <- longitude[N+1]; latitude.new <- latitude[N+1] \\
Xnew <- X[N+1,]; ynew <- y[N+1] \\
longitude <- longitude[1:N]; latitude <- latitude[1:N] \\
X <- X[1:N,]; y <- y[1:N] \\
D <- as.matrix(dist(cbind(longitude,latitude), diag=TRUE, upper=TRUE)) \\
D.new <- sqrt((longitude - longitude.new)\textasciicircum 2 + (latitude - latitude.new)\textasciicircum 2) \\
mon.names <- c("LP","ynew") \\
parm.names <- as.parm.names(list(zeta=rep(0,N), beta=rep(0,2), \\
\hspace*{0.27 in} sigma=rep(0,2), phi=0)) \\
pos.zeta <- grep("zeta", parm.names) \\
pos.beta <- grep("beta", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
pos.phi <- grep("phi", parm.names) \\
PGF <- function(Data) return(c(rmvn(1, rep(0, Data$N), \\
\hspace*{0.27 in} rhalfcauchy(1,25)\textasciicircum 2 *exp(-runif(1,1,5)*Data$D)), \\
\hspace*{0.27 in} rnormv(2,0,1000), rhalfcauchy(2,5), runif(1,1,5))) \\
MyData <- list(D=D, D.new=D.new, latitude=latitude, longitude=longitude, \\
\hspace*{0.27 in} N=N, PGF=PGF, X=X, Xnew=Xnew, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.zeta=pos.zeta, pos.beta=pos.beta, \\
\hspace*{0.27 in} pos.sigma=pos.sigma, pos.phi=pos.phi, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} zeta <- parm[Data$pos.zeta] \\
\hspace*{0.27 in} kappa <- 1 \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 0.1, 10) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} parm[Data$pos.phi] <- phi <- interval(parm[Data$pos.phi], 1, 5) \\
\hspace*{0.27 in} Sigma <- sigma[2]*sigma[2] * exp(-phi * Data$D)\textasciicircum kappa \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} zeta.prior <- dmvn(zeta, rep(0, Data$N), Sigma, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma - 1, 25, log=TRUE)) \\
\hspace*{0.27 in} phi.prior <- dunif(phi, 1, 5, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Interpolation \\
\hspace*{0.27 in} rho <- exp(-phi * Data$D.new)\textasciicircum kappa \\
\hspace*{0.27 in} ynew <- rnorm(1, sum(beta * Data$Xnew) + sum(rho / sum(rho) * zeta), \\
\hspace*{0.62 in} sigma[1]) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, t(beta)) + zeta \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma[1], log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + zeta.prior + sigma.prior + phi.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,ynew), \\
\hspace*{0.62 in} yhat=rnorm(length(mu), mu, sigma[1]), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,N), rep(0,2), rep(1,2), 1)}

\section{Kriging, Predictive Process} \label{kriging.pp}
The first $K$ of $N$ records in $\textbf{y}$ are used as knots for the parent process, and the predictive process involves records $(K+1),\dots,N$. For more information on kriging, see section \ref{kriging}.
\subsection{Form}
$$ \textbf{y} \sim \mathcal{N}(\mu, \sigma^2_1)$$
$$ \mu_{1:K} = \textbf{X}_{1:K,1:J} \beta + \zeta$$
$$ \mu_{(K+1):N} = \textbf{X}_{(K+1):N,1:J} \beta + \sum^{N-K}_{p=1} \frac{\lambda_{p,1:K}}{\sum^{N-K}_{q=1} \lambda_{q,1:K}} \zeta^T$$
$$ \lambda = \exp(-\phi \textbf{D}_P)^\kappa$$
$$ \textbf{y}^{new} = \textbf{X} \beta + \sum^K_{k=1} (\frac{\rho_k}{\sum \rho} \zeta_k)$$
$$ \rho = \exp(-\phi \textbf{D}^{new})^\kappa$$
$$ \zeta \sim \mathcal{N}_K(0, \Sigma)$$
$$ \Sigma = \sigma^2_2 \exp(-\phi \textbf{D})^\kappa$$
$$ \beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,2$$
$$ \sigma_j \sim \mathcal{HC}(25), \quad j=1,\dots,2$$
$$ \phi \sim \mathrm{N}(0, 1000) \in [1, 5]$$
$$ \kappa = 1$$
\subsection{Data}
\code{N <- 100 \\
K <- 30 \#Number of knots \\
longitude <- runif(N+1,0,100) \\
latitude <- runif(N+1,0,100) \\
D <- as.matrix(dist(cbind(longitude,latitude), diag=TRUE, upper=TRUE)) \\
Sigma <- 10000 * exp(-1.5 * D) \\
zeta <- as.vector(apply(rmvn(1000, rep(0,N+1), Sigma), 2, mean)) \\
beta <- c(50,2) \\
X <- matrix(runif((N+1)*2,-2,2),(N+1),2); X[,1] <- 1 \\
mu <- as.vector(tcrossprod(X, t(beta))) \\
y <- mu + zeta \\
longitude.new <- longitude[N+1]; latitude.new <- latitude[N+1] \\
Xnew <- X[N+1,]; ynew <- y[N+1] \\
longitude <- longitude[1:N]; latitude <- latitude[1:N] \\
X <- X[1:N,]; y <- y[1:N] \\
D <- as.matrix(dist(cbind(longitude[1:K],latitude[1:K]), diag=TRUE, \\
\hspace*{0.27 in} upper=TRUE)) \\
D.P <- matrix(0, N-K, K) \\
for (i in (K+1):N) \{ \\
\hspace*{0.27 in} D.P[K+1-i,] <- sqrt((longitude[1:K] - longitude[i])\textasciicircum 2 + \\
\hspace*{0.62 in} (latitude[1:K] - latitude[i])\textasciicircum 2)\} \\
D.new <- sqrt((longitude[1:K] - longitude.new)\textasciicircum 2 + \\
\hspace*{0.27 in} (latitude[1:K] - latitude.new)\textasciicircum 2) \\
mon.names <- c("LP","ynew") \\
parm.names <- as.parm.names(list(zeta=rep(0,K), beta=rep(0,2), \\
\hspace*{0.27 in} sigma=rep(0,2), phi=0)) \\
pos.zeta <- grep("zeta", parm.names) \\
pos.beta <- grep("beta", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
pos.phi <- grep("phi", parm.names) \\
PGF <- function(Data) return(c(rmvn(1, rep(0, Data$K), \\
\hspace*{0.27 in} rhalfcauchy(1,5)\textasciicircum 2 *exp(-runif(1,1,5)*Data$D)), \\
\hspace*{0.27 in} rnormv(2,0,1000), rhalfcauchy(2,5), runif(1,1,5))) \\
MyData <- list(D=D, D.new=D.new, D.P=D.P, K=K, N=N, PGF=PGF, X=X, \\
\hspace*{0.27 in} Xnew=Xnew, latitude=latitude, longitude=longitude, \\
\hspace*{0.27 in} mon.names=mon.names, parm.names=parm.names, pos.zeta=pos.zeta, \\
\hspace*{0.27 in} pos.beta=pos.beta, pos.sigma=pos.sigma, pos.phi=pos.phi, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} zeta <- parm[Data$pos.zeta] \\
\hspace*{0.27 in} kappa <- 1 \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} parm[Data$pos.phi] <- phi <- interval(parm[Data$pos.phi], 1, 5) \\
\hspace*{0.27 in} Sigma <- sigma[2]*sigma[2] * exp(-phi * Data$D)\textasciicircum kappa \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} zeta.prior <- dmvn(zeta, rep(0, Data$K), Sigma, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma - 1, 25, log=TRUE)) \\
\hspace*{0.27 in} phi.prior <- dunif(phi, 1, 5, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Interpolation \\
\hspace*{0.27 in} rho <- exp(-phi * Data$D.new)\textasciicircum kappa \\
\hspace*{0.27 in} ynew <- rnorm(1, sum(beta * Data$Xnew) + sum(rho / sum(rho) * zeta), \\
\hspace*{0.62 in} sigma) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, t(beta)) \\
\hspace*{0.27 in} mu[1:Data$K] <- mu[1:Data$K] + zeta \\
\hspace*{0.27 in} lambda <- exp(-phi * Data$D.P)\textasciicircum kappa \\
\hspace*{0.27 in} mu[(Data$K+1):Data$N] <- mu[(Data$K+1):Data$N] + \\
\hspace*{0.62 in} rowSums(lambda / rowSums(lambda) * \\
\hspace*{0.62 in} matrix(zeta, Data$N - Data$K, Data$K, byrow=TRUE)) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma[1], log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + zeta.prior + sigma.prior + phi.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,ynew), \\
\hspace*{0.62 in} yhat=rnorm(length(mu), mu, sigma[1]), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,K), c(mean(y), 0), rep(1,2), 3)}

\section{Laplace Regression} \label{laplace.reg}
This linear regression specifies that $\textbf{y}$ is Laplace-distributed, where it is usually Gaussian or normally-distributed.  It has been claimed that it should be surprising that the normal distribution became the standard, when the Laplace distribution usually fits better and has wider tails \citep{kotz01}. Another popular alternative is to use the t-distribution (see Robust Regression in section \ref{robust.reg}), though it is more computationally expensive to estimate, because it has three parameters.  The Laplace distribution has only two parameters, location and scale like the normal distribution, and is computationally easier to fit.  This example could be taken one step further, and the parameter vector $\beta$ could be Laplace-distributed.  Laplace's Demon recommends that users experiment with replacing the normal distribution with the Laplace distribution.
\subsection{Form}
$$\textbf{y} \sim \mathcal{L}(\mu, \sigma^2)$$
$$\mu = \textbf{X}\beta$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{N <- 10000 \\
J <- 5 \\
X <- matrix(1,N,J) \\
for (j in 2:J) \{X[,j] <- rnorm(N,runif(1,-3,3),runif(1,0.1,1))\} \\
beta <- runif(J,-3,3) \\
e <- rlaplace(N,0,0.1) \\
y <- tcrossprod(X, t(beta)) + e \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=rep(0,J), sigma=0)) \\
pos.beta <- grep("beta", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
PGF <- function(Data) return(c(rnormv(Data$J,0,1000), rhalfcauchy(1,5))) \\
MyData <- list(J=J, PGF=PGF, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.beta=pos.beta, pos.sigma=pos.sigma, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, t(beta)) \\
\hspace*{0.27 in} LL <- sum(dlaplace(Data$y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rlaplace(length(mu), mu, sigma), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), 1)}

\section{Latent Dirichlet Allocation} \label{lda}
\subsection{Form}
$$\textbf{Y}_{m,n} \sim \mathcal{CAT}(\phi[\textbf{Z}_{m,n},]), \quad m=1,\dots,M, \quad n=1,\dots,N$$
$$\textbf{Z}_{m,n} \sim \mathcal{CAT}(\theta_{m,1:K})$$
$$\phi_{k,v} \sim \mathcal{D}(\beta)$$
$$\theta_{m,k} \sim \mathcal{D}(\alpha)$$
$$\alpha_k = 1, \quad k=1,\dots,K$$
$$\beta_v = 1, \quad v=1,\dots,V$$
\subsection{Data}
\code{K <- 2 \#Number of (latent) topics \\
M <- 4 \#Number of documents in corpus \\
N <- 15 \#Maximum number of (used) words per document \\
V <- 5 \#Maximum number of occurrences of any word (Vocabulary size) \\
Y <- matrix(rcat(M*N,rep(1/V,V)), M, N) \\
rownames(Y) <- paste("doc", 1:nrow(Y), sep="") \\
colnames(Y) <- paste("word", 1:ncol(Y), sep="") \\
\#Note: Y is usually represented as w, a matrix of word counts. \\
if(min(Y) == 0) Y <- Y + 1 \#A zero cannot occur, Y must be 1,2,...,V. \\
V <- max(Y) \#Maximum number of occurrences of any word (Vocabulary size) \\
alpha <- rep(1,K) \# hyperparameters (constant) \\
beta <- rep(1,V) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(phi=matrix(0,K,V), theta=matrix(0,M,K), \\
\hspace*{0.27 in} Z=matrix(0,M,N))) \\
pos.phi <- grep("phi", parm.names) \\
pos.theta <- grep("theta", parm.names) \\
pos.Z <- grep("Z", parm.names) \\
PGF <- function(Data) \{ \\
\hspace*{0.27 in} phi <- matrix(runif(Data$J*Data$V), Data$K, Data$V) \\
\hspace*{0.27 in} phi <- phi / rowSums(phi) \\
\hspace*{0.27 in} theta <- matrix(runif(Data$M*Data$K), Data$M, Data$K) \\
\hspace*{0.27 in} theta <- theta / rowSums(theta) \\
\hspace*{0.27 in} z <- rcat(Data$M*Data$N, rep(1/Data$K,Data$K)) \\
\hspace*{0.27 in} return(c(as.vector(phi), as.vector(theta), z))\} \\
MyData <- list(K=K, M=M, N=N, PGF=PGF, V=V, Y=Y, alpha=alpha, beta=beta, \\
\hspace*{0.27 in} mon.names=mon.names, parm.names=parm.names, pos.phi=pos.phi, \\
\hspace*{0.27 in} pos.theta=pos.theta, pos.Z=pos.Z) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} phi <- matrix(interval(parm[Data$pos.phi], 0, 1), Data$K, Data$V) \\
\hspace*{0.27 in} phi <- phi / rowSums(phi) \\
\hspace*{0.27 in} parm[Data$pos.phi] <- as.vector(phi) \\
\hspace*{0.27 in} theta <- matrix(interval(parm[Data$pos.theta], 0, 1), Data$M, Data$K) \\
\hspace*{0.27 in} theta <- theta / rowSums(theta) \\
\hspace*{0.27 in} parm[Data$pos.theta] <- as.vector(theta) \\
\hspace*{0.27 in} Z <- matrix(parm[Data$pos.Z], Data$M, Data$N) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} phi.prior <- sum(ddirichlet(phi, beta, log=TRUE)) \\
\hspace*{0.27 in} theta.prior <- sum(ddirichlet(theta, alpha, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} LL <- Z.prior <- 0 \\
\hspace*{0.27 in} Yhat <- Data$Y \\
\hspace*{0.27 in} for (m in 1:Data$M) \{for (n in 1:Data$N) \{ \\
\hspace*{0.62 in} Z.prior + Z.prior + dcat(Z[m,n], theta[m,], log=TRUE) \\
\hspace*{0.62 in} LL <- LL + dcat(Data$Y[m,n], as.vector(phi[Z[m,n],]), log=TRUE) \\
\hspace*{0.62 in} Yhat[m,n] <- rcat(1, as.vector(phi[Z[m,n],]))\}\} \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + phi.prior + theta.prior + Z.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=Yhat, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(1/V,K*V), rep(1/K,M*K), rcat(M*N,rep(1/K,K)))}

\section{Linear Regression} \label{linear.reg}
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2)$$
$$\mu = \textbf{X}\beta$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{N <- 10000 \\
J <- 5 \\
X <- matrix(1,N,J) \\
for (j in 2:J) \{X[,j] <- rnorm(N,runif(1,-3,3),runif(1,0.1,1))\} \\
beta <- runif(J,-3,3) \\
e <- rnorm(N,0,0.1) \\
y <- tcrossprod(X, t(beta)) + e \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=rep(0,J), sigma=0)) \\
pos.beta <- grep("beta", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
PGF <- function(Data) return(c(rnormv(Data$J,0,1000), rhalfcauchy(1,5))) \\
MyData <- list(J=J, PGF=PGF, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.beta=pos.beta, pos.sigma=pos.sigma, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- dgamma(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, t(beta)) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rnorm(length(mu), mu, sigma), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), 1)}

\section{Linear Regression, Frequentist} \label{linear.reg.freq}
By eliminating prior probabilities, a frequentist linear regression example is presented. Although frequentism is not endorsed here, the purpose of this example is to illustrate how the \pkg{LaplacesDemon} package can be used for Bayesian or frequentist inference.
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2)$$
$$\mu = \textbf{X}\beta$$
\subsection{Data}
\code{N <- 10000 \\
J <- 5 \\
X <- matrix(1,N,J) \\
for (j in 2:J) \{X[,j] <- rnorm(N,runif(1,-3,3),runif(1,0.1,1))\} \\
beta <- runif(J,-3,3) \\
e <- rnorm(N,0,0.1) \\
y <- tcrossprod(X, t(beta)) + e \\
mon.names <- "LL" \\
parm.names <- as.parm.names(list(beta=rep(0,J), sigma=0)) \\
pos.beta <- grep("beta", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
PGF <- function(Data) return(c(rnormv(Data$J,0,1000), rhalfcauchy(1,5))) \\
MyData <- list(J=J, PGF=PGF, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.beta=pos.beta, pos.sigma=pos.sigma, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma, 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, t(beta)) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} Modelout <- list(LP=LL, Dev=-2*LL, Monitor=LL, \\
\hspace*{0.62 in} yhat=rnorm(length(mu), mu, sigma), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), 1)}

\section{Linear Regression, Hierarchical Bayesian} \label{linear.reg.hb}
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2)$$
$$\mu = \textbf{X}\beta$$
$$\beta_j \sim \mathcal{N}(\gamma, \delta), \quad j=1,\dots,J$$
$$\gamma \sim \mathcal{N}(0, 1000)$$
$$\delta \sim \mathcal{HC}(25)$$
$$\sigma \sim \mathcal{HC}(\tau)$$
$$\tau \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{data(demonsnacks) \\
y <- log(demonsnacks$Calories) \\
X <- cbind(1, as.matrix(log(demonsnacks[,c(1,4,10)]+1))) \\
J <- ncol(X) \\
for (j in 2:J) \{X[,j] <- CenterScale(X[,j])\} \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=rep(0,J), gamma=0, delta=0, sigma=0, \\
\hspace*{0.27 in} tau=0)) \\
pos.beta <- grep("beta", parm.names) \\
pos.gamma <- grep("gamma", parm.names) \\
pos.delta <- grep("delta", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
pos.tau <- grep("tau", parm.names) \\
PGF <- function(Data) return(c(rnormv(Data$J,0,1000), rnormv(1,0,1000), \\
\hspace*{0.27 in} rhalfcauchy(3,5))) \\
MyData <- list(J=J, PGF=PGF, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.beta=pos.beta, pos.gamma=pos.gamma, \\
\hspace*{0.27 in} \\ pos.delta=pos.delta, pos.sigma=pos.sigma, pos.tau=pos.tau, y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} gamma <- parm[Data$pos.gamma] \\
\hspace*{0.27 in} delta <- interval(parm[Data$pos.delta], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.delta] <- delta \\
\hspace*{0.27 in} parm[Data$pos.tau] <- tau <- interval(parm[Data$pos.tau], 1e-100, Inf) \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} \#\#\# Log(Hyperprior Densities) \\
\hspace*{0.27 in} gamma.prior <- dnormv(gamma, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} delta.prior <- dhalfcauchy(delta, 25, log=TRUE) \\
\hspace*{0.27 in} tau.prior <- dhalfcauchy(tau, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, gamma, delta, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, tau, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, t(beta)) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + gamma.prior + delta.prior + sigma.prior + \\
\hspace*{0.62 in} tau.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rnorm(length(mu), mu, sigma), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), 0, rep(1,3))}

\section{Linear Regression, Multilevel} \label{linear.reg.ml}
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2)$$
$$\mu_i = \textbf{X} \beta_{\textbf{m}[i],1:J}$$
$$\beta_{g,1:J} \sim \mathcal{N}_J(\gamma, \Omega^{-1}), \quad g=1,\dots,G$$
$$\Omega \sim \mathcal{W}_{J+1}(\textbf{S}), \quad \textbf{S} = \textbf{I}_J$$
$$\gamma_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\sigma \sim \mathcal{HC}(25)$$
where $\textbf{m}$ is a vector of length $N$, and each element indicates the multilevel group ($g=1,\dots,G$) for the associated record.
\subsection{Data}
\code{N <- 30 \\
J <- 2 \#\#\# Number of predictors (including intercept) \\
G <- 2 \#\#\# Number of Multilevel Groups \\
X <- matrix(rnorm(N,0,1),N,J); X[,1] <- 1 \\
Sigma <- matrix(runif(J*J,-1,1),J,J) \\
diag(Sigma) <- runif(J,1,5) \\
Sigma <- as.positive.definite(Sigma) \\
gamma <- runif(J,-1,1) \\
beta <- matrix(NA,G,J) \\
for (g in 1:G) \{beta[g,] <- rmvn(1, gamma, Sigma)\} \\
m <- rcat(N, rep(1/G,G)) \#\#\# Multilevel group indicator \\
y <- rowSums(beta[m,] * X) + rnorm(N,0,0.1) \\
S <- diag(J) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=matrix(0,G,J), gamma=rep(0,J), \\
\hspace*{0.27 in} sigma=0, U=S), uppertri=c(0,0,0,1)) \\
pos.beta <- grep("beta", parm.names) \\
pos.gamma <- grep("gamma", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
PGF <- function(Data) return(c(rmvnpc(Data$G, rnormv(Data$J,0,100), \\
\hspace*{0.27 in} rwishartc(Data$J+1, Data$S)), rnormv(Data$J,0,100), rhalfcauchy(1,5), \\
\hspace*{0.27 in} upper.triangle(rwishartc(Data$J+1, Data$S), diag=TRUE))) \\
MyData <- list(G=G, J=J, N=N, PGF=PGF, S=S, X=X, m=m, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.beta=pos.beta, pos.gamma=pos.gamma, \\
\hspace*{0.27 in} pos.sigma=pos.sigma, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- matrix(parm[Data$pos.beta], Data$G, Data$J) \\
\hspace*{0.27 in} gamma <- parm[Data$pos.gamma] \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} U <- as.parm.matrix(U, Data$J, parm, Data, chol=TRUE) \\
\hspace*{0.27 in} diag(U) <- exp(diag(U)) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} U.prior <- dwishartc(U, Data$J+1, Data$S, log=TRUE) \\
\hspace*{0.27 in} beta.prior <- sum(dmvnpc(beta, gamma, U, log=TRUE)) \\
\hspace*{0.27 in} gamma.prior <- sum(dnormv(gamma, 0, 100, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- rowSums(beta[Data$m,] * Data$X) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + U.prior + beta.prior + gamma.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rnorm(length(mu), mu, sigma), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial.Values}
\code{Initial.Values <- c(rep(0,G*J), rep(0,J), 1, \\
\hspace*{0.27 in} upper.triangle(S, diag=TRUE))}

\section{Linear Regression with Full Missingness} \label{linear.reg.full.miss}
With `full missingness', there are missing values for both the dependent variable (DV) and at least one independent variable (IV). The `full likelihood` approach to full missingness is excellent as long as the model is identifiable. When it is not identifiable, imputation may be done in a previous stage, such as with the \code{MISS} function. In this example, matrix $\alpha$ is for regression effects for IVs, vector $\beta$ is for regression effects for the DV, vector $\gamma$ is for missing values for IVs, and $\delta$ is for missing values for the DV.
\subsection{Form}
$$\textbf{y}^{imp} \sim \mathcal{N}(\nu, \sigma^2_J)$$
$$\textbf{X}^{imp} \sim \mathcal{N}(\mu, \sigma^2_{-J}$$
$$\nu = \textbf{X}^{imp} \beta$$
$$\mu = \textbf{X}^{imp} \alpha$$
\[\textbf{y}^{imp} = \left\{
\begin{array}{l l}
 $$\delta$$ & \quad \mbox{if $\textbf{y}^{mis}$}\\
 \textbf{y}^{obs} \\ \end{array} \right. \]
\[\textbf{X}^{imp} = \left\{
\begin{array}{l l}
 $$\gamma$$ & \quad \mbox{if $\textbf{X}^{mis}$}\\
 \textbf{X}^{obs} \\ \end{array} \right. \]
$$\alpha_{j,l} \sim \mathcal{N}(0, 1000), \quad j=1,\dots,(J-1), \quad l=1,\dots,(J-1)$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\gamma_m \sim \mathcal{N}(0, 1000), \quad m=1,\dots,M$$
$$\delta_p \sim \mathcal{N}(0, 1000), \quad p=1,\dots,P$$
$$\sigma_j \sim \mathcal{HC}(25), \quad j=1,\dots,J$$
\subsection{Data}
\code{N <- 100 \\
J <- 5 \\
X <- matrix(runif(N*J,-2,2),N,J); X[,1] <- 1 \#Design matrix X \\
M <- matrix(round(runif(N*J)-0.45),N,J); M[,1] <- 0 \#Missing indicators \\
X <- ifelse(M == 1, NA, X) \#Simulated X gets missings according to M \\
beta.orig <- runif(J,-2,2) \\
y <- as.vector(tcrossprod(X, t(beta.orig)) + rnorm(N,0,0.1)) \\
y[sample(1:N, round(N*.05))] <- NA \\
m <- ifelse(is.na(y), 1, 0) \#Missing indicator for vector y \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(alpha=matrix(0,J-1,J-1), \\
\hspace*{0.27 in} beta=rep(0,J), \\
\hspace*{0.27 in} gamma=rep(0,sum(is.na(X))), \\
\hspace*{0.27 in} delta=rep(0,sum(is.na(y))), \\
\hspace*{0.27 in} sigma=rep(0,J))) \\
pos.alpha <- grep("alpha", parm.names) \\
pos.beta <- grep("beta", parm.names) \\
pos.gamma <- grep("gamma", parm.names) \\
pos.delta <- grep("delta", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
PGF <- function(Data) return(c(rnormv((Data$J-1)*(Data$J-1),0,10), \\
\hspace*{0.27 in} rnormv(Data$J,0,10), \\
\hspace*{0.27 in} rnormv(sum(is.na(Data$X)),0,10), \\
\hspace*{0.27 in} rnormv(sum(is.na(Data$y)),mean(Data$y, na.rm=TRUE),1), \\
\hspace*{0.27 in} rhalfcauchy(Data$J,5))) \\
MyData <- list(J=J, N=N, PGF=PGF, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.alpha=pos.alpha, pos.beta=pos.beta, \\
\hspace*{0.27 in} pos.gamma=pos.gamma, pos.delta=pos.delta, pos.sigma=pos.sigma, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- matrix(parm[Data$pos.alpha], Data$J-1, Data$J-1) \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} gamma <- parm[Data$pos.gamma] \\
\hspace*{0.27 in} delta <- parm[Data$pos.delta] \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- sum(dnormv(alpha, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} gamma.prior <- sum(dnormv(gamma, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} delta.prior <- sum(dnormv(delta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- X.imputed <- Data$X \\
\hspace*{0.27 in} X.imputed[which(is.na(X.imputed))] <- gamma \\
\hspace*{0.27 in} y.imputed <- Data$y \\
\hspace*{0.27 in} y.imputed[which(is.na(y.imputed))] <- delta \\
\hspace*{0.27 in} for (j in 2:Data$J) \{mu[,j] <- tcrossprod(X.imputed[,-j], \\
\hspace*{0.62 in} t(alpha[,(j-1)]))\} \\
\hspace*{0.27 in} nu <- tcrossprod(X.imputed, t(beta)) \\
\hspace*{0.27 in} LL <- sum(dnorm(X.imputed[,-1], mu[,-1], \\
\hspace*{0.62 in} matrix(sigma[1:(Data$J-1)], Data$N, Data$J-1), log=TRUE), \\
\hspace*{0.62 in} dnorm(y.imputed, nu, sigma[Data$J], log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior + gamma.prior + delta.prior + \\
\hspace*{0.62 in} sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rnorm(length(nu), nu, sigma[Data$J]), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0, (J-1)\textasciicircum 2), rep(0,J), rep(0, sum(is.na(X))), \\
\hspace*{0.27 in} rep(0, sum(is.na(y))), rep(1,J))}

\section{Linear Regression with Missing Response} \label{linear.reg.miss.resp}
This is an introductory example to missing values using data augmentation with auxiliary variables. The dependent variable, or response, has both observed values, $\textbf{y}^{obs}$, and missing values, $\textbf{y}^{mis}$. The $\alpha$ vector is for missing value imputation, and enables the use of the full-likelihood by augmenting te state with these auxiliary variables. In the model form, $M$ is used to denote the number of missing values, though it is used as an indicator in the data.
\subsection{Form}
$$\textbf{y}^{imp} \sim \mathcal{N}(\mu, \sigma^2)$$
\[\textbf{y}^{imp} = \left\{
\begin{array}{l l}
 $$\alpha$$ & \quad \mbox{if $\textbf{y}^{mis}$}\\
 \textbf{y}^{obs} \\ \end{array} \right. \]
$$\mu = \textbf{X}\beta$$
$$\alpha_m \sim \mathcal{N}(0, 1000), \quad m=1,\dots,M$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{data(demonsnacks) \\
N <- nrow(demonsnacks) \\
J <- ncol(demonsnacks) \\
y <- log(demonsnacks$Calories) \\
y[sample(1:N, round(N*0.05))] <- NA \\
M <- ifelse(is.na(y), 1, 0) \\
X <- cbind(1, as.matrix(demonsnacks[,c(1,3:10)])) \\
for (j in 2:J) \{X[,j] <- CenterScale(X[,j])\} \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(alpha=rep(0,sum(M)), beta=rep(0,J), \\
\hspace*{0.27 in} sigma=0)) \\
pos.alpha <- grep("alpha", parm.names) \\
pos.beta <- grep("beta", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
PGF <- function(Data) return(c(rnorm(sum(Data$M),mean(y,na.rm=TRUE),1), \\
\hspace*{0.27 in} rnormv(Data$J,0,1000), rhalfcauchy(1,5))) \\
MyData <- list(J=J, M=M, PGF=PGF, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.alpha=pos.alpha, pos.beta=pos.beta, \\
\hspace*{0.27 in} pos.sigma=pos.sigma, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[Data$pos.alpha] \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- sum(dnormv(alpha, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- dgamma(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, t(beta)) \\
\hspace*{0.27 in} y.imputed <- Data$y \\
\hspace*{0.27 in} y.imputed[which(is.na(Data$y))] <- alpha \\
\hspace*{0.27 in} LL <- sum(dnorm(y.imputed, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rnorm(length(mu), mu, sigma), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,sum(M)), rep(0,J), 1)}

\section{Linear Regression with Missing Response via ABB} \label{linear.reg.miss.resp.abb}
The Approximate Bayesian Bootstrap (ABB), using the \code{ABB} function, is used to impute missing values in the dependent variable (DV), or response, given a propensity score. In this example, vector $\alpha$ is used to estimate propensity score $\eta$, while vector $\beta$ is for regression effects, and vector $\gamma$ has the monitored missing values. For more information on ABB, see the \code{ABB} function.
\subsection{Form}
$$\textbf{y}^{imp} \sim \mathcal{N}(\mu, \sigma^2)$$
\[\textbf{y}^{imp} = \left\{
\begin{array}{l l}
 $$\gamma$$ & \quad \mbox{if $\textbf{y}^{mis}$}\\
 \textbf{y}^{obs} \\ \end{array} \right. \]
$$\mu = \textbf{X}\beta$$
$$\gamma \sim p(\textbf{y}^{obs} | \textbf{y}^{obs}, \textbf{y}^{mis}, \eta)$$
$$\eta = \frac{1}{1 + \exp(-\nu)}$$
$$\nu = \textbf{X} \alpha$$
$$\alpha_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{data(demonsnacks) \\
N <- nrow(demonsnacks) \\
J <- ncol(demonsnacks) \\
y <- log(demonsnacks$Calories) \\
y[sample(1:N, round(N*0.05))] <- NA \\
M <- ifelse(is.na(y), 1, 0) \\
X <- cbind(1, as.matrix(demonsnacks[,c(1,3:10)])) \\
for (j in 2:J) \{X[,j] <- CenterScale(X[,j])\} \\
mon.names <- c("LP",paste("gamma[",1:sum(is.na(y)),"]",sep="")) \\
parm.names <- as.parm.names(list(alpha=rep(0,J), beta=rep(0,J), sigma=0)) \\
pos.alpha <- grep("alpha", parm.names) \\
pos.beta <- grep("beta", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
PGF <- function(Data) return(c(rnormv(Data$J,0,10), rnormv(Data$J,0,10), \\
\hspace*{0.27 in} rhalfcauchy(1,5))) \\
MyData <- list(J=J, M=M, PGF=PGF, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.alpha=pos.alpha, pos.beta=pos.beta,, 
\hspace*{0.27 in} pos.sigma=pos.sigma, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[Data$pos.alpha] \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- sum(dnormv(alpha, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- dgamma(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} y.imputed <- Data$y \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, t(beta)) \\
\hspace*{0.27 in} nu <- as.vector(tcrossprod(Data$X, t(alpha))) \\
\hspace*{0.27 in} eta <- invlogit(nu) \\
\hspace*{0.27 in} breaks <- as.vector(quantile(eta, probs=c(0,0.2,0.4,0.6,0.8,1))) \\
\hspace*{0.27 in} B <- matrix(breaks[-length(breaks)], length(Data$y), 5, byrow=TRUE) \\
\hspace*{0.27 in} z <- rowSums(eta >= B) \\
\hspace*{0.27 in} for (i in 1:5) \{ \\
\hspace*{0.62 in} if(any(is.na(Data$y[which(z == i)]))) \{ \\
\hspace*{0.95 in} imp <- unlist(ABB(Data$y[which(z == i)])) \\
\hspace*{0.95 in} y.imputed[which(\{z == i\} \& is.na(Data$y))] <- imp\}\} \\
\hspace*{0.27 in} gamma <- y.imputed[which(is.na(Data$y))] \\
\hspace*{0.27 in} LL <- sum(dbern(Data$M, eta, log=TRUE), \\
\hspace*{0.62 in} dnorm(y.imputed, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,gamma), \\
\hspace*{0.62 in} yhat=rnorm(length(mu), mu, sigma), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), rep(0,J), 1)}

\section{Linear Regression with Power Priors} \label{linear.reg.pp}
Power priors \citep{ibrahim00} are a class of informative priors when relevant historical data is available. Power priors may be used when it is desirable to take historical data into account while analyzing similar, current data. Both the current data, $\textbf{y}$ and $\textbf{X}$, and historical data, $\textbf{y}_h$ and $\textbf{X}_h$, are included in the power prior analysis, where $h$ indicates historical data. Each data set receives its own likelihood function, though the likelihood of the historical data is raised to an exponential power, $\alpha \in [0,1]$. In this example, $\alpha$ is a constant.
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2)$$
$$\textbf{y}_h \sim \mathcal{N}(\mu_h, \sigma^2)^\alpha$$
$$\mu = \textbf{X}\beta$$
$$\mu_h = \textbf{X}_h\beta$$
$$\alpha = 0.5$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{N <- 100 \\
J <- 5 \#Number of predictors, including the intercept \\
X <- Xh <- matrix(1,N,J) \\
for (j in 2:J) \{ \\
\hspace*{0.27 in} X[,j] <- rnorm(N,runif(1,-3,3),runif(1,0.1,1)) \\
\hspace*{0.27 in} Xh[,j] <- rnorm(N,runif(1,-3,3),runif(1,0.1,1))\} \\
beta.orig <- runif(J,-3,3) \\
e <- rnorm(N,0,0.1) \\
yh <- as.vector(tcrossprod(beta.orig, Xh) + e) \\
y <- as.vector(tcrossprod(beta.orig, X) + e) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=rep(0,J), sigma=0)) \\
pos.beta <- grep("beta", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
PGF <- function(Data) return(c(rnormv(Data$J,0,1000), rhalfcauchy(1,5))) \\
MyData <- list(alpha=0.5, J=J, PGF=PGF, X=X, Xh=Xh, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.beta=pos.beta, pos.sigma=pos.sigma, y=y, \\
\hspace*{0.27 in} yh=yh) \\
} \\
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} muh <- tcrossprod(Data$Xh, t(beta)) \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, t(beta)) \\
\hspace*{0.27 in} LL <- sum(Data$alpha*dnorm(Data$yh, muh, sigma, log=TRUE) + \\
\hspace*{0.62 in} dnorm(Data$y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rnorm(length(mu), mu, sigma), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), 1)}

\section{Linear Regression with Zellner's g-Prior} \label{linear.reg.g}
For more information on Zellner's g-prior, see the documentation for the \code{dzellner} function.
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2)$$
$$\mu = \textbf{X}\beta$$
$$\beta \sim \mathcal{N}_J(0, g \sigma^2 (\textbf{X}^T \textbf{X})^{-1})$$
$$g \sim \mathcal{HG}(\alpha), \quad \alpha = 3$$
$$\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{data(demonsnacks) \\
y <- log(demonsnacks$Calories) \\
X <- cbind(1, as.matrix(demonsnacks[,c(1,3:10)])) \\
J <- ncol(X) \\
for (j in 2:J) \{X[,j] <- CenterScale(X[,j])\} \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=rep(0,J), g0=0, sigma=0)) \\
pos.beta <- grep("beta", parm.names) \\
pos.g <- grep("g0", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
PGF <- function(Data) return(c(rnormv(Data$J,0,10), rhalfcauchy(2,5))) \\
MyData <- list(J=J, PGF=PGF, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.beta=pos.beta, pos.g=pos.g, 
\hspace*{0.27 in} pos.sigma=pos.sigma, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} parm[Data$pos.g] <- g <- interval(parm[Data$pos.g], 1e-100, Inf) \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} \#\#\# Log(Hyperprior Densities) \\
\hspace*{0.27 in} g.prior <- dhyperg(g, alpha=3, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- dzellner(beta, g, sigma, Data$X, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, t(beta)) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + g.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rnorm(length(mu), mu, sigma), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(1,J), rep(1,2))}

\section{LSTAR} \label{lstar}
This is a Logistic Smooth-Threshold Autoregression (LSTAR), and is specified with a transition function that includes $\gamma$ as the shape parameter, $\textbf{y}$ as the transition variable, $\theta$ as the location parameter, and $d$ as the delay parameter.
\subsection{Form}
$$\textbf{y}_t \sim \mathcal{N}(\mu_t, \sigma^2), \quad t=1,\dots,T$$
$$\mu_t = \pi_t (\alpha_1 + \phi_1 \textbf{y}_{t-1}) + (1 - \pi_t) (\alpha_2 + \phi_2 \textbf{y}_{t-1}), \quad t=2,\dots,T$$
$$\pi_t = \frac{1}{1 + \exp(-(\gamma (\textbf{y}_{t-d} - \theta)))}$$
$$\alpha_j \sim \mathcal{N}(0, 1000) \in [\textbf{y}_{min}, \textbf{y}_{max}], \quad j=1,\dots,2$$
$$\phi_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,2$$
$$\gamma \sim \mathcal{HC}(25)$$
$$\theta \sim \mathcal{U}(\textbf{y}_{min}, \textbf{y}_{max})$$
$$\pi_1 \sim \mathcal{U}(0.001, 0.999)$$
$$\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{y <- c(26.73, 26.75, 26.24, 25.94, 27.40, 26.14, 23.99, 23.08, 22.55, \\
\hspace*{0.27 in} 20.64, 23.28, 24.92, 25.07, 26.53, 28.14, 30.10, 27.43, 27.24, \\
\hspace*{0.27 in} 23.96, 25.85, 26.76, 26.05, 26.79, 26.69, 29.89, 29.09, 23.84, \\
\hspace*{0.27 in} 24.87, 24.47, 22.85, 22.05, 22.82, 22.99, 21.60, 20.32, 20.80, \\
\hspace*{0.27 in} 19.78, 19.87, 18.78, 19.64, 20.00, 21.51, 21.49, 21.96, 22.58, \\
\hspace*{0.27 in} 21.22, 22.34, 22.76, 18.37, 17.50, 17.55, 12.14,  4.76,  3.75, \\ 
\hspace*{0.27 in}  2.05,  2.69,  3.85,  4.72,  5.00,  3.31,  3.02,  3.15,  2.50, \\ 
\hspace*{0.27 in}  3.33,  3.95,  4.00,  3.86,  3.87,  3.51,  3.19,  2.39,  2.33, \\ 
\hspace*{0.27 in}  2.57,  2.80,  2.43,  2.43,  2.10,  2.31,  2.21,  2.11,  2.10, \\ 
\hspace*{0.27 in}  1.70,  1.35,  1.83,  1.55,  1.63,  1.91,  2.14,  2.41,  2.06, \\ 
\hspace*{0.27 in}  1.87,  2.11,  2.28,  2.26,  2.03,  2.06,  2.08,  1.91,  1.95, \\ 
\hspace*{0.27 in}  1.56,  1.44,  1.60,  1.77,  1.77,  1.95,  2.01,  1.65,  1.87, \\ 
\hspace*{0.27 in}  2.01,  1.84,  1.94,  1.93,  1.93,  1.75,  1.73,  1.80,  1.74, \\ 
\hspace*{0.27 in}  1.80,  1.75,  1.67,  1.60,  1.61,  1.55,  1.56,  1.57,  1.55, \\ 
\hspace*{0.27 in}  1.56,  1.57,  1.69,  1.66,  1.74,  1.64,  1.65,  1.62,  1.54, \\ 
\hspace*{0.27 in}  1.58,  1.49,  1.41,  1.42,  1.37,  1.45,  1.31,  1.37,  1.26, \\ 
\hspace*{0.27 in}  1.35,  1.41,  1.29,  1.28,  1.23,  1.08,  1.03,  1.00,  1.04, \\ 
\hspace*{0.27 in}  1.04,  0.92,  0.96,  0.90,  0.85,  0.78,  0.73,  0.59,  0.54, \\ 
\hspace*{0.27 in}  0.53,  0.41,  0.46,  0.52,  0.42,  0.42,  0.43,  0.43,  0.35, \\ 
\hspace*{0.27 in}  0.35,  0.35,  0.42,  0.41,  0.41,  0.50,  0.83,  0.96,  1.38, \\ 
\hspace*{0.27 in}  1.62,  1.26,  1.48,  1.39,  1.20,  1.10,  1.02,  0.95,  1.00, \\ 
\hspace*{0.27 in}  1.07,  1.14,  1.14,  1.10,  1.05,  1.08,  1.16,  1.42,  1.52, \\ 
\hspace*{0.27 in}  1.60,  1.69,  1.62,  1.29,  1.46,  1.43,  1.50,  1.46,  1.40, \\ 
\hspace*{0.27 in}  1.34,  1.41,  1.38,  1.38,  1.46,  1.73,  1.84,  1.95,  2.01, \\ 
\hspace*{0.27 in}  1.90,  1.81,  1.60,  1.84,  1.72,  1.83,  1.81,  1.78,  1.80, \\ 
\hspace*{0.27 in}  1.70,  1.70,  1.66,  1.67,  1.69,  1.66,  1.56,  1.47,  1.64, \\ 
\hspace*{0.27 in}  1.71,  1.66,  1.65,  1.60,  1.61,  1.61,  1.53,  1.48,  1.40, \\ 
\hspace*{0.27 in}  1.47,  1.53,  1.39,  1.41,  1.42,  1.46,  1.46,  1.33,  1.16) \\
T <- length(y) \\
mon.names <- c("LP", "ynew", "pi.new") \\
parm.names <- as.parm.names(list(alpha=rep(0,2), phi=rep(0,2), gamma=0, \\
\hspace*{0.27 in} theta=0, pi=0, sigma=0)) \\
pos.alpha <- grep("alpha", parm.names) \\
pos.phi <- grep("phi", parm.names) \\
pos.gamma <- grep("gamma", parm.names) \\
pos.theta <- grep("theta", parm.names) \\
pos.pi <- grep("pi", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
PGF <- function(Data) return(c(rnormv(2,0,10), rnormv(2,0,10), \\
\hspace*{0.27 in} rhalfcauchy(1,5), runif(1,min(Data$y),max(Data$y)), \\
\hspace*{0.27 in} runif(1,0.001,0.999), rhalfcauchy(1,5))) \\
MyData <- list(PGF=PGF, T=T, mon.names=mon.names, parm.names=parm.names, \\
\hspace*{0.27 in} pos.alpha=pos.alpha, pos.phi=pos.phi, pos.gamma=pos.gamma, \\
\hspace*{0.27 in} pos.theta=pos.theta, pos.pi=pos.pi, pos.sigma=pos.sigma, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- interval(parm[Data$pos.alpha], min(Data$y), max(Data$y)) \\
\hspace*{0.27 in} parm[Data$pos.alpha] <- alpha \\
\hspace*{0.27 in} parm[Data$pos.phi] <- phi <- interval(parm[Data$pos.phi], -1, 1) \\
\hspace*{0.27 in} gamma <- interval(parm[Data$pos.gamma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.gamma] <- gamma \\
\hspace*{0.27 in} theta <- interval(parm[Data$pos.theta], min(Data$y), max(Data$y)) \\
\hspace*{0.27 in} parm[Data$pos.theta] <- theta \\
\hspace*{0.27 in} parm[Data$pos.pi] <- pi <- interval(parm[Data$pos.pi], 0.001, 0.999) \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- sum(dnormv(alpha, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} phi.prior <- sum(dnormv(phi, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} gamma.prior <- dhalfcauchy(gamma, 25, log=TRUE) \\
\hspace*{0.27 in} theta.prior <- dunif(theta, min(Data$y), max(Data$y), log=TRUE) \\
\hspace*{0.27 in} pi.prior <- dunif(pi, 0.001, 0.999, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} pi <- c(pi, 1 / (1 + exp(-(gamma*(Data$y[-Data$T]-theta))))) \\
\hspace*{0.27 in} pi.new <- 1 / (1 + exp(-(gamma*(Data$y[Data$T]-theta)))) \\
\hspace*{0.27 in} mu <- pi * c(alpha[1], alpha[1] + phi[1]*Data$y[-Data$T]) + \\
\hspace*{0.62 in} (1-pi) * c(alpha[2], alpha[2] + phi[2]*Data$y[-Data$T]) \\
\hspace*{0.27 in} ynew <- rnorm(1, pi.new * (alpha[1] + phi[1]*Data$y[Data$T]) + \\
\hspace*{0.62 in} (1-pi.new) * (alpha[2] + phi[2]*Data$y[Data$T]), sigma) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + phi.prior + gamma.prior + theta.prior + \\
\hspace*{0.62 in} pi.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,sigma,ynew,pi.new), \\
\hspace*{0.62 in} yhat=rnorm(length(mu), mu, sigma), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(min(y),4), 1, mean(y), 0.5, 1)}

\section{MANCOVA} \label{mancova}
Since this is a multivariate extension of ANCOVA, please see the ANCOVA example in section \ref{ancova} for a univariate introduction.
\subsection{Form}
$$\textbf{Y}_{i,1:J} \sim \mathcal{N}_K(\mu_{i,1:J}, \Sigma), \quad i=1,\dots,N$$
$$\mu_{i,k} = \alpha_k + \beta_{k,\textbf{X}[i,1]} + \gamma_{k,\textbf{X}[i,1]} + \textbf{X}_{1:N,3:(C+J)} \delta_{k,1:C}$$
$$\epsilon_{i,k} = \textbf{Y}_{i,k} - \mu_{i,k}$$
$$\alpha_k \sim \mathcal{N}(0, 1000), \quad k=1,\dots,K$$
$$\beta_{k,l} \sim \mathcal{N}(0, \sigma^2_1), \quad l=1,\dots,(L-1)$$
$$\beta_{1:K,L} = - \sum^{L-1}_{l=1} \beta_{1:K,l}$$
$$\gamma_{k,m} \sim \mathcal{N}(0, \sigma^2_2), \quad m=1,\dots,(M-1)$$
$$\gamma_{1:K,M} = - \sum^{M-1}_{m=1} \beta_{1:K,m}$$
$$\delta_{k,c} \sim \mathcal{N}(0, 1000)$$
$$\Omega \sim \mathcal{W}_{K+1}(\textbf{S}), \quad \textbf{S} = \textbf{I}_K$$
$$\Sigma = \Omega^{-1}$$
$$\sigma_{1:J} \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{C <- 2 \#Number of covariates \\
J <- 2 \#Number of factors (treatments) \\
K <- 3 \#Number of endogenous (dependent) variables \\
L <- 4 \#Number of levels in factor (treatment) 1 \\
M <- 5 \#Number of levels in factor (treatment) 2 \\
N <- 100 \\
X <- matrix(cbind(rcat(N, rep(1/L,L)), rcat(N, rep(1/M,M)), \\
\hspace*{0.27 in} runif(C*N,0,1)), N, J + C) \\
alpha <- runif(K,-1,1) \\
beta <- matrix(runif(K*L,-2,2), K, L) \\
beta[,L] <- -rowSums(beta[,-L]) \\
gamma <- matrix(runif(K*M,-2,2), K, M) \\
gamma[,M] <- -rowSums(gamma[,-M]) \\
delta <- matrix(runif(K*C), K, C) \\
Y <- matrix(NA,N,K) \\
for (k in 1:K) \{ \\
\hspace*{0.27 in} Y[,k] <- alpha[k] + beta[k,X[,1]] + gamma[k,X[,2]] + \\
\hspace*{0.27 in} tcrossprod(delta[k,], X[,-c(1,2)]) + rnorm(1,0,0.1)\} \\
S <- diag(K) \\
mon.names <- c("LP", "s.o.beta", "s.o.gamma", "s.o.epsilon", \\
\hspace*{0.27 in} as.parm.names(list(s.beta=rep(0,K), s.gamma=rep(0,K), \\
\hspace*{0.27 in} s.epsilon=rep(0,K)))) \\
parm.names <- as.parm.names(list(alpha=rep(0,K), beta=matrix(0,K,(L-1)), \\
\hspace*{0.27 in} gamma=matrix(0,K,(M-1)), delta=matrix(0,K,C), U=diag(K), \\
\hspace*{0.27 in} sigma=rep(0,2)), uppertri=c(0,0,0,0,1,0)) \\
pos.alpha <- grep("alpha", parm.names) \\
pos.beta <- grep("beta", parm.names) \\
pos.gamma <- grep("gamma", parm.names) \\
pos.delta <- grep("delta", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
PGF <- function(Data) return(c(rnormv(Data$K,0,1000), \\
\hspace*{0.27 in} rnorm(Data$K*(Data$L-1),0,rhalfcauchy(1,5)), \\
\hspace*{0.27 in} rnorm(Data$K*(Data$M-1),0,rhalfcauchy(1,5)), \\
\hspace*{0.27 in} rnormv(Data$K*Data$C,0,1000), \\
\hspace*{0.27 in} upper.triangle(rwishartc(Data$K+1,Data$S), diag=TRUE), \\
\hspace*{0.27 in} rhalfcauchy(2,5))) \\
MyData <- list(C=C, J=J, K=K, L=L, M=M, N=N, PGF=PGF, S=S, X=X, Y=Y, \\
\hspace*{0.27 in} mon.names=mon.names, parm.names=parm.names, pos.alpha=pos.alpha, \\
\hspace*{0.27 in} pos.beta=pos.beta, pos.gamma=pos.gamma, pos.delta=pos.delta, \\
\hspace*{0.27 in} pos.sigma=pos.sigma) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[Data$pos.alpha] \\
\hspace*{0.27 in} beta <- matrix(c(parm[Data$pos.beta], rep(0,Data$K)), Data$K, Data$L) \\
\hspace*{0.27 in} beta[,Data$L] <- -rowSums(beta[,-Data$L]) \\
\hspace*{0.27 in} gamma <- matrix(c(parm[Data$[pos.gamma], \\
\hspace*{0.62 in} rep(0,Data$K)), Data$K, Data$M) \\
\hspace*{0.27 in} gamma[,Data$M] <- -rowSums(gamma[,-Data$M]) \\
\hspace*{0.27 in} delta <- matrix(parm[Data$pos.delta], Data$K, Data$C) \\
\hspace*{0.27 in} U <- as.parm.matrix(U, Data$K, parm, Data, chol=TRUE) \\
\hspace*{0.27 in} diag(U) <- exp(diag(U)) \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- sum(dnormv(alpha, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, sigma[1], log=TRUE)) \\
\hspace*{0.27 in} gamma.prior <- sum(dnorm(gamma, 0, sigma[2], log=TRUE)) \\
\hspace*{0.27 in} delta.prior <- sum(dnormv(delta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} U.prior <- dwishartc(U, Data$K+1, Data$S, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- matrix(0,Data$N,Data$K) \\
\hspace*{0.27 in} for (k in 1:Data$K) \{ \\
\hspace*{0.62 in} mu[,k] <- alpha[k] + beta[k,Data$X[,1]] + gamma[k,Data$X[,2]] + \\
\hspace*{0.62 in} tcrossprod(Data$X[,-c(1,2)], t(delta[k,]))\} \\
\hspace*{0.27 in} LL <- sum(dmvnpc(Data$Y, mu, U, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Variance Components, Omnibus \\
\hspace*{0.27 in} s.o.beta <- sd(as.vector(beta)) \\
\hspace*{0.27 in} s.o.gamma <- sd(as.vector(gamma)) \\
\hspace*{0.27 in} s.o.epsilon <- sd(as.vector(Data$Y - mu)) \\
\hspace*{0.27 in} \#\#\# Variance Components, Univariate \\
\hspace*{0.27 in} s.beta <- apply(beta,1,sd) \\
\hspace*{0.27 in} s.gamma <- apply(gamma,1,sd) \\
\hspace*{0.27 in} s.epsilon <- apply(Data$Y - mu,2,sd) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior + gamma.prior + delta.prior + \\
\hspace*{0.62 in} U.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, s.o.beta, s.o.gamma, \\
\hspace*{0.62 in} s.o.epsilon, s.beta, s.gamma, s.epsilon), \\
\hspace*{0.62 in} yhat=rmvnpc(nrow(mu), mu, U), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,K), rep(0,K*(L-1)), rep(0,K*(M-1)), \\
\hspace*{0.27 in} rep(0,C*K), upper.triangle(S, diag=TRUE), rep(1,2))}

\section{MANOVA} \label{manova}
Since this is a multivariate extension of ANOVA, please see the two-way ANOVA example in section \ref{anova.two.way} for a univariate introduction.
\subsection{Form}
$$\textbf{Y}_{i,1:J} \sim \mathcal{N}_K(\mu_{i,1:J}, \Omega^{-1}), \quad i=1,\dots,N$$
$$\mu_{i,k} = \alpha_k + \beta_{k,\textbf{X}[i,1]} + \gamma_{k,\textbf{X}[i,1]}$$
$$\epsilon_{i,k} = \textbf{Y}_{i,k} - \mu_{i,k}$$
$$\alpha_k \sim \mathcal{N}(0, 1000), \quad k=1,\dots,K$$
$$\beta_{k,l} \sim \mathcal{N}(0, \sigma^2_1), \quad l=1,\dots,(L-1)$$
$$\beta_{1:K,L} = - \sum^{L-1}_{l=1} \beta_{1:K,l}$$
$$\gamma_{k,m} \sim \mathcal{N}(0, \sigma^2_2), \quad m=1,\dots,(M-1)$$
$$\gamma_{1:K,M} = - \sum^{M-1}_{m=1} \beta_{1:K,m}$$
$$\Omega \sim \mathcal{W}_{K+1}(\textbf{S}), \quad \textbf{S} = \textbf{I}_K$$
$$\sigma_{1:J} \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{J <- 2 \#Number of factors (treatments) \\
K <- 3 \#Number of endogenous (dependent) variables \\
L <- 4 \#Number of levels in factor (treatment) 1 \\
M <- 5 \#Number of levels in factor (treatment) 2 \\
N <- 100 \\
X <- cbind(rcat(N, rep(1/L,L)), rcat(N, rep(1/M,M))) \\
alpha <- runif(K,-1,1) \\
beta <- matrix(runif(K*L,-2,2), K, L) \\
beta[,L] <- -rowSums(beta[,-L]) \\
gamma <- matrix(runif(K*M,-2,2), K, M) \\
gamma[,M] <- -rowSums(gamma[,-M]) \\
Y <- matrix(NA,N,K) \\
for (k in 1:K) \{ \\
\hspace*{0.27 in} Y[,k] <- alpha[k] + beta[k,X[,1]] + gamma[k,X[,2]] + rnorm(1,0,0.1)\} \\
S <- diag(K) \\
mon.names <- c("LP", "s.o.beta", "s.o.gamma", "s.o.epsilon", \\
\hspace*{0.27 in} as.parm.names(list(s.beta=rep(0,K), s.gamma=rep(0,K), \\
\hspace*{0.27 in} s.epsilon=rep(0,K)))) \\
parm.names <- as.parm.names(list(alpha=rep(0,K), beta=matrix(0,K,(L-1)), \\
\hspace*{0.27 in} gamma=matrix(0,K,(M-1)), U=diag(K), sigma=rep(0,2)), \\
\hspace*{0.27 in} uppertri=c(0,0,0,1,0)) \\
pos.alpha <- grep("alpha", parm.names) \\
pos.beta <- grep("beta", parm.names) \\
pos.gamma <- grep("gamma", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
PGF <- function(Data) return(c(rnormv(Data$K,0,1000), \\
\hspace*{0.27 in} rnorm(Data$K*(Data$L-1),0,rhalfcauchy(1,5)), \\
\hspace*{0.27 in} rnorm(Data$K*(Data$M-1),0,rhalfcauchy(1,5)), \\
\hspace*{0.27 in} upper.triangle(rwishartc(Data$K+1,Data$S), diag=TRUE), \\
\hspace*{0.27 in} rhalfcauchy(2,5))) \\
MyData <- list(J=J, K=K, L=L, M=M, N=N, PGF=PGF, S=S, X=X, Y=Y, \\
\hspace*{0.27 in} mon.names=mon.names, parm.names=parm.names, pos.alpha=pos.alpha, \\
\hspace*{0.27 in} pos.beta=pos.beta, pos.gamma=pos.gamma, pos.sigma=pos.sigma) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[Data$pos.alpha] \\
\hspace*{0.27 in} beta <- matrix(c(parm[Data$pos.beta], rep(0,Data$K)), \\
\hspace*{0.27 in} beta[,Data$L] <- -rowSums(beta[,-Data$L]) \\
\hspace*{0.27 in} gamma <- matrix(c(parm[Data$pos.gamma], \\
\hspace*{0.62 in} rep(0,Data$K)), Data$K, Data$M) \\
\hspace*{0.27 in} gamma[,Data$M] <- -rowSums(gamma[,-Data$M]) \\
\hspace*{0.27 in} U <- as.parm.matrix(U, Data$K, parm, Data, chol=TRUE) \\
\hspace*{0.27 in} diag(U) <- exp(diag(U)) \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- sum(dnormv(alpha, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, sigma[1], log=TRUE)) \\
\hspace*{0.27 in} gamma.prior <- sum(dnorm(gamma, 0, sigma[2], log=TRUE)) \\
\hspace*{0.27 in} U.prior <- dwishartc(U, Data$K+1, Data$S, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- matrix(0,Data$N,Data$K) \\
\hspace*{0.27 in} for (k in 1:Data$K) \{ \\
\hspace*{0.62 in} mu[,k] <- alpha[k] + beta[k,Data$X[,1]] + gamma[k,Data$X[,2]]\} \\
\hspace*{0.27 in} LL <- sum(dmvnpc(Data$Y, mu, U, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Variance Components, Omnibus \\
\hspace*{0.27 in} s.o.beta <- sd(as.vector(beta)) \\
\hspace*{0.27 in} s.o.gamma <- sd(as.vector(gamma)) \\
\hspace*{0.27 in} s.o.epsilon <- sd(as.vector(Data$Y - mu)) \\
\hspace*{0.27 in} \#\#\# Variance Components, Univariate \\
\hspace*{0.27 in} s.beta <- apply(beta,1,sd) \\
\hspace*{0.27 in} s.gamma <- apply(gamma,1,sd) \\
\hspace*{0.27 in} s.epsilon <- apply(Data$Y - mu,2,sd) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior + gamma.prior + U.prior + \\
\hspace*{0.62 in} sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, s.o.beta, s.o.gamma, \\
\hspace*{0.62 in} s.o.epsilon, s.beta, s.gamma, s.epsilon), \\
\hspace*{0.62 in} yhat=rmvnpc(nrow(mu), mu, U), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,K), rep(0,K*(L-1)), rep(0,K*(M-1)), \\
\hspace*{0.27 in} upper.triangle(S, diag=TRUE), rep(1,2))}

\section{Mixture Model, Finite} \label{fmm}
This finite mixture model (FMM) imposes a multilevel structure on each of the $J$ regression effects in $\beta$, so that mixture components share a common residual standard deviation, $\nu_j$. Identifiability is gained at the expense of some shrinkage. The record-level mixture membership parameter vector, $\theta$, is a vector of discrete parameters. Discrete parameters are not supported in all algorithms. The example below is updated with the Griddy-Gibbs sampler.
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu_{1:N,\theta}, \sigma^2)$$
$$\theta_i \sim \mathcal{CAT}(\pi_{1:M}), \quad i=1,\dots,N$$
$$\mu_{1:N,\theta} = \textbf{X}\beta_{\theta,1:J}$$
$$\beta_{m,j} \sim \mathcal{N}(0, \nu^2_j), \quad j=1,\dots,J, \quad m=1,\dots,M$$
$$\nu_j \sim \mathcal{HC}(25)$$
$$\sigma \sim \mathcal{HC}(25)$$
$$\pi_{1:M} \sim \mathcal{D}(\alpha_{1:M})$$
$$\alpha_m = 1$$
\subsection{Data}
\code{M <- 2 \#Number of mixtures \\
alpha <- rep(1,M) \#Prior probability of mixing probabilities \\
data(demonsnacks) \\
N <- nrow(demonsnacks) \\
y <- log(demonsnacks$Calories) \\
X <- cbind(1, as.matrix(log(demonsnacks[,c(1,4,10)]+1))) \\
J <- ncol(X) \\
for (j in 2:J) \{X[,j] <- CenterScale(X[,j])\} \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(theta=rep(0,N), beta=matrix(0,M,J), \\
\hspace*{0.27 in} nu=rep(0,J), sigma=0)) \\
pos.theta <- grep("theta", parm.names) \\
pos.beta <- grep("beta", parm.names) \\
pos.nu <- grep("nu", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
PGF <- function(Data) return(c(rcat(Data$N, rep(1/Data$M, Data$M)), \\
\hspace*{0.27 in} rnorm(Data$M*Data$J,0,1), rhalfcauchy(Data$J+1,5))) \\
MyData <- list(J=J, M=M, N=N, PGF=PGF, X=X, alpha=alpha, \\
\hspace*{0.27 in} mon.names=mon.names, parm.names=parm.names, pos.theta=pos.theta, \\
\hspace*{0.27 in} pos.beta=pos.beta, pos.nu=pos.nu, pos.sigma=pos.sigma, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- matrix(parm[Data$pos.beta], Data$M, Data$J) \\
\hspace*{0.27 in} theta <- parm[Data$pos.theta] \\
\hspace*{0.27 in} parm[Data$pos.nu] <- nu <- interval(parm[Data$pos.nu], 1e-100, Inf) \\
\hspace*{0.27 in} pi <- rep(0, Data$M) \\
\hspace*{0.27 in} tab <- table(theta) \\
\hspace*{0.27 in} pi[as.numeric(names(tab))] <- as.vector(tab) \\
\hspace*{0.27 in} pi <- pi / sum(pi) \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, matrix(rep(nu, Data$M), Data$M, \\
\hspace*{0.62 in} Data$J, byrow=TRUE), log=TRUE)) \\
\hspace*{0.27 in} theta.prior <- sum(dcat(theta, p=pi, log=TRUE)) \\
\hspace*{0.27 in} pi.prior <- ddirichlet(pi, Data$alpha, log=TRUE) \\
\hspace*{0.27 in} nu.prior <- sum(dhalfcauchy(nu, 25, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, beta) \\
\hspace*{0.27 in} mu <- diag(mu[,theta]) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + theta.prior + pi.prior + nu.prior + \\
\hspace*{0.62 in} sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rnorm(length(mu), mu, sigma), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rcat(N,rep(1/M,M)), rep(0,M*J), rep(1,J), 1)}

\section{Mixture Model, Infinite} \label{imm}
This infinite mixture model (IMM) imposes a multilevel structure on each of the $J$ regression effects in $\beta$, so that mixture components share a common residual standard deviation, $\nu_j$. The infinite number of mixture components is truncated to a finite number, and the user specifies the maximum number to explore, $M$, where $M$ is discrete, greater than one, and less than the number of records, $N$. A truncated stick-breaking process within a truncated Dirichlet process defines the nonparametric mixture component selection. The record-level mixture membership parameter vector, $\theta$, is a vector of discrete parameters. Discrete parameters are not supported in all algorithms. The example below is updated with the Griddy-Gibbs sampler.
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu_{1:N,\theta}, \sigma^2)$$
$$\mu_{1:N,\theta} = \textbf{X}\beta_{\theta,1:J}$$
$$\theta_i \sim \mathcal{CAT}(\pi_{1:M})$$
$$\beta_{m,j} \sim \mathcal{N}(0, \nu^2_j), \quad j=1,\dots,J, \quad m=1,\dots,M$$
$$\nu_j \sim \mathcal{HC}(25)$$
$$\sigma \sim \mathcal{HC}(25)$$
$$\pi \sim \mathrm{Stick}(\gamma)$$
$$\alpha \sim \mathcal{HC}(25)$$
$$\iota \sim \mathcal{HC}(25)$$
$$\gamma \sim \mathcal{G}(\alpha, \iota)$$
\subsection{Data}
\code{M <- 3 \#Maximum number of mixtures to explore \\
data(demonsnacks) \\
N <- nrow(demonsnacks) \\
y <- log(demonsnacks$Calories) \\
X <- cbind(1, as.matrix(log(demonsnacks[,c(1,4,10)]+1))) \\
J <- ncol(X) \\
for (j in 2:J) \{X[,j] <- CenterScale(X[,j])\} \\
mon.names <- c("LP", as.parm.names(list(pi=rep(0,M)))) \\
parm.names <- as.parm.names(list(theta=rep(0,N), beta=matrix(0,M,J), \\
\hspace*{0.27 in} nu=rep(0,J), sigma=0, alpha=0, iota=0, gamma=0)) \\
pos.theta <- grep("theta", parm.names) \\
pos.beta <- grep("beta", parm.names) \\
pos.nu <- grep("nu", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
pos.alpha <- grep("alpha", parm.names) \\
pos.iota <- grep("iota", parm.names) \\
pos.gamma <- grep("gamma", parm.names) \\
PGF <- function(Data) return(c(rcat(Data$N, rep(1/Data$M,Data$M)), \\
\hspace*{0.27 in} rnorm(Data$M*Data$J,0,1), rhalfcauchy(Data$J,5), rhalfcauchy(1,5), \\
\hspace*{0.27 in} rhalfcauchy(2,5), rgamma(1,rhalfcauchy(1,25),rhalfcauchy(1,5)))) \\
MyData <- list(J=J, M=M, N=N, PGF=PGF, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.theta=pos.theta, pos.beta=pos.beta, \\
\hspace*{0.27 in} pos.nu=pos.nu, pos.sigma=pos.sigma, pos.alpha=pos.alpha, \\
\hspace*{0.27 in} pos.iota=pos.iota, pos.gamma=pos.gamma, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperhyperparameters \\
\hspace*{0.27 in} alpha <- interval(parm[Data$pos.alpha], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.alpha] <- alpha \\
\hspace*{0.27 in} iota <- interval(parm[Data$pos.iota], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.iota] <- iota \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} gamma <- interval(parm[Data$pos.gamma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.gamma] <- gamma \\
\hspace*{0.27 in} parm[Data$pos.nu] <- nu <- interval(parm[Data$pos.nu], 1e-100, Inf) \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- matrix(parm[Data$pos.beta], Data$M, Data$J) \\
\hspace*{0.27 in} theta <- parm[Data$pos.theta] \\
\hspace*{0.27 in} pi <- rStick(Data$M-1, gamma) \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} \#\#\# Log(Hyperhyperprior Densities) \\
\hspace*{0.27 in} alpha.prior <- dhalfcauchy(alpha, 25, log=TRUE) \\
\hspace*{0.27 in} iota.prior <- dhalfcauchy(iota, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log(Hyperprior Densities) \\
\hspace*{0.27 in} gamma.prior <- dgamma(gamma, alpha, iota, log=TRUE) \\
\hspace*{0.27 in} nu.prior <- sum(dhalfcauchy(nu, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, matrix(rep(nu, Data$M), Data$M, \\
\hspace*{0.62 in} Data$J, byrow=TRUE), log=TRUE)) \\
\hspace*{0.27 in} theta.prior <- sum(dcat(theta, pi, log=TRUE)) \\
\hspace*{0.27 in} pi.prior <- dStick(pi, gamma, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, beta) \\
\hspace*{0.27 in} mu <- diag(mu[,theta]) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + theta.prior + pi.prior + nu.prior + \\
\hspace*{0.62 in} sigma.prior + alpha.prior + iota.prior + gamma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,pi), \\
\hspace*{0.62 in} yhat=rnorm(length(mu), mu, sigma), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rcat(N, rep(1/M,M)), rep(0,M*J), rep(1,J), rep(1,4))}

\section{Mixture Model, Poisson-Gamma} \label{poisson.gamma}
\subsection{Form}
$$\textbf{y} \sim \mathcal{P}(\lambda)$$
$$\lambda \sim \mathcal{G}(\alpha \mu, \alpha)$$
$$\mu = \exp(\textbf{X}\beta)$$
$$\alpha \sim \mathcal{HC}(25)$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
\subsection{Data}
\code{N <- 20 \\
J <- 3 \\
X <- matrix(runif(N*J,-2,2),N,J); X[,1] <- 1 \\
beta <- runif(J,-2,2) \\
y <- as.vector(round(exp(tcrossprod(X, t(beta))))) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(alpha=0, beta=rep(0,J), lambda=rep(0,N))) \\
pos.alpha <- grep("alpha", parm.names) \\
pos.beta <- grep("beta", parm.names) \\
pos.lambda <- grep("lambda", parm.names) \\
PGF <- function(Data) return(c(rhalfcauchy(1,5), \\
\hspace*{0.27 in} rnormv(Data$J,0,1), rgamma(Data$N, \\
\hspace*{0.27 in} exp(tcrossprod(Data$X, t(rnormv(Data$J,0,1))))*rhalfcauchy(1,5), \\
\hspace*{0.27 in} rhalfcauchy(1,5)))) \\
MyData <- list(J=J, N=N, PGF=PGF, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.alpha=pos.alpha, pos.beta=pos.beta, \\
\hspace*{0.27 in} pos.lambda=pos.lambda, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} alpha <- interval(parm[Data$pos.alpha], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.alpha] <- alpha \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} lambda <- interval(parm[Data$pos.lambda], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.lambda] <- lambda \\
\hspace*{0.27 in} mu <- exp(tcrossprod(Data$X, t(beta))) \\
\hspace*{0.27 in} \#\#\# Log(Hyperprior Densities) \\
\hspace*{0.27 in} alpha.prior <- dhalfcauchy(alpha, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} lambda.prior <- sum(dgamma(lambda, alpha*mu, alpha, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} LL <- sum(dpois(Data$y, lambda, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior + lambda.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rpois(length(lambda), lambda), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(1, rep(0,J), rep(1,N))}

\section{Multinomial Logit} \label{mnl}
\subsection{Form}
$$\textbf{y}_i \sim \mathcal{CAT}(\textbf{p}_{i,1:J}), \quad i=1,\dots,N$$
$$\textbf{p}_{i,j} = \frac{\phi_{i,j}}{\sum^J_{j=1} \phi_{i,j}}, \quad \sum^J_{j=1} \textbf{p}_{i,j} = 1$$
$$\phi = \exp(\mu)$$
$$\mu_{i,J} = 0, \quad i=1,\dots,N$$
$$\mu_{i,j} = \textbf{X}_{i,1:K} \beta_{j,1:K} \in [-700,700], \quad j=1,\dots,(J-1)$$
$$\beta_{j,k} \sim \mathcal{N}(0, 1000), \quad j=1,\dots,(J-1), \quad k=1,\dots,K$$
\subsection{Data}
\code{y <- x01 <- x02 <- c(1:300) \\
y[1:100] <- 1 \\
y[101:200] <- 2 \\
y[201:300] <- 3 \\
x01[1:100] <- rnorm(100, 25, 2.5) \\
x01[101:200] <- rnorm(100, 40, 4.0) \\
x01[201:300] <- rnorm(100, 35, 3.5) \\
x02[1:100] <- rnorm(100, 2.51, 0.25) \\
x02[101:200] <- rnorm(100, 2.01, 0.20) \\
x02[201:300] <- rnorm(100, 2.70, 0.27) \\
N <- length(y) \\
J <- 3 \#Number of categories in y \\
K <- 3 \#Number of predictors (including the intercept) \\
X <- matrix(c(rep(1,N),x01,x02),N,K) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=matrix(0,J-1,K))) \\
PGF <- function(Data) return(rnormv((Data$J-1)*Data$K,0,1000)) \\
MyData <- list(J=J, K=K, N=N, PGF=PGF, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- matrix(parm, Data$J-1, Data$K) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- matrix(0, Data$N, Data$J) \\
\hspace*{0.27 in} mu[,-Data$J] <- tcrossprod(Data$X, beta) \\
\hspace*{0.27 in} mu <- interval(mu, -700, 700, reflect=FALSE) \\
\hspace*{0.27 in} phi <- exp(mu) \\
\hspace*{0.27 in} p <- phi / rowSums(phi) \\
\hspace*{0.27 in} LL <- sum(dcat(Data$y, p, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=rcat(nrow(p), p), \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,(J-1)*K))}

\section{Multinomial Logit, Nested} \label{nmnl}
\subsection{Form}
$$\textbf{y}_i \sim \mathcal{CAT}(\textbf{P}_{i,1:J}), \quad i=1,\dots,N$$
$$\textbf{P}_{1:N,1} = \frac{\textbf{R}}{\textbf{R} + \exp(\alpha \textbf{I})}$$
$$\textbf{P}_{1:N,2} = \frac{(1 - \textbf{P}_{1:N,1}) \textbf{S}_{1:N,1}}{\textbf{V}}$$
$$\textbf{P}_{1:N,3} = \frac{(1 - \textbf{P}_{1:N,1}) \textbf{S}_{1:N,2}}{\textbf{V}}$$
$$\textbf{R}_{1:N} = \exp(\mu_{1:N,1})$$
$$\textbf{S}_{1:N,1:2} = \exp(\mu_{1:N,2:3})$$
$$\textbf{I} = \log(\textbf{V})$$
$$\textbf{V}_i = \displaystyle\sum^K_{k=1} \textbf{S}_{i,k}, \quad i=1,\dots,N$$
$$\mu_{1:N,1} = \textbf{X} \iota \in [-700,700]$$
$$\mu_{1:N,2} = \textbf{X} \beta_{2,1:K} \in [-700,700]$$
$$\iota = \alpha \beta_{1,1:K}$$
$$\alpha \sim \mathcal{EXP}(1) \in [0,2]$$
$$\beta_{j,k} \sim \mathcal{N}(0, 1000), \quad j=1,\dots,(J-1) \quad k=1,\dots,K$$
where there are $J=3$ categories of $\textbf{y}$, $K=3$ predictors, $\textbf{R}$ is the non-nested alternative, $\textbf{S}$ is the nested alternative, $\textbf{V}$ is the observed utility in the nest, $\alpha$ is effectively 1 - correlation and has a truncated exponential distribution, and $\iota$ is a vector of regression effects for the isolated alternative after $\alpha$ is taken into account. The third alternative is the reference category.
\subsection{Data}
\code{y <- x01 <- x02 <- c(1:300) \\
y[1:100] <- 1 \\
y[101:200] <- 2 \\
y[201:300] <- 3 \\
x01[1:100] <- rnorm(100, 25, 2.5) \\
x01[101:200] <- rnorm(100, 40, 4.0) \\
x01[201:300] <- rnorm(100, 35, 3.5) \\
x02[1:100] <- rnorm(100, 2.51, 0.25) \\
x02[101:200] <- rnorm(100, 2.01, 0.20) \\
x02[201:300] <- rnorm(100, 2.70, 0.27) \\
N <- length(y) \\
J <- 3 \#Number of categories in y \\
K <- 3 \#Number of predictors (including the intercept) \\
X <- matrix(c(rep(1,N),x01,x02),N,K) \\
mon.names <- c("LP", as.parm.names(list(iota=rep(0,K)))) \\
parm.names <- as.parm.names(list(alpha=0, beta=matrix(0,J-1,K))) \\
pos.alpha <- grep("alpha", parm.names) \\
pos.beta <- grep("beta", parm.names) \\
PGF <- function(Data) return(c(rtrunc(1,"exp",a=0,b=2,rate=1), \\
\hspace*{0.27 in} rnormv((Data$J-1)*Data$K,0,1))) \\
MyData <- list(J=J, K=K, N=N, PGF=PGF, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.alpha=pos.alpha, pos.beta=pos.beta, y=y)
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} alpha.rate <- 1 \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} parm[Data$pos.alpha] <- alpha <- interval(parm[Data$pos.alpha],0,2) \\
\hspace*{0.27 in} beta <- matrix(parm[Data$pos.beta], Data$J-1, Data$K) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dtrunc(alpha, "exp", a=0, b=2, rate=alpha.rate, \\
\hspace*{0.62 in} log=TRUE) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- P <- matrix(0, Data$N, Data$J) \\
\hspace*{0.27 in} iota <- alpha * beta[1,] \\
\hspace*{0.27 in} mu[,1] <- tcrossprod(Data$X, t(iota)) \\
\hspace*{0.27 in} mu[,2] <- tcrossprod(Data$X, t(beta[2,])) \\
\hspace*{0.27 in} mu <- interval(mu, -700, 700, reflect=FALSE) \\
\hspace*{0.27 in} R <- exp(mu[,1]) \\
\hspace*{0.27 in} S <- exp(mu[,-1]) \\
\hspace*{0.27 in} V <- rowSums(S) \\
\hspace*{0.27 in} I <- log(V) \\
\hspace*{0.27 in} P[,1] <- R / (R + exp(alpha*I)) \\
\hspace*{0.27 in} P[,2] <- (1 - P[,1]) * S[,1] / V \\
\hspace*{0.27 in} P[,3] <- (1 - P[,1]) * S[,2] / V \\
\hspace*{0.27 in} LL <- sum(dcat(Data$y, P, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,iota), \\
\hspace*{0.62 in} yhat=rcat(nrow(P), P), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(0.5, rep(0.1,(J-1)*K))}

\section{Multinomial Probit} \label{mnp}
\subsection{Form}
$$\textbf{W}_{i,1:(J-1)} \sim \mathcal{N}_{J-1}(\mu_{i,1:(J-1)}, \Sigma), \quad i=1,\dots,N$$
\[\textbf{W}_{i,j} \in \left\{
\begin{array}{l l}
 $[0,10]$ & \quad \mbox{if $\textbf{y}_i = j$}\\
 $[-10,0]$ \\ \end{array} \right. \]
$$\mu_{1:N,j} = \textbf{X} \beta_{j,1:K}$$
$$\Sigma = \textbf{U}^T \textbf{U}$$
$$\beta_{j,k} \sim \mathcal{N}(0, 10), \quad j=1,\dots,(J-1), \quad k=1,\dots,K$$
$$\textbf{U}_{j,k} \sim \mathcal{N}(0,1), \quad j=1,\dots,(J-1), \quad k=1,\dots,(J-1), \quad j \ge k, \quad j \ne k = 1$$
\subsection{Data}
\code{N <- 50 \\
J <- 5 \#Categories of y \\
K <- 8 \#Number of columns in design matrix X \\
X <- matrix(runif(N*K,-2,2), N, K) \\
X[,1] <- 1 \\
beta <- matrix(runif((J-1)*K), J-1, K) \\
mu <- tcrossprod(X, beta) \\
S <- diag(J-1) \\
u <- c(0, rnorm((J-2) + (factorial(J-1) / \\
\hspace*{0.27 in} (factorial(J-1-2)*factorial(2))),0,1)) \\
U <- diag(J-1) \\
U[upper.tri(U, diag=TRUE)] <- u \\
diag(U) <- exp(diag(U)) \\
Sigma <- t(U) \%*\% U \\
Sigma[1,] <- Sigma[,1] <- U[1,] \\
mu <- tcrossprod(X, beta) \\
W <- rmvn(N, mu, Sigma) + matrix(rnorm(N*(J-1),0,0.1), N, J-1) \\
y <- max.col(cbind(W,0)) \\
table(y) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=matrix(0,(J-1),K), \\
\hspace*{0.27 in} U=U, W=matrix(0,N,J-1)), uppertri=c(0,1,0)) \\
parm.names <- parm.names[-which(parm.names == "U[1,1]")] \\
pos.beta <- grep("beta", parm.names) \\
pos.U <- grep("U", parm.names) \\
pos.W <- grep("W", parm.names) \\
PGF <- function(Data) \{ \\
\hspace*{0.27 in} beta <- rnormv((Data$J-1)*Data$K,0,1) \\
\hspace*{0.27 in} U <- rnorm((Data$J-2) + (factorial(Data$J-1) / \\
\hspace*{0.62 in} (factorial(Data$J-1-2)*factorial(2))),0,1) \\
\hspace*{0.27 in} W <- matrix(runif(Data$N*(Data$J-1),-10,0), Data$N, Data$J-1) \\
\hspace*{0.27 in} Y <- as.indicator.matrix(Data$y) \\
\hspace*{0.27 in} W <- ifelse(Y[,-Data$J] == 1, abs(W), W) \\
\hspace*{0.27 in} return(c(beta, U, as.vector(W)))\} \\
MyData <- list(J=J, K=K, N=N, PGF=PGF, S=S, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.beta=pos.beta, pos.U=pos.U, pos.W=pos.W, \\
\hspace*{0.27 in} y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- matrix(parm[Data$pos.beta], Data$J-1, Data$K) \\
\hspace*{0.27 in} u <- c(0, parm[Data$pos.U]) \\
\hspace*{0.27 in} U <- diag(Data$J-1) \\
\hspace*{0.27 in} U[upper.tri(U, diag=TRUE)] <- u \\
\hspace*{0.27 in} diag(U) <- exp(diag(U)) \\
\hspace*{0.27 in} Sigma <- t(U) \%*\% U \\
\hspace*{0.27 in} Sigma[1,] <- Sigma[,1] <- U[1,] \\
\hspace*{0.27 in} W <- matrix(parm[Data$pos.W], Data$N, Data$J-1) \\
\hspace*{0.27 in} Y <- as.indicator.matrix(Data$y) \\
\hspace*{0.27 in} temp <- which(Y[,-c(Data$J)] == 1) \\
\hspace*{0.27 in} W[temp] <- interval(W[temp], 0, 10) \\
\hspace*{0.27 in} temp <- which(Y[,-c(Data$J)] == 0) \\
\hspace*{0.27 in} W[temp] <- interval(W[temp], -10, 0) \\
\hspace*{0.27 in} parm[Data$pos.W] <- as.vector(W) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 10, log=TRUE)) \\
\hspace*{0.27 in} U.prior <- sum(dnorm(u[-length(u)], 0, 1, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, beta) \\
\hspace*{0.27 in} \#eta <- exp(cbind(mu,0)) \\
\hspace*{0.27 in} \#p <- eta / rowSums(eta) \\
\hspace*{0.27 in} LL <- sum(dmvn(W, mu, Sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + U.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=max.col(cbind(rmvn(nrow(mu), mu, Sigma),0)), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- GIV(Model, MyData, PGF=TRUE)}

\section{Multiple Discrete-Continuous Choice} \label{mdcc}
This form of a multivariate discrete-continuous choice model was introduced in \citet{kim02} and referred to as a variety model. The original version is presented with log-normally distributed errors, but a gamma regression form is used here instead, which has always mixed better in testing. Note that the $\gamma$ parameters are fixed here, as recommended for identifiability in future articles by these authors.
\subsection{Form}
$$\textbf{Y} \sim \mathcal{G}(\lambda\tau, \tau)$$
$$\lambda_{i,j} = \exp(\textbf{Z}_{i,j}\log(\psi1_{m[i],j}) + \textbf{X1}_{i,1:K}\log(\beta) + \textbf{X2}_{i,1:L}\log(\delta))(\textbf{Y}_{i,j} + \gamma_j)^\alpha_j), \quad i=1,\dots,N, j=1,\dots,J$$
$$\alpha_j \sim \mathcal{U}(0,1), \quad j=1,\dots,J$$
$$\log(\beta_k) \sim \mathcal{N}(0,1000), \quad k=1,\dots,K$$
$$\gamma_j = 1, \quad j=1,\dots,J$$
$$\log(\delta_{j,l}) \sim \mathcal{N}(0,1000), \quad j=1,\dots,(J-1), \quad l=1,\dots,L$$
$$\log(\psi0_j) \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\log(\psi1_{g,j}) \sim \mathcal{N}_{J}(\log(\psi0), \Omega^{-1}), \quad g=1,\dots,G, \quad j1=,\dots,J$$
$$\Omega \sim \mathcal{W}_{J+1}(\textbf{S}), \quad \textbf{S} = \textbf{I}_J$$
$$\tau_j \sim \mathcal{HC}(25), \quad j=1,\dots,J$$
\subsection{Data}
\code{G <- 6 \#Number of Multilevel Groups (decision-makers, households, etc.) \\
J <- 3 \#Number of products \\
K <- 4 \#Number of product attributes \\
L <- 5 \#Number of decision-maker attributes \\
N <- 30 \#Number of records \\
X1 <- matrix(rnorm(N*K), N, K) \#Product attributes \\
X2 <- matrix(rnorm(N*L), N, L) \#Decision-maker attributes \\
Sigma <- matrix(runif((J-1)*(J-1),-1,1),J-1,J-1) \\
diag(Sigma) <- runif(J-1,1,5) \\
Sigma <- as.positive.definite(Sigma) / 100 \\
alpha <- runif(J) \\
log.beta <- rnorm(K,0,0.1) \\
log.delta <- matrix(rnorm((J-1)*L,0,0.1), J-1, L) \\
log.psi0 <- rnorm(J) \\
log.psi1 <- rmvn(G, log.psi0, Sigma) \\
m <- rcat(N, rep(1/G,G)) \# Multilevel group indicator \\
Z <- as.indicator.matrix(m) \\
Y <- matrix(0, N, J) \\
Y <- round(exp(tcrossprod(Z, t(cbind(log.psi1,0))) + \\
\hspace*{0.27 in} matrix(tcrossprod(X1, t(log.beta)), N, J) + \\
\hspace*{0.27 in} tcrossprod(X2, rbind(log.delta, colSums(log.delta)*-1))) * \\
\hspace*{0.27 in} (Y + 1)\textasciicircum matrix(alpha,N,J,byrow=TRUE) + \\
\hspace*{0.27 in} matrix(rnorm(N*J,0,0.1),N,J)) \\
S <- diag(J) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(alpha=rep(0,J), log.beta=rep(0,K), \\
\hspace*{0.27 in} log.delta=matrix(0,J-1,L), log.psi0=rep(0,J), \\
\hspace*{0.27 in} log.psi1=matrix(0,G,J), tau=rep(0,J), U=S), \\
\hspace*{0.27 in} uppertri=c(0,0,0,0,0,0,1)) \\
pos.alpha <- grep("alpha", parm.names) \\
pos.log.beta <- grep("log.beta", parm.names) \\
pos.log.delta <- grep("delta", parm.names) \\
pos.log.psi0 <- grep("log.psi0", parm.names) \\
pos.log.psi1 <- grep("log.psi1", parm.names) \\
pos.tau <- grep("tau", parm.names) \\
PGF <- function(Data) \{ \\
\hspace*{0.27 in} log.psi0 <- rnorm(Data$J,0,0.1) \\
\hspace*{0.27 in} U <- rwishartc(Data$J+1, Data$S) \\
\hspace*{0.27 in} return(c(runif(Data$J,0.9,1), rnorm(Data$K,0,0.1), \\
\hspace*{0.27 in} rnorm((Data$J-1)*Data$L,0,0.1), log.psi0, rmvnpc(Data$G, log.psi0, U), \\
\hspace*{0.27 in} runif(Data$J), upper.triangle(U, diag=TRUE)))\} \\
MyData <- list(G=G, J=J, K=K, L=L, N=N, PGF=PGF, S=S, X1=X1, X2=X2, Y=Y, \\
\hspace*{0.27 in} Z=Z, m=m, mon.names=mon.names, parm.names=parm.names, \\
\hspace*{0.27 in} pos.alpha=pos.alpha, pos.log.beta=pos.log.beta, \\
\hspace*{0.27 in} pos.log.delta=pos.log.delta, pos.log.psi0=pos.log.psi0, \\
\hspace*{0.27 in} pos.log.psi1=pos.log.psi1, pos.tau=pos.tau) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} parm[Data$pos.alpha] <- alpha <- interval(parm[Data$pos.alpha], 0, 1) \\
\hspace*{0.27 in} log.beta <- parm[Data$pos.log.beta] \\
\hspace*{0.27 in} log.delta <- matrix(parm[Data$pos.log.delta], Data$J-1, Data$L) \\
\hspace*{0.27 in} log.psi0 <- parm[Data$pos.log.psi0] \\
\hspace*{0.27 in} log.psi1 <- matrix(parm[Data$pos.log.psi1], Data$G, Data$J) \\
\hspace*{0.27 in} parm[Data$pos.tau] <- tau <- interval(parm[Data$pos.tau], 1e-100, Inf) \\
\hspace*{0.27 in} U <- as.parm.matrix(U, Data$J, parm, Data, chol=TRUE) \\
\hspace*{0.27 in} diag(U) <- exp(diag(U)) \\
\hspace*{0.27 in} lambda <- tcrossprod(Data$Z, t(log.psi1)) + \\
\hspace*{0.62 in} matrix(tcrossprod(Data$X1, t(log.beta)), Data$N, Data$J) + \\
\hspace*{0.62 in} tcrossprod(Data$X2, rbind(log.delta, colSums(log.delta)*-1)) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} U.prior <- dwishartc(U, Data$J+1, Data$S, log=TRUE) \\
\hspace*{0.27 in} alpha.prior <- sum(dunif(alpha, 0, 1, log=TRUE)) \\
\hspace*{0.27 in} log.beta.prior <- sum(dnormv(log.beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} log.delta.prior <- sum(dnormv(log.delta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} log.psi0.prior <- sum(dnormv(log.psi0, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} log.psi1.prior <- sum(dmvnpc(lambda, \\
\hspace*{0.62 in} matrix(log.psi0, Data$N, Data$J, byrow=TRUE), U, log=TRUE)) \\
\hspace*{0.27 in} tau.prior <- sum(dhalfcauchy(tau, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} alpha <- matrix(alpha, Data$N, Data$J, byrow=TRUE) \\
\hspace*{0.27 in} lambda <- exp(lambda)*(Data$Y + 1)\textasciicircum alpha \\
\hspace*{0.27 in} tau <- matrix(tau, Data$N, Data$J, byrow=TRUE) \\
\hspace*{0.27 in} LL <- sum(dgamma(Data$Y+1, lambda*tau, tau, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + U.prior + alpha.prior + log.beta.prior + log.delta.prior + \\
\hspace*{0.62 in} log.psi0.prior + log.psi1.prior + tau.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rgamma(prod(dim(lambda)), lambda*tau, tau)-1, \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(runif(J,0.9,1), rnorm(K,0,0.1), \\
\hspace*{0.27 in} rnorm((J-1)*L,0,0.1), rnorm(J,0,0.1), \\
\hspace*{0.27 in} rmvnpc(G, rnorm(J,0,0.1), rwishartc(J+1,S)), runif(J), \\
\hspace*{0.27 in} upper.triangle(rwishartc(J+1,S), diag=TRUE))}

\section{Multivariate Binary Probit} \label{multiv.bin.probit}
\subsection{Form}
$$\textbf{Z}_{i,1:J} \sim \mathcal{N}_J(\mu_{i,1:J}, \Sigma), \quad i=1,\dots,N$$
\[\textbf{Z}_{i,j} \in \left\{
\begin{array}{l l}
 $[0,10]$ & \quad \mbox{if $\textbf{y}_i = j$}\\
 $[-10,0]$ \\ \end{array} \right. \]
$$\mu_{1:N,j} = \textbf{X} \beta_{j,1:K}$$
$$\Sigma \sim \mathcal{IW}_{J+1}(\textbf{S}^-1), \quad \textbf{S} = \textbf{I}_J, \quad \Sigma[1,1] = 1$$
$$\beta_{j,k} \sim \mathcal{N}(0, 1000), \quad j=1,\dots,(J-1), \quad k=1,\dots,K$$
$$\beta_{J,k} = - \sum^{J-1}_{j=1} \beta_{j,k}$$
$$\textbf{Z}_{i,j} \sim \mathcal{N}(0, 1000) \in [-10,10]$$
\subsection{Data}
\code{N <- 30 \\
J <- 2 \#Number of binary dependent variables \\
K <- 3 \#Number of columns to be in design matrix X \\
X <- matrix(1, N, K) \\
for (k in 2:K) \{X[,k] <- rnorm(N, runif(1,-3,3), runif(1,0.1,3))\} \\
beta <- matrix(runif(J*K), J, K) \\
mu <- tcrossprod(X, beta) \\
S <- diag(J) \\
u <- c(0, rnorm((J-1) + (factorial(J) / \\
\hspace*{0.27 in} (factorial(J-2)*factorial(2))),0,1)) \\
U <- diag(J) \\
U[upper.tri(U, diag=TRUE)] <- u \\
diag(U) <- exp(diag(U)) \\
Sigma <- t(U) \%*\% U \\
Sigma[1,] <- Sigma[,1] <- U[1,] \\
W <- rmvn(N, mu, Sigma) + matrix(rnorm(N*J,0,0.1), N, J) \\
Y <- 1 * (W >= 0) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=matrix(0,J,K), \\
\hspace*{0.27 in} U=U, W=matrix(0,N,J)), uppertri=c(0,1,0)) \\
parm.names <- parm.names[-which(parm.names == "U[1,1]")] \\
pos.beta <- grep("beta", parm.names)\\
pos.U <- grep("U", parm.names) \\
pos.W <- grep("W", parm.names) \\
PGF <- function(Data) \{ \\
\hspace*{0.27 in} beta <- rnormv(Data$J*Data$K,0,1) \\
\hspace*{0.27 in} U <- rnorm((Data$J-1) + (factorial(Data$J) / \\
\hspace*{0.62 in} (factorial(Data$J-2)*factorial(2))),0,1) \\
\hspace*{0.27 in} W <- matrix(runif(Data$N*Data$J,-10,0), Data$N, Data$J) \\
\hspace*{0.27 in} W <- ifelse(Y == 1, abs(W), W) \\
\hspace*{0.27 in} return(c(beta, U, as.vector(W)))\} \\
MyData <- list(J=J, K=K, N=N, PGF=PGF, S=S, X=X, Y=Y, \\
\hspace*{0.27 in} mon.names=mon.names, parm.names=parm.names, pos.beta=pos.beta, \\
\hspace*{0.27 in} pos.U=pos.U, pos.W=pos.W) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- matrix(parm[Data$pos.beta], Data$J, Data$K) \\
\hspace*{0.27 in} u <- c(0, parm[Data$pos.U]) \\
\hspace*{0.27 in} U <- diag(Data$J) \\
\hspace*{0.27 in} U[upper.tri(U, diag=TRUE)] <- u \\
\hspace*{0.27 in} diag(U) <- exp(diag(U)) \\
\hspace*{0.27 in} Sigma <- t(U) \%*\% U \\
\hspace*{0.27 in} Sigma[1,] <- Sigma[,1] <- U[1,] \\
\hspace*{0.27 in} W <- matrix(parm[Data$pos.W], Data$N, Data$J) \\
\hspace*{0.27 in} W[Data$Y == 0] <- interval(W[Data$Y == 0], -10, 0) \\
\hspace*{0.27 in} W[Data$Y == 1] <- interval(W[Data$Y == 1], 0, 10) \\
\hspace*{0.27 in} parm[Data$pos.W] <- as.vector(W) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} U.prior <- sum(dnorm(u[-length(u)], 0, 1, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, beta) \\
\hspace*{0.27 in} LL <- sum(dmvn(W, mu, Sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + U.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=1*(rmvn(prod(nrow(mu)), mu, Sigma) >= 0), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- GIV(Model, MyData, PGF=TRUE)}

\section{Multivariate Laplace Regression} \label{multivariate.lap.reg}
\subsection{Form}
$$\textbf{Y}_{i,k} \sim \mathcal{L}_K(\mu_{i,k}, \Sigma), \quad i=1,\dots,N; \quad k=1,\dots,K$$
$$\mu_{i,k} = \textbf{X}_{1:N,k} \beta_{k,1:J}$$
$$\Sigma = \Omega^{-1}$$
$$\Omega \sim \mathcal{W}_{K+1}(\textbf{S}), \quad \textbf{S} = \textbf{I}_K$$
$$\beta_{k,j} \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
\subsection{Data}
\code{N <- 100 \\
J <- 6 \#Number of columns in design matrix \\
K <- 3 \#Number of DVs \\
X <- matrix(runif(N*J),N,J); X[,1] <- 1 \\
Y <- mu <- tcrossprod(X, matrix(rnorm(J*K),K,J)) \\
Sigma <- matrix(runif(K*K),K,K); diag(Sigma) <- runif(K,1,K) \\
Sigma <- as.symmetric.matrix(Sigma) \\
for (i in 1:N) \{Y[i,] <- colMeans(rmvn(1000, mu[i,], Sigma))\} \\
S <- diag(K) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=matrix(0,K,J), U=diag(K)), \\
\hspace*{0.27 in} uppertri=c(0,1)) \\
pos.beta <- grep("beta", parm.names) \\
PGF <- function(Data) return(c(rnormv(Data$K*Data$J,0,1), \\
\hspace*{0.27 in} upper.triangle(rwishartc(Data$K+1,Data$S), diag=TRUE))) \\
MyData <- list(J=J, K=K, N=N, PGF=PGF, S=S, X=X, Y=Y, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.beta=pos.beta) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- matrix(parm[Data$pos.beta], Data$K, Data$J) \\
\hspace*{0.27 in} U <- as.parm.matrix(U, Data$K, parm, Data, chol=TRUE) \\
\hspace*{0.27 in} diag(U) <- exp(diag(U)) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} U.prior <- dwishart(U, Data$K+1, Data$S, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, beta) \\
\hspace*{0.27 in} LL <- sum(dmvlc(Data$Y, mu, U, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + U.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rmvlc(nrow(mu), mu, U), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J*K), upper.triangle(S, diag=TRUE))}

\section{Multivariate Regression} \label{multivariate.reg}
\subsection{Form}
$$\textbf{Y}_{i,k} \sim \mathcal{N}_K(\mu_{i,k}, \Omega^{-1}), \quad i=1,\dots,N; \quad k=1,\dots,K$$
$$\mu_{i,k} = \textbf{X}_{1:N,k} \beta_{k,1:J}$$
$$\Omega \sim \mathcal{W}_{K+1}(\textbf{S}), \quad \textbf{S} = \textbf{I}_K$$
$$\beta_{k,j} \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
\subsection{Data}
\code{N <- 100 \\
J <- 6 \#Number of columns in design matrix \\
K <- 3 \#Number of DVs \\
X <- matrix(runif(N*J),N,J); X[,1] <- 1 \\
Y <- mu <- tcrossprod(X, matrix(rnorm(J*K),K,J)) \\
Sigma <- matrix(runif(K*K),K,K); diag(Sigma) <- runif(K,1,K) \\
Sigma <- as.symmetric.matrix(Sigma) \\
for (i in 1:N) \{Y[i,] <- colMeans(rmvn(1000, mu[i,], Sigma))\} \\
S <- diag(K) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=matrix(0,K,J), U=diag(K)), \\
\hspace*{0.27 in} uppertri=c(0,1)) \\
pos.beta <- grep("beta", parm.names) \\
PGF <- function(Data) return(c(rnormv(Data$K*Data$J,0,1), \\
\hspace*{0.27 in} upper.triangle(rwishartc(Data$K+1,Data$S), diag=TRUE))) \\
MyData <- list(J=J, K=K, N=N, PGF=PGF, S=S, X=X, Y=Y, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.beta=pos.beta) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- matrix(parm[Data$pos.beta], Data$K, Data$J) \\
\hspace*{0.27 in} U <- as.parm.matrix(U, Data$K, parm, Data, chol=TRUE) \\
\hspace*{0.27 in} diag(U) <- exp(diag(U)) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} U.prior <- dwishartc(U, Data$K+1, Data$S, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, beta) \\
\hspace*{0.27 in} LL <- sum(dmvnpc(Data$Y, mu, U, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + U.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rmvnpc(nrow(mu), mu, U), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J*K), upper.triangle(S, diag=TRUE))}

\section{Negative Binomial Regression} \label{negbin.reg}
This example was contributed by Jim Robison-Cox.
\subsection{Form}
$$\textbf{y} \sim \mathcal{NB}(\mu, \kappa)$$
$$p = \frac{\kappa}{\kappa + \mu}$$
$$\mu = \exp(\textbf{X} \beta)$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\kappa \sim \mathcal{HC}(25) \in (0,\infty]$$
\subsection{Data}
\code{N <- 100 \\
J <- 5 \#Number of predictors, including the intercept \\
kappa.orig <- 2 \\
beta.orig <- runif(J,-2,2) \\
X <- matrix(runif(J*N,-2, 2), N, J); X[,1] <- 1 \\
mu <- exp(tcrossprod(X, t(beta.orig)) + rnorm(N)) \\
p <- kappa.orig / (kappa.orig + mu) \\
y <- rnbinom(N, size=kappa.orig, mu=mu) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=rep(0,J), kappa=0)) \\
pos.beta <- grep("beta", parm.names) \\
pos.kappa <- grep("kappa", parm.names) \\
PGF <- function(Data) return(c(rnormv(Data$J,0,1000), rhalfcauchy(1,5))) \\
MyData <- list(J=J, PGF=PGF, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.beta=pos.beta, pos.kappa=pos.kappa, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} parm[Data$J + 1] <- kappa <- interval(parm[Data$pos.kappa], \\
\hspace*{0.62 in} .Machine$double.xmin, Inf) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} kappa.prior <- dhalfcauchy(kappa, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- as.vector(exp(tcrossprod(Data$X, t(beta)))) \\
\hspace*{0.27 in} \#p <- kappa / (kappa + mu) \\
\hspace*{0.27 in} LL <- sum(dnbinom(Data$y, size=kappa, mu=mu, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + kappa.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rnbinom(length(mu), size=kappa, mu=mu), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), 1)}

\section{Normal, Multilevel} \label{norm.ml}
This is Gelman's school example \citep{gelman04}. Note that \pkg{LaplacesDemon} is slower to converge than \proglang{WinBUGS} through the \pkg{R2WinBUGS} package \citep{r:r2winbugs}, an \proglang{R} package on CRAN. This example is very sensitive to the prior distributions. The recommended, default, half-Cauchy priors with scale 25 on scale parameters overwhelms the likelihood, so uniform priors are used.
\subsection{Form}
$$\textbf{y}_j \sim \mathcal{N}(\theta_j, \sigma^2_j), \quad j=1,\dots,J$$
$$\theta_j \sim \mathcal{N}(\theta_{\mu}, \theta_\sigma^2)$$
$$\theta_{\mu} \sim \mathcal{N}(0, 1000000)$$
$$\theta_{\sigma[j]} \sim \mathcal{N}(0, 1000)$$
$$\sigma \sim \mathcal{U}(0, 1000)$$
\subsection{Data}
\code{J <- 8 \\
y <- c(28.4, 7.9, -2.8, 6.8, -0.6, 0.6, 18.0, 12.2) \\
sd <- c(14.9, 10.2, 16.3, 11.0, 9.4, 11.4, 10.4, 17.6) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(theta=rep(0,J), theta.mu=0, \\
\hspace*{0.27 in} theta.sigma=0)) \\
pos.theta <- 1:J \\
pos.theta.mu <- J+1 \\
pos.theta.sigma <- J+2 \\
PGF <- function(Data) return(c(rnorm(Data$J, rnormp(1,0,1E-6), \\
\hspace*{0.27 in} runif(1,0,10)), rnormp(1,0,1E-6), runif(1,0,10))) \\
MyData <- list(J=J, PGF=PGF, mon.names=mon.names, parm.names=parm.names, \\
\hspace*{0.27 in} pos.theta=pos.theta, pos.theta.mu=pos.theta.mu, \\
\hspace*{0.27 in} pos.theta.sigma=pos.theta.sigma, sd=sd, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} theta.mu <- parm[Data$pos.theta.mu] \\
\hspace*{0.27 in} theta.sigma <- interval(parm[Data$pos.theta.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.theta.sigma] <- theta.sigma \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} theta <- parm[Data$pos.theta] \\
\hspace*{0.27 in} \#\#\# Log(Hyperprior Densities) \\
\hspace*{0.27 in} theta.mu.prior <- dnormp(theta.mu, 0, 1.0E-6, log=TRUE) \\
\hspace*{0.27 in} theta.sigma.prior <- dunif(theta.sigma, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} theta.prior <- sum(dnorm(theta, theta.mu, theta.sigma, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dunif(Data$sd, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, theta, Data$sd, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + theta.prior + theta.mu.prior + theta.sigma.prior + \\
\hspace*{0.62 in} sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rnorm(length(theta), theta, Data$sd), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(mean(y),J), mean(y), 1)}

\section{Ordinal Logit} \label{ordinal.logit}
\subsection{Form}
$$\textbf{y}_i \sim \mathcal{CAT}(P_{i,1:J})$$
$$P_{,J} = 1 - Q_{,(J-1)}$$
$$P_{,j} = |Q_{,j} - Q_{,(j-1)}|, \quad j=2,\dots,(J-1)$$
$$P_{,1} = Q_{,1}$$
$$Q = \frac{1}{1 + \exp(\mu)}$$
$$\mu_{,j} = \delta_j - \textbf{X} \beta, \quad \in [-5,5]$$
$$\beta_k \sim \mathcal{N}(0, 1000), \quad k=1,\dots,K$$
$$\delta_j \sim \mathcal{N}(0, 1) \in [(j-1),j] \in [-5,5], \quad j=1,\dots,(J-1)$$
\subsection{Data}
\code{data(demonsnacks) \\
N <- nrow(demonsnacks) \\
J <- 3 \#Number of categories in y \\
X <- as.matrix(demonsnacks[,c(1,3:10)]) \\
K <- ncol(demonsnacks) \#Number of columns in design matrix X \\
y <- log(demonsnacks$Calories) \\
y <- ifelse(y < 4.5669, 1, ifelse(y > 5.5268, 3, 2)) \#Discretize \\
for (k in 1:K) \{X[,k] <- CenterScale(X[,k])\} \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=rep(0,K), delta=rep(0,J-1))) \\
pos.beta <- grep("beta", parm.names) \\
pos.delta <- grep("delta", parm.names) \\
PGF <- function(Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} delta <- rnorm(Data$J-1,0,1) \\
\hspace*{0.27 in} delta <- delta[order(delta)] \\
\hspace*{0.27 in} return(c(rnormv(Data$K,0,10), delta)) \\
\hspace*{0.27 in} \} \\
MyData <- list(J=J, K=K, N=N, PGF=PGF, X=X, mon.names=mon.names, \\
     parm.names=parm.names, pos.beta=pos.beta, pos.delta=pos.delta, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} delta <- interval(parm[Data$pos.delta], -5, 5) \\
\hspace*{0.27 in} delta <- delta[order(delta)] \\
\hspace*{0.27 in} parm[Data$pos.delta] <- delta \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} delta.prior <- sum(dtrunc(delta, "norm", a=-5, b=5, log=TRUE, \\
\hspace*{0.62 in} mean=0, sd=1) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- matrix(delta, Data$N, Data$J-1, byrow=TRUE) - \\
\hspace*{0.62 in} matrix(tcrossprod(Data$X, t(beta)), Data$N, Data$J-1) \\
\hspace*{0.27 in} P <- Q <- invlogit(mu) \\
\hspace*{0.27 in} P[,-1] <- abs(Q[,-1] - Q[,-(Data$J-1)]) \\
\hspace*{0.27 in} P <- cbind(P, 1 - Q[,(Data$J-1)]) \\
\hspace*{0.27 in} LL <- sum(dcat(Data$y, P, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + delta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=rcat(nrow(P), P) \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,K), seq(from=-1, to=1, len=(J-1)))}

\section{Ordinal Probit} \label{ordinal.probit}
\subsection{Form}
$$\textbf{y}_i \sim \mathcal{CAT}(P_{i,1:J})$$
$$P_{,J} = 1 - Q_{,(J-1)}$$
$$P_{,j} = |Q_{,j} - Q_{,(j-1)}|, \quad j=2,\dots,(J-1)$$
$$P_{,1} = Q_{,1}$$
$$Q = \phi(\mu)$$
$$\mu_{,j} = \delta_j - \textbf{X} \beta, \quad \in [-5,5]$$
$$\beta_k \sim \mathcal{N}(0, 1000), \quad k=1,\dots,K$$
$$\delta_j \sim \mathcal{N}(0, 1) \in [(j-1),j] \in [-5,5], \quad j=1,\dots,(J-1)$$
\subsection{Data}
\code{data(demonsnacks) \\
N <- nrow(demonsnacks) \\
J <- 3 \#Number of categories in y \\
X <- as.matrix(demonsnacks[,c(1,3:10)]) \\
K <- ncol(demonsnacks) \#Number of columns in design matrix X \\
y <- log(demonsnacks$Calories) \\
y <- ifelse(y < 4.5669, 1, ifelse(y > 5.5268, 3, 2)) \#Discretize \\
for (k in 1:K) \{X[,k] <- CenterScale(X[,k])\} \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=rep(0,K), delta=rep(0,J-1))) \\
pos.beta <- grep("beta", parm.names) \\
pos.delta <- grep("delta", parm.names) \\
PGF <- function(Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} delta <- rnorm(Data$J-1,0,1) \\
\hspace*{0.27 in} delta <- delta[order(delta)] \\
\hspace*{0.27 in} return(c(rnormv(Data$K,0,10), delta)) \\
\hspace*{0.27 in} \} \\
MyData <- list(J=J, K=K, N=N, PGF=PGF, X=X, mon.names=mon.names, \\
     parm.names=parm.names, pos.beta=pos.beta, pos.delta=pos.delta, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} delta <- interval(parm[Data$pos.delta], -5, 5) \\
\hspace*{0.27 in} delta <- delta[order(delta)] \\
\hspace*{0.27 in} parm[Data$pos.delta] <- delta \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} delta.prior <- sum(dtrunc(delta, "norm", a=-5, b=5, log=TRUE, \\
\hspace*{0.62 in} mean=0, sd=1) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- matrix(delta, Data$N, Data$J-1, byrow=TRUE) - \\
\hspace*{0.62 in} matrix(tcrossprod(Data$X, t(beta)), Data$N, Data$J-1) \\
\hspace*{0.27 in} P <- Q <- pnorm(mu) \\
\hspace*{0.27 in} P[,-1] <- abs(Q[,-1] - Q[,-(Data$J-1)]) \\
\hspace*{0.27 in} P <- cbind(P, 1 - Q[,(Data$J-1)]) \\
\hspace*{0.27 in} LL <- sum(dcat(Data$y, P, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + delta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=rcat(nrow(P), P) \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,K), seq(from=-1, to=1, len=(J-1)))}

\section{Panel, Autoregressive Poisson} \label{panel.ap}
\subsection{Form}
$$\textbf{Y} \sim \mathcal{P}(\Lambda)$$
$$\Lambda_{1:N,1} = \exp(\alpha + \beta \textbf{x})$$
$$\Lambda_{1:N,t} = \exp(\alpha + \beta \textbf{x} + \rho \log(\textbf{Y}_{1:N,t-1})), \quad t=2,\dots,T$$
$$\alpha_i \sim \mathcal{N}(\alpha_\mu, \alpha^2_\sigma), \quad i=1,\dots,N$$
$$\alpha_\mu \sim \mathcal{N}(0, 1000)$$
$$\alpha_\sigma \sim \mathcal{HC}(25)$$
$$\beta \sim \mathcal{N}(0, 1000)$$
$$\rho \sim \mathcal{N}(0, 1000)$$
\subsection{Data}
\code{N <- 10 \\
T <- 10 \\
alpha <- rnorm(N,2,0.5) \\
rho <- 0.5 \\
beta <- 0.5 \\
x <- runif(N,0,1) \\
Y <- matrix(NA,N,T) \\
Y[,1] <- exp(alpha + beta*x) \\
for (t in 2:T) \{Y[,t] <- exp(alpha + beta*x + rho*log(Y[,t-1]))\} \\
Y <- round(Y) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(alpha=rep(0,N), alpha.mu=0, \\
\hspace*{0.27 in} alpha.sigma=0, beta=0, rho=0)) \\
pos.alpha <- 1:N \\
pos.alpha.mu <- grep("alpha.mu", parm.names) \\
pos.alpha.sigma <- grep("alpha.sigma", parm.names) \\
pos.beta <- grep("beta", parm.names) \\
pos.rho <- grep("rho", parm.names) \\
PGF <- function(Data) return(c(rnorm(Data$N, rnormv(1,0,10), \\
\hspace*{0.27 in} rhalfcauchy(1,5)), rnormv(1,0,10), rhalfcauchy(1,5), \\
\hspace*{0.27 in} rnormv(2,0,10))) \\
MyData <- list(N=N, PGF=PGF, T=T, Y=Y, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.alpha=pos.alpha, pos.alpha.mu=pos.alpha.mu, \\
\hspace*{0.27 in} pos.alpha.sigma=pos.alpha.sigma, pos.beta=pos.beta, pos.rho=pos.rho, \\
\hspace*{0.27 in} x=x) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} alpha.mu <- parm[Data$pos.alpha.mu] \\
\hspace*{0.27 in} alpha.sigma <- interval(parm[Data$pos.alpha.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.alpha.sigma] <- alpha.sigma \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[Data$pos.alpha] \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} rho <- parm[Data$pos.rho] \\
\hspace*{0.27 in} \#\#\# Log(Hyperprior Densities) \\
\hspace*{0.27 in} alpha.mu.prior <- dnormv(alpha.mu, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} alpha.sigma.prior <- dhalfcauchy(alpha.sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- sum(dnorm(alpha, alpha.mu, alpha.sigma, log=TRUE)) \\
\hspace*{0.27 in} beta.prior <- dnormv(beta, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} rho.prior <- dnormv(rho, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} Lambda <- Data$Y \\
\hspace*{0.27 in} Lambda[,1] <- exp(alpha + beta*x) \\
\hspace*{0.27 in} Lambda[,2:Data$T] <- exp(alpha + beta*Data$x + \\
\hspace*{0.62 in} rho*log(Data$Y[,1:(Data$T-1)])) \\
\hspace*{0.27 in} LL <- sum(dpois(Data$Y, Lambda, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + alpha.mu.prior + alpha.sigma.prior + \\
\hspace*{0.62 in} beta.prior + rho.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rpois(prod(dim(Lambda)), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,N), 0, 1, 0, 0)}

\section{Penalized Spline Regression} \label{pspline}
This example applies penalized splines to one predictor in a linear regression. The user selects the degree of the polynomial, $D$, and the number of knots, $K$.
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2_1)$$
$$\mu = \textbf{X} \beta + \textbf{S}$$
$$\textbf{S} = \textbf{Z} \gamma$$
\[\textbf{Z}_{i,k} = \left\{
\begin{array}{l l}
  (\textbf{x}_i - k)^D & \quad \mbox{if $\textbf{Z}_{i,k} > 0$}\\ 
  0 \\ \end{array} \right. \]
$$\beta_d \sim \mathcal{N}(0, 1000), \quad d=1,\dots,(D+1)$$
$$\gamma_k \sim \mathcal{N}(0, \sigma^2_2), \quad k=1,\dots,K$$
$$\sigma_j \sim \mathcal{HC}(25), \quad j=1,\dots,2$$
\subsection{Data}
\code{N <- 100 \\
x <- 1:N \\
y <- sin(2*pi*x/N) + runif(N,-1,1) \\
K <- 10 \#Number of knots \\
D <- 2 \#Degree of polynomial \\
x <- CenterScale(x) \\
k <- as.vector(quantile(x, probs=(1:K / (K+1)))) \\
X <- cbind(1, matrix(x, N, D)) \\
for (d in 1:D) \{X[,d+1] <- X[,d+1]\textasciicircum d\} \\
Z <- matrix(x, N, K) - matrix(k, N, K, byrow=TRUE) \\
Z <- ifelse(Z > 0, Z, 0); Z <- Z\textasciicircum D \\
mon.names <- c("LP", paste("S[", 1:nrow(X) ,"]", sep="")) \\
parm.names <- as.parm.names(list(beta=rep(0,1+D), gamma=rep(0,K), \\
\hspace*{0.27 in} log.sigma=rep(0,2))) \\
pos.beta <- grep("beta", parm.names) \\
pos.gamma <- grep("gamma", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
PGF <- function(Data) return(c(rnormv(1+Data$D,0,10), rnorm(Data$K,0,10), \\
\hspace*{0.27 in} rhalfcauchy(2,5))) \\
MyData <- list(D=D, K=K, N=N, PGF=PGF, Z=Z, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.beta=pos.beta, pos.gamma=pos.gamma, \\
\hspace*{0.27 in} pos.sigma=pos.sigma, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} gamma <- parm[Data$pos.gamma] \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} gamma.prior <- sum(dnorm(gamma, 0, sigma[2], log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} S <- as.vector(tcrossprod(Data$Z, t(gamma))) \\
\hspace*{0.27 in} mu <- as.vector(tcrossprod(Data$X, t(beta))) + S \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma[1], log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + gamma.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,S), \\
\hspace*{0.62 in} yhat=rnorm(length(mu), mu, sigma[1]), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
     \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,1+D), rep(0,K), c(1,1))}

\section{Poisson Regression} \label{poisson.reg}
\subsection{Form}
$$\textbf{y} \sim \mathcal{P}(\lambda)$$
$$\lambda = \exp(\textbf{X}\beta)$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
\subsection{Data}
\code{N <- 10000 \\
J <- 5 \\
X <- matrix(runif(N*J,-2,2),N,J); X[,1] <- 1 \\
beta <- runif(J,-2,2) \\
y <- round(exp(tcrossprod(X, t(beta)))) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=rep(0,J))) \\
PGF <- function(Data) return(rnormv(Data$J,0,1000)) \\
MyData <- list(J=J, PGF=PGF, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} lambda <- exp(tcrossprod(Data$X, t(beta))) \\
\hspace*{0.27 in} LL <- sum(dpois(Data$y, lambda, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rpois(length(lambda), lambda), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- rep(0,J)}

\section{Polynomial Regression} \label{polynomial.reg}
In this univariate example, the degree of the polynomial is specified as $D$. For a more robust extension to estimating nonlinear relationships between $\textbf{y}$ and $\textbf{x}$, see penalized spline regression in section \ref{penalized.spline}.
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2)$$
$$\mu = \textbf{X} \beta$$
$$\textbf{X}_{i,d} = \textbf{x}^{d-1}_i, \quad d=1,\dots,(D+1)$$
$$\textbf{X}_{i,1} = 1$$
$$\beta_d \sim \mathcal{N}(0, 1000), \quad d=1,\dots,(D+1)$$
$$\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{data(demonsnacks) \\
N <- nrow(demonsnacks) \\
D <- 2 \#Degree of polynomial \\
y <- log(demonsnacks$Calories) \\
x <- log(demonsnacks[,10]+1) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=rep(0,D+1), sigma=0)) \\
pos.beta <- grep("beta", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
PGF <- function(Data) return(c(rnormv(Data$D+1,0,1000), rhalfcauchy(1,5))) \\
MyData <- list(D=D, N=N, PGF=PGF, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.beta=pos.beta, pos.sigma=pos.sigma, x=x, \\
\hspace*{0.27 in} y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} X <- matrix(Data$x, Data$N, Data$D) \\
\hspace*{0.27 in} for (d in 2:Data$D) \{X[,d] <- X[,d]\textasciicircum d\} \\
\hspace*{0.27 in} X <- cbind(1,X) \\
\hspace*{0.27 in} mu <- tcrossprod(X, t(beta)) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rnorm(length(mu), mu, sigma), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,D+1), 1)}

\section{Proportional Hazards Regression, Weibull} \label{prop.haz.weib}
Although the dependent variable is usually denoted as $\textbf{t}$ in survival analysis, it is denoted here as $\textbf{y}$ so Laplace's Demon recognizes it as a dependent variable for posterior predictive checks. This example does not support censoring, but it will be included soon.
\subsection{Form}
$$\textbf{y}_i \sim \mathcal{WEIB}(\gamma, \mu_i), \quad i=1,\dots,N$$
$$\mu = \exp(\textbf{X} \beta)$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\gamma \sim \mathcal{G}(1, 0.001)$$
\subsection{Data}
\code{N <- 50 \\
J <- 5 \\
X <- matrix(runif(N*J,-2,2),N,J); X[,1] <- 1 \\
beta <- c(1,runif(J-1,-1,1)) \\
y <- round(exp(tcrossprod(X, t(beta)))) + 1 \# Undefined at zero \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=rep(0,J), gamma=0)) \\
pos.beta <- grep("beta", parm.names) \\
pos.gamma <- grep("gamma", parm.names) \\
PGF <- function(Data) return(c(rnormv(Data$J,0,10), rgamma(1,1E-3))) \\
MyData <- list(J=J, N=N, PGF=PGF, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.beta=pos.beta, pos.gamma=pos.gamma, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} gamma <- interval(parm[Data$pos.gamma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.gamma] <- gamma \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} gamma.prior <- dgamma(gamma, 1, 1.0E-3, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- exp(tcrossprod(Data$X, t(beta))) \\
\hspace*{0.27 in} LL <- sum(dweibull(Data$y, gamma, mu, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + gamma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rweibull(length(mu), gamma, mu), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), 1)}

\section{Quantile Regression} \label{quantile.reg}
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\phi, \sigma^2)$$
$$\phi = \frac{(1 - 2P)}{P(1 - P)} \zeta + \mu$$
$$\mu = \textbf{X} \beta$$
$$\sigma = \frac{P (1 - P) \tau}{2 \zeta}$$
$$\beta \sim \mathcal{N}(0, 1000)$$
$$\tau \sim \mathcal{HC}(25)$$
$$\zeta \sim \mathcal{EXP}(\tau)$$
where $P$ is the user-specified quantile in $(0,1)$.
\subsection{Data}
\code{data(demonsnacks) \\
y <- log(demonsnacks$Calories) \\
X <- cbind(1, as.matrix(log(demonsnacks[,c(1,4,10)]+1))) \\
N <- nrow(X) \\
J <- ncol(X) \\
P <- 0.5 \#Quantile in (0,1) \\
for (j in 2:J) \{X[,j] <- CenterScale(X[,j])\} \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=rep(0,J), tau=0, zeta=rep(0,N))) \\
pos.beta <- grep("beta", parm.names) \\
pos.tau <- grep("tau", parm.names) \\
pos.zeta <- grep("zeta", parm.names) \\
PGF <- function(Data) return(c(rnormv(Data$J,0,10), rhalfcauchy(1,5), \\
\hspace*{0.27 in} rexp(Data$N))) \\
MyData <- list(J=J, N=N, P=P, PGF=PGF, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.beta=pos.beta, pos.tau=pos.tau, \\
\hspace*{0.27 in} pos.zeta=pos.zeta, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} parm[Data$pos.tau] <- tau <- interval(parm[Data$pos.tau], 1e-100, Inf) \\
\hspace*{0.27 in} zeta <- interval(parm[Data$pos.zeta], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.zeta] <- zeta \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} tau.prior <- dhalfcauchy(tau, 25, log=TRUE) \\
\hspace*{0.27 in} zeta.prior <- sum(dexp(zeta, tau, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, t(beta)) \\
\hspace*{0.27 in} phi <- (1 - 2*Data$P) / (Data$P*(1 - Data$P))*zeta + mu \\
\hspace*{0.27 in} sigma <- (Data$P*(1 - Data$P)*tau) / (2*zeta) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, phi, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + tau.prior + zeta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rnorm(length(phi), phi, sigma), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), 1, rep(1,N))}

\section{Revision, Normal} \label{revision.normal}
This example provides both an analytic solution and numerical approximation of the revision of a normal distribution. Given a normal prior distribution ($\alpha$) and data distribution ($\beta$), the posterior ($\gamma$) is the revised normal distribution. This is an introductory example of Bayesian inference, and allows the user to experiment with numerical approximation, such as with MCMC in \code{LaplacesDemon}. Note that, regardless of the data sample size $N$ in this example, Laplace Approximation is inappropriate due to asymptotics since the data ($\beta$) is perceived by the algorithm as a single datum rather than a collection of data. MCMC, on the other hand, is biased only by the effective number of samples taken of the posterior. \\
\code{\#\#\# Analytic Solution \\
prior.mu <- 0 \\
prior.sigma <- 10 \\
N <- 10 \\
data.mu <- 1 \\
data.sigma <- 2 \\
posterior.mu <- (prior.sigma\textasciicircum -2 * prior.mu + N * data.sigma\textasciicircum -2 * data.mu) / \\
\hspace*{0.27 in} (prior.sigma\textasciicircum -2 + N * data.sigma\textasciicircum -2) \\
posterior.sigma <- sqrt(1/(prior.sigma\textasciicircum -2 + data.sigma\textasciicircum -2)) \\
posterior.mu \\
posterior.sigma \\
}
\subsection{Form}
$$\alpha \sim \mathcal{N}(0,10)$$
$$\beta \sim \mathcal{N}(1,2)$$
$$\gamma = \frac{\alpha^{-2}_\sigma \alpha + N \beta^{-2}_\sigma \beta}{\alpha^{-2}_\sigma + N \beta^{-2}_\sigma}$$
\subsection{Data}
\code{N <- 10 \\
mon.names <- c("LP","gamma") \\
parm.names <- c("alpha","beta") \\
PGF <- function(Data) return(c(rnorm(1,0,10), rnorm(1,1,2))) \\
MyData <- list(N=N, PGF=PGF, mon.names=mon.names, parm.names=parm.names) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} alpha.mu <- 0 \\
\hspace*{0.27 in} alpha.sigma <- 10 \\
\hspace*{0.27 in} beta.mu <- 1 \\
\hspace*{0.27 in} beta.sigma <- 2 \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1] \\
\hspace*{0.27 in} beta <- parm[2] \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnorm(alpha, alpha.mu, alpha.sigma, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood Density \\
\hspace*{0.27 in} LL <- dnorm(beta, beta.mu, beta.sigma, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Posterior \\
\hspace*{0.27 in} gamma <- (alpha.sigma\textasciicircum -2 * alpha + N * beta.sigma\textasciicircum -2 * beta) / \\
\hspace*{0.62 in} (alpha.sigma\textasciicircum -2 + N * beta.sigma\textasciicircum -2) \\
\hspace*{0.27 in} \#\#\# Log(Posterior Density) \\
\hspace*{0.27 in} LP <- LL + alpha.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,gamma), \\
\hspace*{0.62 in} yhat=rnorm(1, beta.mu, beta.sigma), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(0,0)}

\section{Ridge Regression} \label{ridge.reg}
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2_1)$$
$$\mu = \textbf{X}\beta$$
$$\beta_1 \sim \mathcal{N}(0, 1000)$$
$$\beta_j \sim \mathcal{N}(0, \sigma^2_2), \quad j=2,\dots,J$$
$$\sigma_k \sim \mathcal{HC}(25), \quad k=1,\dots,2$$
\subsection{Data}
\code{data(demonsnacks) \\
y <- log(demonsnacks$Calories) \\
X <- cbind(1, as.matrix(log(demonsnacks[,-2]+1))) \\
J <- ncol(X) \\
for (j in 2:J) \{X[,j] <- CenterScale(X[,j])\} \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=rep(0,J), sigma=rep(0,2))) \\
pos.beta <- grep("beta", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
PGF <- function(Data) return(c(rnormv(Data$J,0,10), rhalfcauchy(2,5))) \\
MyData <- list(J=J, PGF=PGF, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.beta=pos.beta, pos.sigma=pos.sigma, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, c(1000, rep(sigma[2], Data$J-1)), \\
\hspace*{0.62 in} log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, t(beta)) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma[1], log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rnorm(length(mu), mu, sigma[1]), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(1,J), rep(1,2))}

\section{Robust Regression} \label{robust.reg}
By replacing the normal distribution with the Student t distribution, linear regression is often called robust regression. As an alternative approach to robust regression, consider Laplace regression (see section \ref{laplace.reg}).
\subsection{Form}
$$\textbf{y} \sim \mathrm{t}(\mu, \sigma^2, \nu)$$
$$\mu = \textbf{X}\beta$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\sigma \sim \mathcal{HC}(25)$$
$$\nu \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{N <- 100 \\
J <- 5 \\
X <- matrix(1,N,J) \\
for (j in 2:J) \{X[,j] <- rnorm(N,runif(1,-3,3),runif(1,0.1,1))\} \\
beta <- runif(J,-3,3) \\
e <- rst(N,0,1,5) \\
y <- tcrossprod(X, t(beta)) + e \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=rep(0,J), sigma=0, nu=0)) \\
pos.beta <- grep("beta", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
pos.nu <- grep("nu", parm.names) \\
PGF <- function(Data) return(c(rnormv(Data$J,0,10), rhalfcauchy(2,5))) \\
MyData <- list(J=J, PGF=PGF, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.beta=pos.beta, pos.sigma=pos.sigma, \\
\hspace*{0.27 in} pos.nu=pos.nu, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} parm[Data$pos.nu] <- nu <- interval(parm[Data$pos.nu], 1e-100, Inf) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} nu.prior <- dhalfcauchy(nu, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, t(beta)) \\
\hspace*{0.27 in} LL <- sum(dst(Data$y, mu, sigma, nu, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + sigma.prior + nu.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rst(length(mu), mu, sigma, nu), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), 1, 5)}

\section{Seemingly Unrelated Regression (SUR)} \label{sur}
The following data was used by \citet{zellner62} when introducing the Seemingly Unrelated Regression methodology. This model uses the conjugate Wishart distribution for precision in a multivariate normal distribution. See section \ref{cov.sep.strat} for a non-Wishart alternative that is more flexible and converges much faster.
\subsection{Form}
$$\textbf{Y}_{t,k} \sim \mathcal{N}_K(\mu_{t,k}, \Omega^{-1}), \quad t=1,\dots,T; \quad k=1,\dots,K$$
$$\mu_{1,t} = \alpha_1 + \alpha_2 \textbf{X}_{t-1,1} + \alpha_3 \textbf{X}_{t-1,2}, \quad t=2,\dots,T$$
$$\mu_{2,t} = \beta_1 + \beta_2 \textbf{X}_{t-1,3} + \beta_3 \textbf{X}_{t-1,4}, \quad t=2,\dots,T$$
$$\Omega \sim \mathcal{W}_{K+1}(\textbf{S}), \quad \textbf{S} = \textbf{I}_K$$
$$\alpha_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
where J=3, K=2, and T=20.
\subsection{Data}
\code{T <- 20 \#Time-periods \\
year <- c(1935,1936,1937,1938,1939,1940,1941,1942,1943,1944,1945,1946, \\
\hspace*{0.27 in} 1947,1948,1949,1950,1951,1952,1953,1954) \\
IG <- c(33.1,45.0,77.2,44.6,48.1,74.4,113.0,91.9,61.3,56.8,93.6,159.9, \\
\hspace*{0.27 in} 147.2,146.3,98.3,93.5,135.2,157.3,179.5,189.6) \\
VG <- c(1170.6,2015.8,2803.3,2039.7,2256.2,2132.2,1834.1,1588.0,1749.4, \\
\hspace*{0.27 in} 1687.2,2007.7,2208.3,1656.7,1604.4,1431.8,1610.5,1819.4,2079.7, \\
\hspace*{0.27 in} 2371.6,2759.9) \\
CG <- c(97.8,104.4,118.0,156.2,172.6,186.6,220.9,287.8,319.9,321.3,319.6, \\
\hspace*{0.27 in} 346.0,456.4,543.4,618.3,647.4,671.3,726.1,800.3,888.9) \\
IW <- c(12.93,25.90,35.05,22.89,18.84,28.57,48.51,43.34,37.02,37.81, \\
\hspace*{0.27 in} 39.27,53.46,55.56,49.56,32.04,32.24,54.38,71.78,90.08,68.60) \\
VW <- c(191.5,516.0,729.0,560.4,519.9,628.5,537.1,561.2,617.2,626.7, \\
\hspace*{0.27 in} 737.2,760.5,581.4,662.3,583.8,635.2,723.8,864.1,1193.5,1188.9) \\
CW <- c(1.8,0.8,7.4,18.1,23.5,26.5,36.2,60.8,84.4,91.2,92.4,86.0,111.1, \\
\hspace*{0.27 in} 130.6,141.8,136.7,129.7,145.5,174.8,213.5) \\
J <- 2 \#Number of dependent variables \\
Y <- matrix(c(IG,IW), T, J) \\
S <- diag(J) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(alpha=rep(0,3), beta=rep(0,3), \\
\hspace*{0.27 in} U=diag(J)), uppertri=c(0,0,1)) \\
pos.alpha <- grep("alpha", parm.names) \\
pos.beta <- grep("beta", parm.names) \\
PGF <- function(Data) return(c(rnormv(3,0,10), rnormv(3,0,10), \\
\hspace*{0.27 in} upper.triangle(rwishartc(Data$J+1,Data$S), diag=TRUE))) \\
MyData <- list(J=J, PGF=PGF, S=S, T=T, Y=Y, CG=CG, CW=CW, IG=IG, IW=IW, \\
\hspace*{0.27 in} VG=VG, VW=VW, mon.names=mon.names, parm.names=parm.names, \\
\hspace*{0.27 in} pos.alpha=pos.alpha, pos.beta=pos.beta) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[Data$pos.alpha] \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} U <- as.parm.matrix(U, Data$J, parm, Data, chol=TRUE) \\
\hspace*{0.27 in} diag(U) <- exp(diag(U)) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- sum(dnormv(alpha, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} U.prior <- dwishartc(U, Data$J+1, Data$S, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- Data$Y \\
\hspace*{0.27 in} mu[-1,1] <- alpha[1] + alpha[2]*Data$CG[-Data$T] + \\
\hspace*{0.62 in} alpha[3]*Data$VG[-Data$T] \\
\hspace*{0.27 in} mu[-1,2] <- beta[1] + beta[2]*Data$CW[-Data$T] + \\
\hspace*{0.62 in} beta[3]*Data$VW[-Data$T] \\
\hspace*{0.27 in} LL <- sum(dmvnpc(Data$Y[-1,], mu[-1,], U, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior + U.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rmvnpc(nrow(mu), mu, U), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,3), rep(0,3), upper.triangle(S, diag=TRUE))}

\section{Simultaneous Equations} \label{simultaneous}
This example of simultaneous equations uses Klein's Model I \citep{kleine50} regarding economic fluctations in the United States in 1920-1941 (\textbf{N}=22). Usually, this example is modeled with 3-stage least sqaures (3SLS), excluding the uncertainty from multiple stages. By constraining each element in the instrumental variables matrix $\nu \in [-10,10]$, this example estimates the model without resorting to stages. The dependent variable is matrix \textbf{Y}, in which $\textbf{Y}_{1,1:N}$ is \textbf{C} or Consumption, $\textbf{Y}_{2,1:N}$ is \textbf{I} or Investment, and $\textbf{Y}_{3,1:N}$ is \textbf{Wp} or Private Wages. Here is a data dictionary: \\
\code{\hspace*{0.27 in} A = Time Trend measured as years from 1931 \\
\hspace*{0.27 in} \textbf{C} = Consumption \\
\hspace*{0.27 in} \textbf{G} = Government Nonwage Spending \\
\hspace*{0.27 in} \textbf{I} = Investment \\
\hspace*{0.27 in} \textbf{K} = Capital Stock \\
\hspace*{0.27 in} \textbf{P} = Private (Corporate) Profits \\
\hspace*{0.27 in} \textbf{T} = Indirect Business Taxes Plus Neg Exports \\
\hspace*{0.27 in} \textbf{Wg} = Government Wage Bill \\
\hspace*{0.27 in} \textbf{Wp} = Private Wages \\
\hspace*{0.27 in} \textbf{X} = Equilibrium Demand (GNP) \\
}
See \citet{kleine50} for more information.
\subsection{Form}
$$\textbf{Y} \sim \mathcal{N}_3(\mu, \Omega^{-1})$$
$$ \mu_{1,1} = \alpha_1 + \alpha_2 \nu_{1,1} + \alpha_4 \nu_{2,1}$$
$$ \mu_{1,i} = \alpha_1 + \alpha_2 \nu_{1,i} + \alpha_3 \textbf{P}_{i-1} + \alpha_4 \nu_{2,i}, \quad i=2,\dots,N$$
$$ \mu_{2,1} = \beta_1 + \beta_2 \nu_{1,1} + \beta_4 \textbf{K}_1$$
$$ \mu_{2,i} = \beta_1 + \beta_2 \nu_{1,i} + \beta_3 \textbf{P}_{i-1} + \beta_4 \textbf{K}_i, \quad i=2,\dots,N$$
$$\mu_{3,1} = \gamma_1 + \gamma_2 \nu_{3,1} + \gamma_4 \textbf{A}_1$$
$$\mu_{3,i} = \gamma_1 + \gamma_2 \nu_{3,i} + \gamma_3 \textbf{X}_{i-1} + \gamma_4 \textbf{A}_i, \quad i=2,\dots,N$$
$$\textbf{Z}_{j,i} \sim \mathcal{N}(\nu_{j,i}, \sigma^2_j), \quad j=1,\dots,3$$
$$\nu_{j,1} = \pi_{j,1} + \pi_{j,3} \textbf{K}_1 + \pi_{j,5} \textbf{A}_1 + \pi_{j,6} \textbf{T}_1 + \pi_{j,7} \textbf{G}_1, \quad j=1,\dots,3$$
$$\nu_{j,i} = \pi_{j,1} + \pi_{j,2} \textbf{P}_{i-1} + \pi_{j,3} \textbf{K}_i + \pi_{j,4} \textbf{X}_{i-1} + \pi_{j,5} \textbf{A}_i + \pi_{j,6} \textbf{T}_i + \pi \textbf{G}_i, \quad i=1,\dots,N, \quad j=1,\dots,3$$
$$\alpha_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,4$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,4$$
$$\gamma_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,4$$
$$\pi_{j,i} \sim \mathcal{N}(0, 1000) \in [-10,10], \quad j=1,\dots,3, \quad i=1,\dots,N$$
$$\sigma_j \sim \mathcal{HC}(25), \quad j=1,\dots,3$$
$$\Omega \sim \mathcal{W}_4(\textbf{S}), \quad \textbf{S} = \textbf{I}_3$$
\subsection{Data}
\code{N <- 22 \\
A <- c(-11,-10,-9,-8,-7,-6,-5,-4,-3,-2,-1,0,1,2,3,4,5,6,7,8,9,10) \\
C <- c(39.8,41.9,45,49.2,50.6,52.6,55.1,56.2,57.3,57.8,55,50.9,45.6,46.5, \\
\hspace*{0.27 in} 48.7,51.3,57.7,58.7,57.5,61.6,65,69.7) \\
G <- c(2.4,3.9,3.2,2.8,3.5,3.3,3.3,4,4.2,4.1,5.2,5.9,4.9,3.7,4,4.4,2.9,4.3, \\
\hspace*{0.27 in} 5.3,6.6,7.4,13.8) \\
I <- c(2.7,-0.2,1.9,5.2,3,5.1,5.6,4.2,3,5.1,1,-3.4,-6.2,-5.1,-3,-1.3,2.1,2, \\
\hspace*{0.27 in} -1.9,1.3,3.3,4.9) \\
K <- c(180.1,182.8,182.6,184.5,189.7,192.7,197.8,203.4,207.6,210.6,215.7, \\
\hspace*{0.27 in} 216.7,213.3,207.1,202,199,197.7,199.8,201.8,199.9,201.2,204.5) \\
P <- c(12.7,12.4,16.9,18.4,19.4,20.1,19.6,19.8,21.1,21.7,15.6,11.4,7,11.2, \\
\hspace*{0.27 in} 12.3,14,17.6,17.3,15.3,19,21.1,23.5) \\
T <- c(3.4,7.7,3.9,4.7,3.8,5.5,7,6.7,4.2,4,7.7,7.5,8.3,5.4,6.8,7.2,8.3,6.7, \\
\hspace*{0.27 in} 7.4,8.9,9.6,11.6) \\
Wg <- c(2.2,2.7,2.9,2.9,3.1,3.2,3.3,3.6,3.7,4,4.2,4.8,5.3,5.6,6,6.1,7.4, \\
\hspace*{0.27 in} 6.7,7.7,7.8,8,8.5) \\
Wp <- c(28.8,25.5,29.3,34.1,33.9,35.4,37.4,37.9,39.2,41.3,37.9,34.5,29,28.5, \\
\hspace*{0.27 in} 30.6,33.2,36.8,41,38.2,41.6,45,53.3) \\
X <- c(44.9,45.6,50.1,57.2,57.1,61,64,64.4,64.5,67,61.2,53.4,44.3,45.1, \\
\hspace*{0.27 in} 49.7,54.4,62.7,65,60.9,69.5,75.7,88.4) \\
year <- c(1920,1921,1922,1923,1924,1925,1926,1927,1928,1929,1930,1931,1932, \\
\hspace*{0.27 in} 1933,1934,1935,1936,1937,1938,1939,1940,1941) \\
Y <- matrix(c(C,I,Wp),3,N, byrow=TRUE) \\
Z <- matrix(c(P, Wp+Wg, X), 3, N, byrow=TRUE) \\
S <- diag(nrow(Y)) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(alpha=rep(0,4), beta=rep(0,4), \\
\hspace*{0.27 in} gamma=rep(0,4), pi=matrix(0,3,7), sigma=rep(0,3), \\
\hspace*{0.27 in} U=diag(3)), uppertri=c(0,0,0,0,0,1)) \\
pos.alpha <- grep("alpha", parm.names) \\
pos.beta <- grep("beta", parm.names) \\
pos.gamma <- grep("gamma", parm.names) \\
pos.pi <- grep("pi", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
PGF <- function(Data) return(c(rnormv(4,0,10), rnormv(4,0,10), \\
\hspace*{0.27 in} rnormv(4,0,10), rnormv(3*7,0,10), rhalfcauchy(3,5), \\
\hspace*{0.27 in} upper.triangle(rwishartc(nrow(Data$S)+1,Data$S), diag=TRUE))) \\
MyData <- list(A=A, C=C, G=G, I=I, K=K, N=N, P=P, PGF=PGF, S=S, T=T, Wg=Wg, \\
\hspace*{0.27 in} Wp=Wp, X=X, Y=Y, Z=Z, mon.names=mon.names, parm.names=parm.names, \\
\hspace*{0.27 in} pos.alpha=pos.alpha, pos.beta=pos.beta, pos.gamma=pos.gamma, \\
\hspace*{0.27 in} pos.pi=pos.pi, pos.sigma=pos.sigma) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[Data$pos.alpha] \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} gamma <- parm[Data$pos.gamma] \\
\hspace*{0.27 in} parm[Data$pos.pi] <- pi <- interval(parm[Data$pos.pi], -10, 10) \\
\hspace*{0.27 in} pi <- matrix(pi, 3, 7)
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma
\hspace*{0.27 in} U <- as.parm.matrix(U, nrow(Data$S), parm, Data, chol=TRUE) \\
\hspace*{0.27 in} parm[grep("Omega", Data$parm.names)] <- upper.triangle(Omega, \\
\hspace*{0.62 in} diag=TRUE) \\
\hspace*{0.27 in} diag(U) <- exp(diag(U)) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- sum(dnormv(alpha, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} gamma.prior <- sum(dnormv(gamma, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} pi.prior <- sum(dnormv(pi, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} U.prior <- dwishartc(U, nrow(Data$S)+1, Data$S, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- nu <- matrix(0,3,Data$N) \\
\hspace*{0.27 in} for (i in 1:3) \{ \\
\hspace*{0.62 in} nu[i,1] <- pi[i,1] + pi[i,3]*Data$K[1] + pi[i,5]*Data$A[1] + \\
\hspace*{0.95 in} pi[i,6]*Data$T[1] + pi[i,7]*Data$G[1] \\
\hspace*{0.62 in} nu[i,-1] <- pi[i,1] + pi[i,2]*Data$P[-Data$N] + \\
\hspace*{0.95 in} pi[i,3]*Data$K[-1] + pi[i,4]*Data$X[-Data$N] + \\
\hspace*{0.95 in} pi[i,5]*Data$A[-1] + pi[i,6]*Data$T[-1] + \\
\hspace*{0.95 in} pi[i,7]*Data$G[-1]\} \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$Z, nu, matrix(sigma, 3, Data$N), log=TRUE)) \\
\hspace*{0.27 in} mu[1,1] <- alpha[1] + alpha[2]*nu[1,1] + alpha[4]*nu[2,1] \\
\hspace*{0.27 in} mu[1,-1] <- alpha[1] + alpha[2]*nu[1,-1] + \\
\hspace*{0.62 in} alpha[3]*Data$P[-Data$N] + alpha[4]*nu[2,-1] \\
\hspace*{0.27 in} mu[2,1] <- beta[1] + beta[2]*nu[1,1] + beta[4]*Data$K[1] \\
\hspace*{0.27 in} mu[2,-1] <- beta[1] + beta[2]*nu[1,-1] + \\
\hspace*{0.62 in} beta[3]*Data$P[-Data$N] + beta[4]*Data$K[-1] \\
\hspace*{0.27 in} mu[3,1] <- gamma[1] + gamma[2]*nu[3,1] + gamma[4]*Data$A[1] \\
\hspace*{0.27 in} mu[3,-1] <- gamma[1] + gamma[2]*nu[3,-1] + \\
\hspace*{0.62 in} gamma[3]*Data$X[-Data$N] + gamma[4]*Data$A[-1] \\
\hspace*{0.27 in} LL <- LL + sum(dmvnpc(t(Data$Y), t(mu), U, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior + gamma.prior + pi.prior + \\
\hspace*{0.62 in} sigma.prior + U.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=t(rmvnp(ncol(mu), t(mu), U)), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,4), rep(0,4), rep(0,4), rep(0,3*7), rep(1,3), \\
\hspace*{0.27 in} upper.triangle(S, diag=TRUE))}

\section{Space-Time, Dynamic} \label{spacetime.dynamic}
This approach to space-time or spatiotemporal modeling applies kriging to a stationary spatial component for points in space $s=1,\dots,S$ first at time $t=1$, where space is continuous and time is discrete. Vector $\zeta$ contains these spatial effects. Next, SSM (State Space Model) or DLM (Dynamic Linear Model) components are applied to the spatial parameters ($\phi$, $\kappa$, and $\lambda$) and regression effects ($\beta$). These parameters are allowed to vary dynamically with time $t=2,\dots,T$, and the resulting spatial process is estimated for each of these time-periods. When time is discrete, a dynamic space-time process can be applied. The matrix $\Theta$ contains the dynamically varying stationary spatial effects, or space-time effects. Spatial coordinates are given in longitude and latitude for $s=1,\dots,S$ points in space and measurements are taken across discrete time-periods $t=1,\dots,T$ for $\textbf{Y}_{s,t}$. The dependent variable is also a function of design matrix $\textbf{X}$ (which may also be dynamic, but is static in this example) and dynamic regression effects matrix $\beta_{1:J,1:T}$. For more information on kriging, see section \ref{kriging}. For more information on SSMs or DLMs, see section \ref{ssm.lin.reg}. To extend this to a large spatial data set, consider incorporating the predictive process kriging example in section \ref{kriging.pp}.
\subsection{Form}
$$\textbf{Y}_{s,t} \sim \mathcal{N}(\mu_{s,t}, \sigma^2_1), \quad s=1,\dots,S, \quad t=1,\dots,T$$
$$\mu_{s,t} = \textbf{X}_{s,1:J} \beta_{1:J,t} + \Theta_{s,t}$$
$$\Theta_{s,t} = \frac{\Sigma_{s,s,t}}{\sum^S_{r=1} \Sigma_{r,s,t}} \Theta_{s,t-1}, \quad s=1,\dots,S, \quad t=2,\dots,T$$
$$\Theta_{s,1} = \zeta_s$$
$$\zeta \sim \mathcal{N}_S(0, \Sigma_{1:S,1:S,1})$$
$$\Sigma_{1:S,1:S,t} = \lambda^2_t \exp(-\phi_t \textbf{D})^{\kappa[t]}$$
$$\sigma_1 \sim \mathcal{HC}(25)$$
$$\beta_{j,1} \sim \mathcal{N}(0, 1000), \quad j=1,\dots,2$$
$$\beta_{1,t} \sim \mathcal{N}(\beta_{1,t-1}, \sigma^2_2), \quad t=2,\dots,T$$
$$\beta_{2,t} \sim \mathcal{N}(\beta_{2,t-1}, \sigma^2_3), \quad t=2,\dots,T$$
$$\phi_1 \sim \mathcal{HN}(1000)$$
$$\phi_t \sim \mathcal{N}(\phi_{t-1}, \sigma^2_4) \in [0,\infty], \quad t=2,\dots,T$$
$$\kappa_1 \sim \mathcal{HN}(1000)$$
$$\kappa_t \sim \mathcal{N}(\kappa_{t-1}, \sigma^2_5) \in [0,\infty], \quad t=2,\dots,T$$
$$\lambda_1 \sim \mathcal{HN}(1000)$$
$$\lambda_t \sim \mathcal{N}(\lambda_{t-1}, \sigma^2_6) \in [0,\infty], \quad t=2,\dots,T$$
\subsection{Data}
\code{S <- 20 \\
T <- 10 \\
longitude <- runif(S,0,100) \\
latitude <- runif(S,0,100) \\
D <- as.matrix(dist(cbind(longitude,latitude), diag=TRUE, upper=TRUE)) \\
beta <- matrix(c(50,2), 2, T) \\
phi <- rep(1,T); kappa <- rep(1.5,T); lambda <- rep(10000,T) \\
for (t in 2:T) \{ \\
\hspace*{0.27 in} beta[1,t-1] <- beta[1,t-1] + rnorm(1,0,1) \\
\hspace*{0.27 in} beta[2,t-1] <- beta[2,t-1] + rnorm(1,0,0.1) \\
\hspace*{0.27 in} phi[t] <- phi[t-1] + rnorm(1,0,0.1) \\
\hspace*{0.27 in} if(phi[t] < 0.001) phi[t] <- 0.001 \\
\hspace*{0.27 in} kappa[t] <- kappa[t-1] + rnorm(1,0,0.1) \\
\hspace*{0.27 in} lambda[t] <- lambda[t-1] + rnorm(1,0,1000)\} \\
Sigma <- array(0, dim=c(S,S,T)) \\
for (t in 1:T) \{ \\
\hspace*{0.27 in} Sigma[ , ,t] <- lambda[t] * exp(-phi[t] * D)\textasciicircum kappa[t]\} \\
zeta <- as.vector(apply(rmvn(1000, rep(0,S), Sigma[ , ,1]), 2, mean)) \\
Theta <- matrix(zeta,S,T) \\
for (t in 2:T) \{for (s in 1:S) \{ \\
\hspace*{0.27 in} Theta[,t] <- sum(Sigma[,s,t] / sum(Sigma[,s,t]) * Theta[,t-1])\}\} \\
X <- matrix(runif(S*2,-2,2),S,2); X[,1] <- 1 \\
mu <- tcrossprod(X, t(beta)) \\
Y <- mu + Theta + matrix(rnorm(S*T,0,0.1),S,T) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(zeta=rep(0,S), beta=matrix(0,2,T), \\
\hspace*{0.27 in} phi=rep(0,T), kappa=rep(0,T), lambda=rep(0,T), sigma=rep(0,6))) \\
pos.zeta <- grep("zeta", parm.names) \\
pos.beta <- grep("beta", parm.names) \\
pos.phi <- grep("phi", parm.names) \\
pos.kappa <- grep("kappa", parm.names) \\
pos.lambda <- grep("lambda", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
PGF <- function(Data) return(c(rmvn(1, rep(0,Data$S), \\
\hspace*{0.27 in} rhalfnorm(1,sqrt(1000))\textasciicircum 2 * \\
\hspace*{0.27 in} exp(-rhalfnorm(1,sqrt(1000))*Data$D)\textasciicircum rhalfnorm(1,sqrt(1000))), \\
\hspace*{0.27 in} rnormv(2*Data$T,0,1000), rhalfnorm(Data$T,sqrt(1000)), \\
\hspace*{0.27 in} rhalfnorm(Data$T,sqrt(1000)), rhalfnorm(Data$T,sqrt(1000)), \\
\hspace*{0.27 in} rhalfcauchy(6,5))) \\
MyData <- list(D=D, PGF=PGF, S=S, T=T, X=X, Y=Y, latitude=latitude, \\
\hspace*{0.27 in} longitude=longitude, mon.names=mon.names, parm.names=parm.names, \\
\hspace*{0.27 in} pos.zeta=pos.zeta, pos.beta=pos.beta, pos.phi=pos.phi, \\
\hspace*{0.27 in} pos.kappa=pos.kappa, pos.lambda=pos.lambda, pos.sigma=pos.sigma) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- matrix(parm[Data$pos.beta], 2, Data$T) \\
\hspace*{0.27 in} zeta <- parm[Data$pos.zeta] \\
\hspace*{0.27 in} parm[Data$pos.phi] <- phi <- interval(parm[Data$pos.phi], 1e-100, Inf) \\
\hspace*{0.27 in} kappa <- interval(parm[Data$pos.kappa], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.kappa] <- kappa \\
\hspace*{0.27 in} lambda <- interval(parm[Data$pos.lambda], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.lambda] <- lambda \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} Sigma <- array(0, dim=c(Data$S, Data$S, Data$T)) \\
\hspace*{0.27 in} for (t in 1:Data$T) \{ \\
\hspace*{0.62 in} Sigma[ , ,t] <- lambda[t]\textasciicircum 2 * exp(-phi[t] * Data$D)\textasciicircum kappa[t]\} \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta[,1], 0, 1000, log=TRUE), \\
\hspace*{0.62 in} dnorm(beta[,-1], beta[,-Data$T], matrix(sigma[2:3], 2, \\
\hspace*{0.62 in} Data$T-1), log=TRUE)) \\
\hspace*{0.27 in} zeta.prior <- dmvn(zeta, rep(0,Data$S), Sigma[ , , 1], log=TRUE) \\
\hspace*{0.27 in} phi.prior <- sum(dhalfnorm(phi[1], sqrt(1000), log=TRUE), \\
\hspace*{0.62 in} dtrunc(phi[-1], "norm", a=0, b=Inf, mean=phi[-Data$T], \\
\hspace*{0.62 in} sd=sigma[4], log=TRUE)) \\
\hspace*{0.27 in} kappa.prior <- sum(dhalfnorm(kappa[1], sqrt(1000), log=TRUE), \\
\hspace*{0.62 in} dtrunc(kappa[-1], "norm", a=0, b=Inf, mean=kappa[-Data$T], \\
\hspace*{0.62 in} sd=sigma[5], log=TRUE)) \\
\hspace*{0.27 in} lambda.prior <- sum(dhalfnorm(lambda[1], sqrt(1000), log=TRUE), \\
\hspace*{0.62 in} dtrunc(lambda[-1], "norm", a=0, b=Inf, mean=lambda[-Data$T], \\
\hspace*{0.62 in} sd=sigma[6], log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, t(beta)) \\
\hspace*{0.27 in} Theta <- matrix(zeta, Data$S, Data$T) \\
\hspace*{0.27 in} for (t in 2:Data$T) \{ \\
\hspace*{0.62 in} for (s in 1:Data$S) \{ \\
\hspace*{0.98 in} Theta[,t] <- Sigma[,s,t] / sum(Sigma[,s,t]) * Theta[,t-1]\}\} \\
\hspace*{0.27 in} mu <- mu + Theta \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$Y, mu, sigma[1], log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + zeta.prior + sum(phi.prior) + \\
\hspace*{0.62 in} sum(kappa.prior) + sum(lambda.prior) + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rnorm(prod(dim(mu)), mu, sigma[1]), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,S), rep(c(mean(Y),0),T), rep(1,T), rep(1,T), \\
\hspace*{0.27 in} rep(1,T), rep(1,6))}

\section{Space-Time, Nonseparable} \label{spacetime.nonsep}
This approach to space-time or spatiotemporal modeling applies kriging both to the stationary spatial and temporal components, where space is continuous and time is discrete. Matrix $\Xi$ contains the space-time effects. Spatial coordinates are given in longitude and latitude for $s=1,\dots,S$ points in space and measurements are taken across time-periods $t=1,\dots,T$ for $\textbf{Y}_{s,t}$. The dependent variable is also a function of design matrix $\textbf{X}$ and regression effects vector $\beta$. For more information on kriging, see section \ref{kriging}. This example uses a nonseparable, stationary covariance function in which space and time are separable only when $\psi=0$. To extend this to a large space-time data set, consider incorporating the predictive process kriging example in section \ref{kriging.pp}.
\subsection{Form}
$$\textbf{Y}_{s,t} \sim \mathcal{N}(\mu_{s,t}, \sigma^2_1), \quad s=1,\dots,S, \quad t=1,\dots,T$$
$$\mu = \textbf{X} \beta + \Xi$$
$$\Xi \sim \mathcal{N}_{ST}(\Xi_\mu, \Sigma)$$
$$ \Sigma = \sigma^2_2 \exp \left (-\frac{\textbf{D}_S}{\phi_1}^\kappa - \frac{\textbf{D}_T}{\phi_2}^\lambda - \psi \frac{\textbf{D}_S}{\phi_1}^\kappa \frac{\textbf{D}_T}{\phi_2}^\lambda \right )$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\phi_k \sim \mathcal{U}(1, 5), \quad k=1,\dots,2$$
$$\sigma_k \sim \mathcal{HC}(25), \quad k=1,\dots,2$$
$$\psi \sim \mathcal{HC}(25)$$
$$\Xi_\mu = 0$$
$$\kappa = 1, \quad \lambda = 1$$
\subsection{Data}
\code{S <- 10 \\
T <- 5 \\
longitude <- runif(S,0,100) \\
latitude <- runif(S,0,100) \\
D.S <- as.matrix(dist(cbind(rep(longitude,T),rep(latitude,T)), diag=TRUE, \\
\hspace*{0.27 in} upper=TRUE)) \\
D.T <- as.matrix(dist(cbind(rep(1:T,each=S),rep(1:T,each=S)), diag=TRUE, \\
\hspace*{0.27 in} upper=TRUE)) \\
Sigma <- 10000 * exp(-D.S/3 - D.T/2 - 0.2*(D.S/3)*(D.T/2)) \\
Xi <- as.vector(apply(rmvn(1000, rep(0,S*T), Sigma), 2, mean)) \\
Xi <- matrix(Xi,S,T) \\
beta <- c(50,2) \\
X <- matrix(runif(S*2,-2,2),S,2); X[,1] <- 1 \\
mu <- as.vector(tcrossprod(X, t(beta))) \\
Y <- mu + Xi \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(Xi=matrix(0,S,T), beta=rep(0,2), \\
\hspace*{0.27 in} phi=rep(0,2), sigma=rep(0,2), psi=0)) \\
pos.Xi <- grep("Xi", parm.names) \\
pos.beta <- grep("beta", parm.names) \\
pos.phi <- grep("phi", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
pos.psi <- grep("psi", parm.names) \\
PGF <- function(Data) return(c(rmvn(1, rep(0,Data$S*Data$T), \\
\hspace*{0.27 in} rhalfcauchy(1,25)\textasciicircum 2 * exp(-(Data$D.S / runif(1,1,5)) - \\
\hspace*{0.27 in} (Data$D.T / runif(1,1,5)) - \\
\hspace*{0.27 in} rhalfcauchy(1,5)*(Data$D.S / rhalfcauchy(1,5)))), \\
\hspace*{0.27 in} rnormv(2,0,1000), runif(2,1,5), rhalfcauchy(3,25))) \\
MyData <- list(D.S=D.S, D.T=D.T, PGF=PGF, S=S, T=T, X=X, Y=Y, \\
\hspace*{0.27 in} latitude=latitude, longitude=longitude, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.Xi=pos.Xi, pos.beta=pos.beta, \\
\hspace*{0.27 in} pos.phi=pos.phi, pos.sigma=pos.sigma, pos.psi=pos.psi) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} Xi.mu <- rep(0,Data$S*Data$T) \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} Xi <- parm[Data$pos.Xi] \\
\hspace*{0.27 in} kappa <- 1; lambda <- 1 \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} parm[Data$pos.phi] <- phi <- interval(parm[Data$pos.phi], 1, 5) \\
\hspace*{0.27 in} parm[Data$pos.psi] <- psi <- interval(parm[Data$pos.psi], 1e-100, Inf) \\
\hspace*{0.27 in} Sigma <- sigma[2]*sigma[2] * exp(-(Data$D.S / phi[1])\textasciicircum kappa - \\
\hspace*{0.62 in} (Data$D.T / phi[2])\textasciicircum lambda - \\
\hspace*{0.62 in} psi*(Data$D.S / phi[1])\textasciicircum kappa * (Data$D.T / phi[2])\textasciicircum lambda) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} Xi.prior <- dmvn(Xi, Xi.mu, Sigma, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} phi.prior <- sum(dunif(phi, 1, 5, log=TRUE)) \\
\hspace*{0.27 in} psi.prior <- dhalfcauchy(psi, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} Xi <- matrix(Xi, Data$S, Data$T) \\
\hspace*{0.27 in} mu <- as.vector(tcrossprod(Data$X, t(beta))) + Xi \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$Y, mu, sigma[1], log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + Xi.prior + sigma.prior + phi.prior + psi.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rnorm(prod(dim(mu)), mu, sigma[1]), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,S*T), mean(Y), 1, rep(1,2), rep(1,2), 1)}

\section{Space-Time, Separable} \label{spacetime.sep}
This introductory approach to space-time or spatiotemporal modeling applies kriging both to the stationary spatial and temporal components, where space is continuous and time is discrete. Vector $\zeta$ contains the spatial effects and vector $\theta$ contains the temporal effects. Spatial coordinates are given in longitude and latitude for $s=1,\dots,S$ points in space and measurements are taken across time-periods $t=1,\dots,T$ for $\textbf{Y}_{s,t}$. The dependent variable is also a function of design matrix $\textbf{X}$ and regression effects vector $\beta$. For more information on kriging, see section \ref{kriging}. This example uses separable space-time covariances, which is more convenient but usually less appropriate than a nonseparable covariance function. To extend this to a large space-time data set, consider incorporating the predictive process kriging example in section \ref{kriging.pp}.
\subsection{Form}
$$\textbf{Y}_{s,t} \sim \mathcal{N}(\mu_{s,t}, \sigma^2_1), \quad s=1,\dots,S, \quad t=1,\dots,T$$
$$\mu_{s,t} = \textbf{X}_{s,1:J} \beta + \zeta_s + \Theta_{s,t}$$
$$\Theta_{s,1:T} = \theta$$
$$\theta \sim \mathcal{N}_N(\theta_\mu, \Sigma_T)$$
$$\Sigma_T = \sigma^2_3 \exp(-\phi_2 \textbf{D}_T)^\lambda$$
$$ \zeta \sim \mathcal{N}_N(\zeta_\mu, \Sigma_S)$$
$$ \Sigma_S = \sigma^2_2 \exp(-\phi_1 \textbf{D}_S)^\kappa$$
$$ \beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,2$$
$$ \sigma_k \sim \mathcal{HC}(25), \quad k=1,\dots,3$$
$$ \phi_k \sim \mathcal{U}(1, 5), \quad k=1,\dots,2$$
$$ \zeta_\mu = 0$$
$$ \theta_\mu = 0$$
$$ \kappa = 1, \quad \lambda = 1$$
\subsection{Data}
\code{S <- 20 \\
T <- 10 \\
longitude <- runif(S,0,100) \\
latitude <- runif(S,0,100) \\
D.S <- as.matrix(dist(cbind(longitude,latitude), diag=TRUE, upper=TRUE)) \\
Sigma.S <- 10000 * exp(-1.5 * D.S) \\
zeta <- as.vector(apply(rmvn(1000, rep(0,S), Sigma.S), 2, mean)) \\
D.T <- as.matrix(dist(cbind(c(1:T),c(1:T)), diag=TRUE, upper=TRUE)) \\
Sigma.T <- 10000 * exp(-3 * D.T) \\
theta <- as.vector(apply(rmvn(1000, rep(0,T), Sigma.T), 2, mean)) \\
Theta <- matrix(theta,S,T,byrow=TRUE) \\
beta <- c(50,2) \\
X <- matrix(runif(S*2,-2,2),S,2); X[,1] <- 1 \\
mu <- as.vector(tcrossprod(X, t(beta))) \\
Y <- mu + zeta + Theta + matrix(rnorm(S*T,0,0.1),S,T) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(zeta=rep(0,S), theta=rep(0,T), \\
\hspace*{0.27 in} beta=rep(0,2), phi=rep(0,2), sigma=rep(0,3))) \\
pos.zeta <- grep("zeta", parm.names) \\
pos.theta <- grep("theta", parm.names) \\
pos.beta <- grep("beta", parm.names) \\
pos.phi <- grep("phi", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
PGF <- function(Data) return(c(rmvn(1, rep(0,Data$S), \\
\hspace*{0.27 in} rhalfcauchy(1,5)\textasciicircum 2 * exp(-runif(1,1,5)*Data$D.S)), \\
\hspace*{0.27 in} rmvn(1, rep(0,Data$T), rhalfcauchy(1,5)\textasciicircum 2 * \\
\hspace*{0.27 in} exp(-runif(1,1,5)*Data$D.T)), rnormv(2,0,1000), runif(2,1,5), \\
\hspace*{0.27 in} rhalfcauchy(3,5))) \\
MyData <- list(D.S=D.S, D.T=D.T, PGF=PGF, S=S, T=T, X=X, Y=Y, \\
\hspace*{0.27 in} latitude=latitude, longitude=longitude, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.namespos.zeta=pos.zeta, pos.theta=pos.theta, \\
\hspace*{0.27 in} pos.beta=pos.beta, pos.phi=pos.phi, pos.sigma=pos.sigma) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} zeta.mu <- rep(0,Data$S) \\
\hspace*{0.27 in} theta.mu <- rep(0,Data$T) \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} zeta <- parm[Data$pos.zeta] \\
\hspace*{0.27 in} theta <- parm[Data$pos.theta] \\
\hspace*{0.27 in} kappa <- 1; lambda <- 1 \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} parm[Data$pos.phi] <- phi <- interval(parm[Data$pos.phi], 1, 5) \\
\hspace*{0.27 in} Sigma.S <- sigma[2]\textasciicircum 2 * exp(-phi[1] * Data$D.S)\textasciicircum kappa \\
\hspace*{0.27 in} Sigma.T <- sigma[3]\textasciicircum 2 * exp(-phi[2] * Data$D.T)\textasciicircum lambda \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} zeta.prior <- dmvn(zeta, zeta.mu, Sigma.S, log=TRUE) \\
\hspace*{0.27 in} theta.prior <- dmvn(theta, theta.mu, Sigma.T, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(25, log=TRUE)) \\
\hspace*{0.27 in} phi.prior <- sum(dunif(phi, 1, 5, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} Theta <- matrix(theta, Data$S, Data$T, byrow=TRUE) \\
\hspace*{0.27 in} mu <- as.vector(tcrossprod(Data$X, t(beta))) + zeta + Theta \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$Y, mu, sigma[1], log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + zeta.prior + theta.prior + sigma.prior + \\
\hspace*{0.62 in} phi.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rnorm(prod(dim(mu)), mu, sigma[1]), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,S), rep(0,T), rep(0,2), rep(1,2), rep(1,3))}

\section{Spatial Autoregression (SAR)} \label{sar}
The spatial autoregressive (SAR) model in this example uses areal data that consists of first-order neighbors that were specified and converted from point-based data with longitude and latitude coordinates.
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2)$$
$$\mu = \textbf{X}\beta + \phi \textbf{z}$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\phi \sim \mathcal{U}(-1, 1)$$
$$\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{N <- 100 \\
latitude <- runif(N,0,100); longitude <- runif(N,0,100) \\
J <- 3 \#Number of predictors, including the intercept \\
X <- matrix(runif(N*J,0,3), N, J); X[,1] <- 1 \\
beta.orig <- runif(J,0,3); phi <- runif(1,0,1) \\
D <- as.matrix(dist(cbind(longitude, latitude), diag=TRUE, upper=TRUE)) \\
W <- exp(-D) \#Inverse distance as weights \\
W <- ifelse(D == 0, 0, W) \\
epsilon <- rnorm(N,0,1) \\
y <- tcrossprod(X, t(beta.orig)) + sqrt(latitude) + sqrt(longitude) + \\
\hspace*{0.27 in} epsilon \\
Z <- W / matrix(rowSums(W), N, N) * matrix(y, N, N, byrow=TRUE) \\
z <- as.vector(apply(Z, 1, sum)) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=rep(0,J), phi=0, sigma=0)) \\
pos.beta <- grep("beta", parm.names) \\
pos.phi <- grep("phi", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
PGF <- function(Data) return(c(rnormv(Data$J,0,1000), runif(1,-1,1), \\
\hspace*{0.27 in} rhalfcauchy(1,5))) \\
MyData <- list(J=J, PGF=PGF, X=X, latitude=latitude, longitude=longitude, \\
\hspace*{0.27 in} mon.names=mon.names, parm.names=parm.names, pos.beta=pos.beta, \\
\hspace*{0.27 in} pos.phi=pos.phi, pos.sigma=pos.sigma, y=y, z=z)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} parm[Data$pos.phi] <- phi <- interval(parm[Data$pos.phi], -1, 1) \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} phi.prior <- dunif(phi, -1, 1, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, t(beta)) + phi*Data$z \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + phi.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rnorm(length(mu), mu, sigma), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), 0.5, 1)}

\section{STARMA(1,1)} \label{starma}
The data in this example of a space-time autoregressive moving average (STARMA) are coordinate-based, and the adjacency matrix \textbf{A} is created from $K$ nearest neighbors. Otherwise, an adjacency matrix may be specified as usual for areal data. Spatial coordinates are given in longitude and latitude for $s=1,\dots,S$ points in space and measurements are taken across time-periods $t=1,\dots,T$ for $\textbf{Y}_{s,t}$.
\subsection{Form}
$$\textbf{Y} \sim \mathcal{N}(\mu, \sigma^2)$$
$$\mu_{s,t} = \textbf{X}_{s,t} \beta + \phi \textbf{W1}_{s,t-1} + \theta \textbf{W2}_{s,t-1}, \quad s=1,\dots,S, \quad t=2,\dots,T$$
$$\textbf{W1} = \textbf{V} \textbf{Y}$$
$$\textbf{W2} = \textbf{V} \epsilon$$
$$\epsilon = \textbf{Y} - \mu$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\phi \sim \mathcal{U}(-1, 1)$$
$$\sigma \sim \mathcal{HC}(25)$$
$$\theta \sim \mathcal{N}(0, 1000)$$
where \textbf{V} is an adjacency matrix that is scaled so that each row sums to one, $\beta$ is a vector of regression effects, $\phi$ is the autoregressive space-time parameter, $\sigma$ is the residual variance, and $\theta$ is the moving average space-time parameter.
\subsection{Data}
\code{S <- 100 \\
T <- 10 \\
K <- 5 \#Number of nearest neighbors \\
latitude <- runif(S,0,100) \\
longitude <- runif(S,0,100) \\
X1 <- matrix(runif(S*T,-2,2), S, T) \\
X2 <- matrix(runif(S*T,-2,2), S, T) \\
for (t in 2:T) \{ \\
\hspace*{0.27 in} X1[,t] <- X1[,t-1] + runif(S,-0.1,0.1) \\
\hspace*{0.27 in} X2[,t] <- X2[,t-1] + runif(S,-0.1,0.1)\} \\
beta.orig <- runif(3,-2,2); phi.orig <- 0.8; theta.orig <- 1 \\
epsilon <- matrix(rnorm(S*T,0,0.1), S, T) \\
Z <- matrix(rnorm(S*T,0,0.1), S, T) \\
D <- as.matrix(dist(cbind(longitude, latitude), diag=TRUE, upper=TRUE)) \\
A <- exp(-D) \\
A <- ifelse(D == 0, max(D), A) \\
A <- apply(A, 1, rank) \\
A <- ifelse(A <= K, 1, 0) \\
V <- A / rowSums(A) \#Scaled matrix \\
V <- ifelse(is.nan(V), 1/ncol(V), V) \\
Y <- beta.orig[1] + beta.orig[2]*X1 + beta.orig*X2 \\
W1 <- tcrossprod(V, t(Y)) \\
Y <- Y + phi.orig*cbind(rep(0,S), W1[,-T]) \\
W2 <- tcrossprod(V, t(epsilon)) \\
Y <- Y + theta.orig*cbind(rep(0,S), W2[,-T]) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=rep(0,3), phi=0, sigma=0, theta=0)) \\
pos.beta <- grep("beta", parm.names) \\
pos.phi <- grep("phi", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
pos.theta <- grep("theta", parm.names) \\
PGF <- function(Data) return(c(rnormv(3,0,1000), runif(1,-1,1), \\
\hspace*{0.27 in} rhalfcauchy(1,5), rnormv(1,0,1000))) \\
MyData <- list(K=K, PGF=PGF, S=S, T=T, V=V, X1=X1, X2=X2, Y=Y, \\
\hspace*{0.27 in} latitude=latitude, longitude=longitude, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.beta=pos.beta, pos.phi=pos.phi, \\
\hspace*{0.27 in} pos.sigma=pos.sigma, pos.theta=pos.theta) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} parm[Data$pos.phi] <- phi <- interval(parm[Data$pos.phi], -1, 1) \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} theta <- parm[Data$pos.theta] \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} phi.prior <- dunif(phi, -1, 1, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} theta.prior <- dnormv(theta, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} W1 <- tcrossprod(Data$V, t(Data$Y)) \\
\hspace*{0.27 in} mu <- beta[1] + beta[2]*Data$X1 + beta[3]*Data$X2 + \\
\hspace*{0.62 in} phi*cbind(rep(0, Data$S), W1[,-Data$T]) \\
\hspace*{0.27 in} epsilon <- Data$Y - mu \\
\hspace*{0.27 in} W2 <- tcrossprod(Data$V, t(epsilon)) \\
\hspace*{0.27 in} mu <- mu + theta*cbind(rep(0, Data$S), W2[,-Data$T]) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$Y[,-1], mu[,-1], sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + phi.prior + sigma.prior + theta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rnorm(prod(dim(mu)), mu, sigma), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,3), 0, 1, 0)}

\section{State Space Model (SSM), Linear Regression} \label{ssm.lin.reg}
The data is presented so that the time-series is subdivided into three 
sections: modeled ($t=1,\dots,T_m$), one-step ahead forecast ($t=T_m+1$), 
and future forecast [$t=(T_m+2),\dots,T$]. Note that \code{Dyn} must also be specified for the SAMWG and SMWG MCMC algorithms.
\subsection{Form}
$$\textbf{y}_t \sim \mathcal{N}(\mu_t, \sigma^2_1), \quad t=1,\dots,T_m$$
$$\textbf{y}^{new}_t \sim \mathcal{N}(\mu_t, \sigma^2_1), \quad t=(T_m+1),\dots,T$$
$$\mu_t = \alpha + \textbf{x}_t \beta_t, \quad t=1,\dots,T$$
$$\alpha \sim \mathcal{N}(0, 1000)$$
$$\beta_1 \sim \mathcal{N}(0, 1000)$$
$$\beta_t \sim \mathcal{N}(\beta_{t-1}, \sigma^2_2), \quad t=2,\dots,T$$
$$\sigma_j \sim \mathcal{HC}(25), \quad j=1,\dots,2$$
\subsection{Data}
\code{T <- 20 \\
T.m <- 14 \\
beta.orig <- x <- rep(0,T) \\
for (t in 2:T) \{ \\
\hspace*{0.27 in} beta.orig[t] <- beta.orig[t-1] + rnorm(1,0,0.1) \\
\hspace*{0.27 in} x[t] <- x[t-1] + rnorm(1,0,0.1)\} \\
y <- 10 + beta.orig*x + rnorm(T,0,0.01) \\
y[(T.m+2):T] <- NA \\
mon.names <- rep(NA, (T-T.m)) \\
for (i in 1:(T-T.m)) {mon.names[i] <- paste("mu[",(T.m+i),"]", sep="")} \\
parm.names <- as.parm.names(list(alpha=0, beta=rep(0,T), sigma=rep(0,2))) \\
pos.alpha <- grep("alpha", parm.names) \\
pos.beta <- grep("beta", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
PGF <- function(Data) return(c(rnorm(1,0,1), rnorm(Data$T,0,1), \\
\hspace*{0.27 in} rhalfcauchy(2,5))) \\
MyData <- list(PGF=PGF, T=T, T.m=T.m, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.alpha=pos.alpha, pos.beta=pos.beta, \\
\hspace*{0.27 in} pos.sigma=pos.sigma, x=x, y=y) \\
Dyn <- matrix(paste("beta[",1:T,"]",sep=""), T, 1) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[Data$pos.alpha] \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnormv(alpha, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta[1], 0, 1000, log=TRUE), \\
\hspace*{0.62 in} dnorm(beta[-1], beta[-Data$T], sigma[2], log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- alpha + beta*Data$x \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y[1:Data$T.m], mu[1:Data$T.m], sigma[1], \\
\hspace*{0.62 in} log=TRUE)) \\
\hspace*{0.27 in} yhat <- rnorm(length(mu), alpha + c(beta[1], rnorm(Data$T-1, \\
\hspace*{0.62 in} beta[-Data$T], sigma[2])) * Data$x, sigma[1]) \#One-step ahead \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=mu[(Data$T.m+1):Data$T], \\
\hspace*{0.62 in} yhat=yhat, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(0, rep(0,T), rep(1,2))}

\section{State Space Model (SSM), Local Level} \label{ssm.ll}
The local level model is the simplest, non-trivial example of a state space model (SSM). As such, this version of a local level SSM has static variance parameters.
\subsection{Form}
$$\textbf{y}_t \sim \mathcal{N}(\mu_t, \sigma^2_1), \quad t=1,\dots,T$$
$$\mu_t \sim \mathcal{N}(\mu_{t-1}, \sigma^2_2), \quad t=2,\dots,T$$
$$\mu_1 \sim \mathcal{N}(0, 1000)$$
$$\sigma_j \sim \mathcal{HC}(25), \quad j=1,\dots,2$$
\subsection{Data}
\code{T <- 20 \\
T.m <- 14 \\
mu.orig <- rep(0,T) \\
for (t in 2:T) \{mu.orig[t] <- mu.orig[t-1] + rnorm(1,0,1)\} \\
y <- mu.orig + rnorm(T,0,0.1) \\
y[(T.m+2):T] <- NA \\
mon.names <- rep(NA, (T-T.m)) \\
for (i in 1:(T-T.m)) mon.names[i] <- paste("yhat[",(T.m+i),"]", sep="") \\
parm.names <- as.parm.names(list(mu=rep(0,T), sigma=rep(0,2))) \\
pos.mu <- grep("mu", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
PGF <- function(Data) return(c(rnormv(Data$T,0,1000), rhalfcauchy(2,5))) \\
MyData <- list(PGF=PGF, T=T, T.m=T.m, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.mu=pos.mu, pos.sigma=pos.sigma, y=y) \\
Dyn <- matrix(paste("mu[",1:T,"]",sep=""), T, 1) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} mu <- parm[Data$pos.mu] \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} mu.prior <- sum(dnormv(mu[1], 0, 1000, log=TRUE), \\
\hspace*{0.62 in} dnorm(mu[-1], mu[-Data$T], sigma[2], log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y[1:Data$T.m], mu[1:Data$T.m], sigma[1], \\
\hspace*{0.62 in} log=TRUE)) \\
\hspace*{0.27 in} yhat <- rnorm(length(mu), c(mu[1], rnorm(Data$T-1, mu[-Data$T], \\
\hspace*{0.62 in} sigma[2])), sigma[1]) \#One-step ahead
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + mu.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=mu[(Data$T.m+1):Data$T], \\
\hspace*{0.62 in} yhat=yhat, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,T), rep(1,2))}

\section{State Space Model (SSM), Local Linear Trend} \label{ssm.llt}
The local linear trend model is a state space model (SSM) that extends the local level model to include a dynamic slope parameter. For more information on the local level model, see section \ref{ssm.ll}. This example has static variance parameters.
\subsection{Form}
$$\textbf{y}_t \sim \mathcal{N}(\mu_t, \sigma^2_1), \quad t=1,\dots,T$$
$$\mu_t \sim \mathcal{N}(\mu_{t-1} + \delta_{t-1}, \sigma^2_2), \quad t=2,\dots,T$$
$$\mu_1 \sim \mathcal{N}(0, 1000)$$
$$\delta_t \sim \mathcal{N}(\delta_{t-1}, \sigma^2_3), \quad t=2,\dots,T$$
$$\delta_1 \sim \mathcal{N}(0, 1000)$$
$$\sigma_j \sim \mathcal{HC}(25), \quad j=1,\dots,3$$
\subsection{Data}
\code{T <- 20 \\
T.m <- 14 \\
mu.orig <- delta.orig <- rep(0,T) \\
for (t in 2:T) \{ \\
\hspace*{0.27 in} delta.orig[t] <- delta.orig[t-1] + rnorm(1,0,0.1) \\
\hspace*{0.27 in} mu.orig[t] <- mu.orig[t-1] + delta.orig[t-1] + rnorm(1,0,1)\} \\
y <- mu.orig + rnorm(T,0,0.1) \\
y[(T.m+2):T] <- NA \\
mon.names <- rep(NA, (T-T.m)) \\
for (i in 1:(T-T.m)) mon.names[i] <- paste("yhat[",(T.m+i),"]", sep="") \\
parm.names <- as.parm.names(list(mu=rep(0,T), delta=rep(0,T), \\
\hspace*{0.27 in} sigma=rep(0,3))) \\
pos.mu <- grep("mu", parm.names) \\
pos.delta <- grep("delta", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
PGF <- function(Data) return(c(rnormv(Data$T,0,10), \\
\hspace*{0.27 in} rnormv(Data$T,0,10), rhalfcauchy(3,5))) \\
MyData <- list(PGF=PGF, T=T, T.m=T.m, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.mu=pos.mu, pos.delta=pos.delta, \\
\hspace*{0.27 in} pos.sigma=pos.sigma, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} mu <- parm[Data$pos.mu] \\
\hspace*{0.27 in} delta <- parm[Data$pos.delta] \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} mu.prior <- sum(dnormv(mu[1], 0, 1000, log=TRUE), \\
\hspace*{0.62 in} dnorm(mu[-1], mu[-Data$T]+delta[-Data$T], sigma[2], \\
\hspace*{0.62 in} log=TRUE)) \\
\hspace*{0.27 in} delta.prior <- sum(dnormv(delta[1], 0, 1000, log=TRUE), \\
\hspace*{0.62 in} dnorm(delta[-1], delta[-Data$T], sigma[3], log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y[1:Data$T.m], mu[1:Data$T.m], sigma[1], \\
\hspace*{0.62 in} log=TRUE)) \\
\hspace*{0.27 in} yhat <- rnorm(length(mu), c(mu[1], rnorm(Data$T-1, mu[-Data$T], \\
\hspace*{0.62 in} sigma[2])), sigma[1]) \#One-step ahead \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + mu.prior + delta.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=mu[(Data$T.m+1):Data$T], \\
\hspace*{0.62 in} yhat=yhat, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,T), rep(0,T), rep(1,3))}

\section{State Space Model (SSM), Stochastic Volatility (SV)} \label{sv}
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(0, \sigma^2)$$
$$\sigma^2 = \frac{1}{\exp(\theta)}$$
$$\beta = \exp(\mu / 2)$$
$$\theta_1 \sim \mathcal{N}(\mu + \phi (\alpha - \mu), \tau)$$
$$\theta_t \sim \mathcal{N}(\mu + \phi (\theta_{t-1} - \mu), \tau), \quad t=2,\dots,T$$
$$\alpha \sim \mathcal{N}(\mu, \tau)$$
$$\phi \sim \mathcal{U}(-1, 1)$$
$$\mu \sim \mathcal{N}(0, 10)$$
$$\tau \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{T <- 20 \\
y <- rep(10,T); epsilon <- rnorm(T,0,1) \\
for (t in 2:T) \{y[t] <- 0.8*y[t-1] + epsilon[t-1]\} \\
mon.names <- c("LP",paste("sigma2[",1:T,"]",sep="")) \\
parm.names <- as.parm.names(list(theta=rep(0,T), alpha=0, phi=0, mu=0, \\
\hspace*{0.27 in} tau=0)) \\
pos.theta <- grep("theta", parm.names) \\
pos.alpha <- grep("alpha", parm.names) \\
pos.phi <- grep("phi", parm.names) \\
pos.mu <- grep("mu", parm.names) \\
pos.tau <- grep("tau", parm.names) \\
PGF <- function(Data) return(c(rnormv(Data$T,0,10), \\
\hspace*{0.27 in} rnormv(1,rnorm(1,0,10),rhalfcauchy(1,5)), \\
\hspace*{0.27 in} runif(1,-1,1), rnormv(1,0,10), rhalfcauchy(1,5)))  \\
MyData <- list(PGF=PGF, T=T, mon.names=mon.names, parm.names=parm.names, 
\hspace*{0.27 in} pos.theta=pos.theta, pos.alpha=pos.alpha, pos.phi=pos.phi, \\
\hspace*{0.27 in} pos.mu=pos.mu, pos.tau=pos.tau y=y) \\
Dyn <- matrix(paste("theta[",1:T,"]",sep=""), T, 1) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} theta <- parm[Data$pos.theta] \\
\hspace*{0.27 in} alpha <- parm[Data$pos.alpha] \\
\hspace*{0.27 in} parm[Data$pos.phi] <- phi <- interval(parm[Data$pos.phi], -1, 1) \\
\hspace*{0.27 in} mu <- parm[Data$pos.mu] \\
\hspace*{0.27 in} parm[Data$pos.tau] <- tau <- interval(parm[Data$pos.tau], 1e-100, Inf) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnormv(alpha, mu, tau, log=TRUE) \\
\hspace*{0.27 in} theta.prior <- sum(dnormv(theta[1], mu + phi*(alpha-mu), tau, \\
\hspace*{0.62 in} log=TRUE), dnormv(theta[-1], mu + phi*(theta[-Data$T]-mu), tau, \\
\hspace*{0.62 in} log=TRUE)) \\
\hspace*{0.27 in} phi.prior <- dunif(phi, -1, 1, log=TRUE) \\
\hspace*{0.27 in} mu.prior <- dnormv(mu, 0, 10, log=TRUE) \\
\hspace*{0.27 in} tau.prior <- dhalfcauchy(tau, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} beta <- exp(mu / 2) \\
\hspace*{0.27 in} sigma2 <- 1 / exp(theta) \\
\hspace*{0.27 in} LL <- sum(dnormv(Data$y, 0, sigma2, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + theta.prior + phi.prior + mu.prior + \\
\hspace*{0.62 in} tau.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, sigma2), \\
\hspace*{0.62 in} yhat=rnormv(length(Data$y), 0, sigma2), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,T), 0, 0, 0, 1)}

\section{TARCH(1)} \label{tarch}
In this TARCH example, there are two regimes, one for positive residuals in the previous time-period, and the other for negative. The TARCH parameters are the $\theta$ vector.
\subsection{Form}
$$\textbf{y}_t \sim \mathcal{N}(\mu_t, \sigma^2_t), \quad t=2,\dots,T$$
$$\sigma^2_t = \omega + \theta_1 \delta_{t-1} \epsilon^2_{t-1} + \theta_2 (1-\delta_{t-1}) \epsilon^2_{t-1}, \quad t=2,\dots,T$$
\[\delta_t = \left\{ 
\begin{array}{l l}
  1 & \quad \mbox{if $\epsilon_t > 0$}\\
  0 \\ \end{array} \right. \]
$$\epsilon = \textbf{y} - \mu$$
$$\mu_t = \alpha + \phi \textbf{y}_{t-1}, \quad t=2,\dots,T$$
$$\alpha \sim \mathcal{N}(0, 1000)$$
$$\phi \sim \mathcal{U}(-1, 1)$$
$$\omega \sim \mathcal{HC}(25)$$
$$\theta_j \sim \mathcal{U}(0, 1), \quad j=1,\dots,2$$
\subsection{Data}
\code{T <- 20 \\
phi <- 0.8 \\
epsilon <- rnorm(T) \\
epsilon <- ifelse(epsilon < 0, epsilon * 2, epsilon) \\
y <- rep(0,T) \\
for (t in 2:T) \{y[t] <- phi*y[t-1] + epsilon[t]\} \\
mon.names <- c("LP","ynew","sigma2.new") \\
parm.names <- as.parm.names(list(alpha=0, phi=0, omega=0, theta=rep(0,2))) \\
pos.alpha <- grep("alpha", parm.names) \\
pos.phi <- grep("phi", parm.names) \\
pos.omega <- grep("omega", parm.names) \\
pos.theta <- grep("theta", parm.names) \\
PGF <- function(Data) return(c(rnormv(1,0,1000), runif(1,-1,1), \\
\hspace*{0.27 in} rhalfcauchy(1,5), runif(2))) \\
MyData <- list(PGF=PGF, T=T, mon.names=mon.names, parm.names=parm.names, \\
\hspace*{0.27 in} pos.alpha=pos.alpha, pos.phi=pos.phi, pos.omega=pos.omega, \\
\hspace*{0.27 in} pos.theta=pos.theta, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[Data$pos.alpha] \\
\hspace*{0.27 in} parm[Data$pos.phi] <- phi <- interval(parm[Data$pos.phi], -1, 1) \\
\hspace*{0.27 in} omega <- interval(parm[Data$pos.omega], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.omega] <- omega \\
\hspace*{0.27 in} theta <- interval(parm[Data$pos.theta], 0.001, 0.999) \\
\hspace*{0.27 in} parm[Data$pos.theta] <- theta \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnormv(alpha, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} phi.prior <- dunif(phi, -1, 1, log=TRUE) \\
\hspace*{0.27 in} omega.prior <- dhalfcauchy(omega, 25, log=TRUE) \\
\hspace*{0.27 in} theta.prior <- sum(dunif(theta, 0, 1, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- alpha + c(0, Data$y[-Data$T]) * phi \\
\hspace*{0.27 in} epsilon <- Data$y - mu \\
\hspace*{0.27 in} delta <- (epsilon > 0) * 1 \\
\hspace*{0.27 in} sigma2 <- omega + theta[1] * c(0,delta[-Data$T]) * \\
\hspace*{0.62 in} c(0, epsilon[-Data$T]\textasciicircum 2) \\
\hspace*{0.27 in} sigma2[-1] <- sigma2[-1] + theta[2] * (1 - delta[-Data$T]) * \\
\hspace*{0.62 in} epsilon[-Data$T]\textasciicircum 2 \\
\hspace*{0.27 in} sigma2.new <- omega + theta[1] * delta[Data$T] * epsilon[Data$T]\textasciicircum 2 + \\
\hspace*{0.62 in} theta[2] * (1 - delta[Data$T]) * epsilon[Data$T]\textasciicircum 2 \\
\hspace*{0.27 in} ynew <- rnormv(1, alpha + Data$y[Data$T] * phi, sigma2.new) \\
\hspace*{0.27 in} LL <- sum(dnormv(Data$y[-1], mu[-1], sigma2[-1], log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + phi.prior + omega.prior + theta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, ynew, sigma2.new), \\
\hspace*{0.62 in} yhat=rnormv(length(mu), mu, sigma2), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(0, 0, 1, 0.5, 0.5)}

\section{Threshold Autoregression (TAR)} \label{tar}
\subsection{Form}
$$\textbf{y}_t \sim \mathcal{N}(\nu_t, \sigma^2), \quad t=1,\dots,T$$
$$\textbf{y}^{new} = \alpha_2 + \phi_2 \textbf{y}_T$$
\[\nu_t = \left\{
\begin{array}{l l}
  \alpha_1 + \phi_1 \textbf{y}_{t-1}, \quad t=1,\dots,T & \quad \mbox{if $t \ge \theta$}\\
  \alpha_2 + \phi_2 \textbf{y}_{t-1}, \quad t=1,\dots,T & \quad \mbox{if $t < \theta$} \\ \end{array} \right. \]
$$\alpha_j \sim \mathcal{N}(0, 1000) \in [-1,1], \quad j=1,\dots,2$$
$$\phi_j \sim \mathcal{N}(0, 1000), \in [-1,1], \quad j=1,\dots,2$$
$$\theta \sim \mathcal{U}(2, T-1)$$
$$\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{y <- c(0.02, -0.51, -0.30,  1.46, -1.26, -2.15, -0.91, -0.53, -1.91, \\
\hspace*{0.27 in}  2.64,  1.64,  0.15,  1.46,  1.61,  1.96, -2.67, -0.19, -3.28, \\ 
\hspace*{0.27 in}  1.89,  0.91, -0.71,  0.74, -0.10,  3.20, -0.80, -5.25,  1.03, \\ 
\hspace*{0.27 in} -0.40, -1.62, -0.80,  0.77,  0.17, -1.39, -1.28,  0.48, -1.02, \\ 
\hspace*{0.27 in}  0.09, -1.09,  0.86,  0.36,  1.51, -0.02,  0.47,  0.62, -1.36, \\ 
\hspace*{0.27 in}  1.12,  0.42, -4.39, -0.87,  0.05, -5.41, -7.38, -1.01, -1.70, \\ 
\hspace*{0.27 in}  0.64,  1.16,  0.87,  0.28, -1.69, -0.29,  0.13, -0.65,  0.83, \\ 
\hspace*{0.27 in}  0.62,  0.05, -0.14,  0.01, -0.36, -0.32, -0.80, -0.06,  0.24, \\ 
\hspace*{0.27 in}  0.23, -0.37,  0.00, -0.33,  0.21, -0.10, -0.10, -0.01, -0.40, \\ 
\hspace*{0.27 in} -0.35,  0.48, -0.28,  0.08,  0.28,  0.23,  0.27, -0.35, -0.19, \\ 
\hspace*{0.27 in}  0.24,  0.17, -0.02, -0.23,  0.03,  0.02, -0.17,  0.04, -0.39, \\ 
\hspace*{0.27 in} -0.12,  0.16,  0.17,  0.00,  0.18,  0.06, -0.36,  0.22,  0.14, \\ 
\hspace*{0.27 in} -0.17,  0.10, -0.01,  0.00, -0.18, -0.02,  0.07, -0.06,  0.06, \\ 
\hspace*{0.27 in} -0.05, -0.08, -0.07,  0.01, -0.06,  0.01,  0.01, -0.02,  0.01, \\ 
\hspace*{0.27 in}  0.01,  0.12, -0.03,  0.08, -0.10,  0.01, -0.03, -0.08,  0.04, \\ 
\hspace*{0.27 in} -0.09, -0.08,  0.01, -0.05,  0.08, -0.14,  0.06, -0.11,  0.09, \\ 
\hspace*{0.27 in}  0.06, -0.12, -0.01, -0.05, -0.15, -0.05, -0.03,  0.04,  0.00, \\ 
\hspace*{0.27 in} -0.12,  0.04, -0.06, -0.05, -0.07, -0.05, -0.14, -0.05, -0.01, \\ 
\hspace*{0.27 in} -0.12,  0.05,  0.06, -0.10,  0.00,  0.01,  0.00, -0.08,  0.00, \\ 
\hspace*{0.27 in}  0.00,  0.07, -0.01,  0.00,  0.09,  0.33,  0.13,  0.42,  0.24, \\ 
\hspace*{0.27 in} -0.36,  0.22, -0.09, -0.19, -0.10, -0.08, -0.07,  0.05,  0.07, \\ 
\hspace*{0.27 in}  0.07,  0.00, -0.04, -0.05,  0.03,  0.08,  0.26,  0.10,  0.08, \\ 
\hspace*{0.27 in}  0.09, -0.07, -0.33,  0.17, -0.03,  0.07, -0.04, -0.06, -0.06, \\ 
\hspace*{0.27 in}  0.07, -0.03,  0.00,  0.08,  0.27,  0.11,  0.11,  0.06, -0.11, \\ 
\hspace*{0.27 in} -0.09, -0.21,  0.24, -0.12,  0.11, -0.02, -0.03,  0.02, -0.10, \\ 
\hspace*{0.27 in}  0.00, -0.04,  0.01,  0.02, -0.03, -0.10, -0.09,  0.17,  0.07, \\
\hspace*{0.27 in} -0.05, -0.01, -0.05,  0.01,  0.00, -0.08, -0.05, -0.08,  0.07, \\
\hspace*{0.27 in}  0.06, -0.14,  0.02,  0.01,  0.04,  0.00, -0.13, -0.17) \\
T <- length(y) \\
mon.names <- c("LP", "ynew") \\
parm.names <- as.parm.names(list(alpha=rep(0,2), phi=rep(0,2), theta=0, \\
\hspace*{0.27 in} sigma=0)) \\
pos.alpha <- grep("alpha", parm.names) \\
pos.phi <- grep("phi", parm.names) \\
pos.theta <- grep("theta", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
PGF <- function(Data) return(c(rtrunc(4, "norm", a=-1, b=1, mean=0, \\
\hspace*{0.27 in} sd=sqrt(1000)), runif(1,2,Data$T-1), rhalfcauchy(1,5))) \\
MyData <- list(PGF=PGF, T=T, mon.names=mon.names, parm.names=parm.names, \\
\hspace*{0.27 in} pos.alpha=pos.alpha, pos.phi=pos.phi, pos.theta=pos.theta, \\
\hspace*{0.27 in} pos.sigma=pos.sigma, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} parm[Data$pos.alpha] <- alpha <- interval(parm[Data$pos.alpha], -1, 1) \\
\hspace*{0.27 in} parm[Data$pos.phi] <- phi <- interval(parm[Data$pos.phi], -1, 1) \\
\hspace*{0.27 in} theta <- interval(parm[Data$pos.theta], 2, Data$T-1) \\
\hspace*{0.27 in} parm[Data$pos.theta] <- theta \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- sum(dtrunc(alpha, "norm", a=-1, b=1, mean=0, \\
\hspace*{0.62 in} sd=sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} phi.prior <- sum(dtrunc(phi, "norm", a=-1, b=1, mean=0, \\
\hspace*{0.62 in} sd=sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} alpha.prior <- sum(dnormv(alpha, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} phi.prior <- sum(dnormv(phi, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} theta.prior <- dunif(theta, 2, Data$T-1, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- matrix(0, Data$T, 2) \\
\hspace*{0.27 in} mu[,1] <- c(alpha[1], alpha[1] + phi[1]*Data$y[-Data$T]) \\
\hspace*{0.27 in} mu[,2] <- c(alpha[2], alpha[2] + phi[2]*Data$y[-Data$T]) \\
\hspace*{0.27 in} nu <- mu[,2]; temp <- which(1:Data$T < theta) \\
\hspace*{0.27 in} nu[temp] <- mu[temp,1] \\
\hspace*{0.27 in} ynew <- rnorm(1, alpha[2] + phi[2]*Data$y[Data$T], sigma) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, nu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + phi.prior + theta.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,ynew), \\
\hspace*{0.62 in} yhat=rnorm(length(nu), nu, sigma), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,4), T/2, 1)}

\section{Time Varying AR(1) with Chebyshev Series} \label{tvarcs}
This example consists of a first-order autoregressive model, AR(1), with a time-varying parameter (TVP) $\phi$, that is a Chebyshev series constructed from a linear combination of orthonormal Chebyshev time polynomials (CTPs) and parameter vector $\beta$. The user creates basis matrix \textbf{P}, specifying polynomial degree $D$ and time $T$. Each column is a CTP of a different degree, and the first column is restricted to 1, the linear basis. CTPs are very flexible for TVPs, and estimate quickly because each is orthogonal, unlike simple polynomials and splines.
\subsection{Form}
$$\textbf{y}_t \sim \mathcal{N}(\mu_t, \sigma^2), \quad t=1,\dots,T$$
$$\mu_t = \alpha + \phi_{t-1} \textbf{y}_{t-1}$$
$$\phi_t = \textbf{P} \beta$$
$$\alpha \sim \mathcal{N}(0, 1000)$$
$$\beta_d \sim \mathcal{N}(0, 1000), \quad d=1,\dots,(D+1)$$
$$\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{D <- 6 \#Maximum degree of Chebyshev time polynomials \\
T <- 100 \\
P <- matrix(1, T, D+1) \\
for (d in 1:D) \{P[,d+1] <- sqrt(2)*cos(d*pi*(c(1:T)-0.5)/T)\} \\
alpha.orig <- 0; alpha.orig \\
beta.orig <- runif(D+1,-0.3,0.3); beta.orig \\
phi.orig <- tcrossprod(P, t(beta.orig)) \\
e <- rnorm(T,0,1) \\
y <- rep(0,T) \\
for (t in 2:T) \{y[t] <- alpha.orig + phi.orig[t-1]*y[t-1] + e[t]\} \\
mon.names <- c("LP", "ynew", as.parm.names(list(phi=rep(0,T-1)))) \\
parm.names <- as.parm.names(list(alpha=0, beta=rep(0,D+1), sigma=0)) \\
pos.alpha <- grep("alpha", parm.names) \\
pos.beta <- grep("beta", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
PGF <- function(Data) return(c(rnormv(D+2,0,10), rhalfcauchy(1,5))) \\
MyData <- list(D=D, P=P, PGF=PGF, T=T, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.alpha=pos.alpha, pos.beta=pos.beta, \\
\hspace*{0.27 in} pos.sigma=pos.sigma, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[Data$pos.alpha] \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnormv(alpha, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} phi <- tcrossprod(Data$P[-Data$T,], t(beta)) \\
\hspace*{0.27 in} mu <- c(alpha, alpha + phi*Data$y[-Data$T]) \\
\hspace*{0.27 in} ynew <- rnorm(1, alpha + tcrossprod(Data$P[Data$T,], t(beta))* \\
\hspace*{0.62 in} Data$y[Data$T], sigma) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,ynew,phi), \\
\hspace*{0.62 in} yhat=rnorm(length(mu), mu, sigma), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,D+2), 1)}

\section{Variable Selection, BAL} \label{bal}
This approach to variable selection is one of several forms of the Bayesian Adaptive Lasso (BAL). The lasso applies shrinkage to exchangeable scale parameters, $\gamma$, for the regression effects, $\beta$.
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2)$$
$$\mu = \textbf{X}\beta$$
$$\beta_1 \sim \mathcal{L}(0, 1000)$$
$$\beta_j \sim \mathcal{L}(0, \gamma_j), \quad j=2,\dots,J$$
$$\gamma_j \sim \mathcal{G}^{-1}(\delta, \tau), \quad \in [0,\infty]$$
$$\delta \sim \mathcal{HC}(25)$$
$$\tau \sim \mathcal{HC}(25)$$
$$\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{data(demonsnacks) \\
J <- ncol(demonsnacks) \\
y <- log(demonsnacks$Calories) \\
X <- cbind(1, as.matrix(demonsnacks[,c(1,3:10)])) \\
for (j in 2:J) \{X[,j] <- CenterScale(X[,j])\} \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=rep(0,J), gamma=rep(0,J-1), \\
\hspace*{0.27 in} log.delta=0, log.tau=0, log.sigma=0)) \\
pos.beta <- grep("beta", parm.names) \\
pos.gamma <- grep("gamma", parm.names) \\
pos.delta <- grep("delta", parm.names) \\
pos.tau <- grep("tau", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
PGF <- function(Data) return(c(rnormv(Data$J,0,10), \\
\hspace*{0.27 in} rgamma(Data$J-1,rhalfcauchy(1,5),rhalfcauchy(1,5)), rhalfcauchy(3,5))) \\
MyData <- list(J=J, PGF=PGF, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.beta=pos.beta, pos.gamma=pos.gamma, \\
\hspace*{0.27 in} pos.delta=pos.delta, pos.tau=pos.tau, pos.sigma=pos.sigma, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperhyperparameters \\
\hspace*{0.27 in} delta <- interval(parm[Data$pos.delta], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.delta] <- delta \\
\hspace*{0.27 in} parm[Data$pos.tau] <- tau <- interval(parm[Data$pos.tau], 1e-100, Inf) \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} gamma <- interval(parm[Data$pos.gamma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.gamma] <- gamma \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} \#\#\# Log(Hyperhyperprior Densities) \\
\hspace*{0.27 in} delta.prior <- dhalfcauchy(delta, 25, log=TRUE) \\
\hspace*{0.27 in} tau.prior <- dhalfcauchy(tau, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log(Hyperprior Densities) \\
\hspace*{0.27 in} gamma.prior <- sum(dinvgamma(gamma, delta, tau, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dlaplace(beta, 0, c(1000, gamma), log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, t(beta)) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + gamma.prior + delta.prior + tau.prior + \\
\hspace*{0.62 in} sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rnorm(length(mu), mu, sigma), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), rep(0,J-1), rep(1,3))}

\section{Variable Selection, Horseshoe} \label{horseshoe}
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2)$$
$$\mu = \textbf{X}\beta$$
$$\beta_1 \sim \mathcal{N}(0, 1000)$$
$$\beta_j \sim \mathcal{HS}(0, \lambda_j, \tau, \sigma), \quad j=2,\dots,J$$
$$\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{data(demonsnacks) \\
J <- ncol(demonsnacks) \\
y <- log(demonsnacks$Calories) \\
X <- cbind(1, as.matrix(demonsnacks[,c(1,3:10)])) \\
for (j in 2:J) \{X[,j] <- CenterScale(X[,j])\} \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=rep(0,J), lambda=rep(0,J-1), \\
\hspace*{0.27 in} tau=0, sigma=0)) \\
pos.beta <- grep("beta", parm.names) \\
pos.lambda <- grep("lambda", parm.names) \\
pos.tau <- grep("tau", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
PGF <- function(Data) return(c(rnormv(Data$J,0,10), runif(Data$J,0,100), \\
\hspace*{0.27 in} runif(2,0,100))) \\
MyData <- list(J=J, PGF=PGF, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.beta=pos.beta, pos.lambda=pos.lambda, \\
\hspace*{0.27 in} pos.tau=pos.tau, pos.sigma=pos.sigma, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} lambda <- interval(parm[Data$pos.lambda], 1e-100, 1000) \\
\hspace*{0.27 in} parm[Data$pos.lambda] <- lambda \\
\hspace*{0.27 in} parm[Data$pos.tau] <- tau <- interval(parm[Data$pos.tau], 1e-100, 1000) \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, 1000) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta[1], 0, 1000, log=TRUE), \\
\hspace*{0.62 in} dhs(beta[-1], lambda, tau, sigma, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, t(beta)) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rnorm(length(mu), mu, sigma), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), rep(1,J), rep(1,2))}

\section{Variable Selection, RJ} \label{rj}
This example uses the RJ (Reversible-Jump) algorithm of the \code{LaplacesDemon} function for variable selection and Bayesian Model Averaging (BMA). Other MCMC algorithms will not perform variable selection with this example, as presented. This is an example of variable selection in a linear regression. The only difference between the following example, and the example of linear regression (\ref{linear.reg}), is that RJ specifications are also included for the RJ algorithm, and that the RJ algorithm must be used.
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2)$$
$$\mu = \textbf{X}\beta$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{N <- 1000 \\
J <- 100 \#Number of predictors, including the intercept \\
X <- matrix(1,N,J) \\
for (j in 2:J) \{X[,j] <- rnorm(N,runif(1,-3,3),runif(1,0.1,1))\} \\
beta.orig <- runif(J,-3,3) \\
zero <- sample(2:J, round(J*0.9)) \#Assign most parameters to be zero \\
beta.orig[zero] <- 0 \\
e <- rnorm(N,0,0.1) \\
y <- as.vector(tcrossprod(beta.orig, X) + e) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=rep(0,J), sigma=0)) \\
pos.beta <- grep("beta", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
PGF <- function(Data) return(c(rnormv(Data$J,0,1), rhalfcauchy(1,5))) \\
MyData <- list(J=J, PGF=PGF, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.beta=pos.beta, pos.sigma=pos.sigma, y=y) \\
\#\#\# Reversible-Jump Specifications
bin.n <- J-1 \#Maximum allowable model size \\
bin.p <- 0.4 \#Most probable size: bin.p x bin.n is binomial mean and median \\
parm.p <- rep(1/J,J+1) \\
selectable=c(0, rep(1,J-1), 0) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, t(beta)) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rnorm(length(mu), mu, sigma), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- GIV(Model, MyData, PGF=TRUE)}

\section{Variable Selection, SSVS} \label{ssvs}
This example uses a modified form of the random-effects (or global adaptation) Stochastic Search Variable Selection (SSVS) algorithm presented in \citet{ohara09}, which selects variables according to practical significance rather than statistical significance. Here, SSVS is applied to linear regression, though this method is widely applicable. For $J$ variables, each regression effect $\beta_j$ is conditional on $\gamma_j$, a binary inclusion variable. Each $\beta_j$ is a discrete mixture distribution with respect to $\gamma_j = 0$ or $\gamma_j = 1$, with precision 100 or $\beta_\sigma = 0.1$, respectively. As with other representations of SSVS, these precisions may require tuning.

The binary inclusion variables are discrete parameters, and discrete parameters are not supported in all algorithms. The example below is updated with the Griddy-Gibbs sampler.

When the goal is to select the best model, each $\textbf{X}_{1:N,j}$ is retained for a future run when the posterior mean of $\gamma_j \ge 0.5$. When the goal is model-averaging, the results of this model may be used directly, which would please L. J. Savage, who said that ``models should be as big as an elephant'' \citep{draper95}.

\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2)$$
$$\mu = \textbf{X} \beta$$
$$(\beta_j | \gamma_j) \sim (1 - \gamma_j)\mathcal{N}(0, 0.01) + \gamma_j \mathcal{N}(0, \beta^2_\sigma) \quad j=1,\dots,J$$
$$\beta_\sigma \sim \mathcal{HC}(25)$$
$$\gamma_j \sim \mathcal{BERN}(1/(J-1)), \quad j=1,\dots,(J-1)$$
$$\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{data(demonsnacks) \\
J <- ncol(demonsnacks) \\
y <- log(demonsnacks$Calories) \\
X <- cbind(1, as.matrix(demonsnacks[,c(1,3:10)])) \\
for (j in 2:J) \{X[,j] <- CenterScale(X[,j])\} \\
mon.names <- c("LP", "min.beta.sigma") \\
parm.names <- as.parm.names(list(beta=rep(0,J), gamma=rep(0,J-1), \\
\hspace*{0.27 in} b.sd=0, sigma=0)) \\
pos.beta <- grep("beta", parm.names) \\
pos.gamma <- grep("gamma", parm.names) \\
pos.b.sd <- grep("b.sd", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
PGF <- function(Data) return(c(rnorm(Data$J,0,1), rcat(Data$J-1, p=rep(0.5,2)), \\
\hspace*{0.27 in} rhalfcauchy(2,5))) \\
MyData <- list(J=J, PGF=PGF, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.beta=pos.beta, pos.gamma=pos.gamma, \\
\hspace*{0.27 in} pos.b.sd=pos.b.sd, pos.sigma=pos.sigma, y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} beta.sigma <- interval(parm[Data$pos.b.sd], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.b.sd] <- beta.sigma \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} gamma <- parm[Data$pos.gamma] \\
\hspace*{0.27 in} beta.sigma <- rep(beta.sigma, Data$J-1) \\
\hspace*{0.27 in} beta.sigma[gamma == 0] <- 0.1 \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} \#\#\# Log(Hyperprior Densities) \\
\hspace*{0.27 in} beta.sigma.prior <- sum(dhalfcauchy(beta.sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, beta.sigma, log=TRUE)) \\
\hspace*{0.27 in} gamma.prior <- sum(dbern(gamma, 1/(Data$J-1), log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, t(beta*c(1,gamma))) \\
\hspace*{0.27 in} LL <- sum(dnorm(y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + beta.sigma.prior + gamma.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, min(beta.sigma)), \\
\hspace*{0.62 in} yhat=rnorm(length(mu), mu, sigma), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), rep(1,J-1), rep(1,2))}

\section{Vector Autoregression, VAR(1)} \label{var1}
\subsection{Form}
$$\textbf{Y}_{t,j} \sim \mathcal{N}(\mu_{t,j}, \sigma^2_j), \quad t=1,\dots,T, \quad j=1,\dots,J$$
$$\mu_{t,j} = \alpha_j + \Phi_{1:J,j} \textbf{Y}_{t-1,j}$$
$$\textbf{y}^{new}_j = \alpha_j + \Phi_{1:J,j} \textbf{Y}_{T,j}$$
$$\alpha_j \sim \mathcal{N}(0, 1000)$$
$$\sigma_j \sim \mathcal{HC}(25)$$
$$\Phi_{i,k} \sim \mathcal{N}(0, 1000), \quad i=1,\dots,J, \quad k=1,\dots,J$$ 
\subsection{Data}
\code{T <- 100 \\
J <- 3 \\
Y <- matrix(0,T,J) \\
for (j in 1:J) \{for (t in 2:T) \{ \\
\hspace*{0.27 in} Y[t,j] <- Y[t-1,j] + rnorm(1,0,0.1)\}\} \\
mon.names <- c("LP", as.parm.names(list(ynew=rep(0,J)))) \\
parm.names <- as.parm.names(list(alpha=rep(0,J), Phi=matrix(0,J,J), \\
\hspace*{0.27 in} sigma=rep(0,J))) \\
pos.alpha <- grep("alpha", parm.names) \\
pos.Phi <- grep("Phi", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
PGF <- function(Data) return(c(rnormv(Data$J,0,1), \\
\hspace*{0.27 in} rnormv(Data$J*Data$J,0,1), rhalfcauchy(Data$J,5))) \\
MyData <- list(J=J, PGF=PGF, T=T, Y=Y, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.alpha=pos.alpha, pos.Phi=pos.Phi, \\
\hspace*{0.27 in} pos.sigma=pos.sigma) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[Data$pos.alpha] \\
\hspace*{0.27 in} Phi <- matrix(parm[Data$pos.phi], Data$J, Data$J) \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- sum(dnormv(alpha, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} Phi.prior <- sum(dnormv(Phi, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- matrix(alpha,Data$T,Data$J,byrow=TRUE) \\
\hspace*{0.62 in} mu[-1,] <- mu[-1,] + tcrossprod(Data$Y[-Data$T,], Phi) \\
\hspace*{0.27 in} Sigma <- matrix(sigma, Data$T, Data$J, byrow=TRUE) \\
\hspace*{0.27 in} ynew <- rnorm(Data$J, alpha + as.vector(crossprod(Phi, Data$Y[Data$T,])), \\
\hspace*{0.62 in} sigma) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$Y, mu, Sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + Phi.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,ynew), \\
\hspace*{0.62 in} yhat=rnorm(prod(dim(mu)), mu, Sigma), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(colMeans(Y), rep(0,J*J), rep(1,J))}

\section{Weighted Regression} \label{weighted.reg}
It is easy enough to apply record-level weights to the likelihood. Here, weights are applied to the linear regression example in section \ref{linear.reg}.
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2)$$
$$\mu = \textbf{X}\beta$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{data(demonsnacks) \\
N <- nrow(demonsnacks) \\
J <- ncol(demonsnacks) \\
y <- log(demonsnacks$Calories) \\
X <- cbind(1, as.matrix(demonsnacks[,c(1,3:10)])) \\
for (j in 2:J) \{X[,j] <- CenterScale(X[,j])\} \\
w <- c(rep(1,5), 0.2, 1, 0.01, rep(1,31)) \\
w <- w * (sum(w) / N) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=rep(0,J), sigma=0)) \\
pos.beta <- grep("beta", parm.names) \\
pos.sigma <- grep("sigma", parm.names) \\
PGF <- function(Data) return(c(rnormv(Data$J,0,1000), rhalfcauchy(1,5))) \\
MyData <- list(J=J, PGF=PGF, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, pos.beta=pos.beta, pos.sigma=pos.sigma, w=w, \\
\hspace*{0.27 in} y=y) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[Data$pos.beta] \\
\hspace*{0.27 in} sigma <- interval(parm[Data$pos.sigma], 1e-100, Inf) \\
\hspace*{0.27 in} parm[Data$pos.sigma] <- sigma \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, t(beta)) \\
\hspace*{0.27 in} LL <- sum(w * dnorm(Data$y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rnorm(length(mu), mu, sigma), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), 1)}

\section{Zero-Inflated Poisson (ZIP)} \label{zip}
\subsection{Form}
$$\textbf{y} \sim \mathcal{P}(\Lambda_{1:N,2})$$
$$\textbf{z} \sim \mathcal{BERN}(\Lambda_{1:N,1})$$
\[\textbf{z}_i = \left\{ 
\begin{array}{l l}
  1 & \quad \mbox{if $\textbf{y}_i = 0$}\\
  0 \\ \end{array} \right. \]
\[\Lambda_{i,2} = \left\{ 
\begin{array}{l l}
  0 & \quad \mbox{if $\Lambda_{i,1} \ge 0.5$}\\
  \Lambda_{i,2} \\ \end{array} \right. \]
$$\Lambda_{1:N,1} = \frac{1}{1 + \exp(-\textbf{X}_1 \alpha)}$$
$$\Lambda_{1:N,2} = \exp(\textbf{X}_2 \beta)$$
$$\alpha_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J_1$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J_2$$
\subsection{Data}
\code{N <- 1000 \\
J1 <- 4 \\
J2 <- 3 \\
X1 <- matrix(runif(N*J1,-2,2),N,J1); X1[,1] <- 1 \\
X2 <- matrix(runif(N*J2,-2,2),N,J2); X2[,1] <- 1 \\
alpha <- runif(J1,-1,1) \\
beta <- runif(J2,-1,1) \\
p <- invlogit(tcrossprod(X1, t(alpha)) + rnorm(N,0,0.1)) \\
mu <- round(exp(tcrossprod(X2, t(beta)) + rnorm(N,0,0.1))) \\
y <- ifelse(p > 0.5, 0, mu) \\
z <- ifelse(y == 0, 1, 0) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(alpha=rep(0,J1), beta=rep(0,J2))) \\
pos.alpha <- grep("alpha", parm.names) \\
pos.beta <- grep("beta", parm.names) \\
PGF <- function(Data) return(c(rnormv(Data$J1,0,5), rnormv(Data$J2,0,5))) \\
MyData <- list(J1=J1, J2=J2, N=N, PGF=PGF, X1=X1, X2=X2, \\
\hspace*{0.27 in} mon.names=mon.names, parm.names=parm.names, pos.alpha=pos.alpha, \\
\hspace*{0.27 in} pos.beta=pos.beta, y=y, z=z) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} parm[Data$pos.alpha] <- alpha <- interval(parm[Data$pos.alpha], -5, 5) \\
\hspace*{0.27 in} parm[Data$pos.beta] <- beta <- interval(parm[Data$pos.beta], -5, 5) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- sum(dnormv(alpha, 0, 5, log=TRUE)) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 5, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} Lambda <- matrix(NA, Data$N, 2) \\
\hspace*{0.27 in} Lambda[,1] <- invlogit(tcrossprod(Data$X1, t(alpha))) \\
\hspace*{0.27 in} Lambda[,2] <- exp(tcrossprod(Data$X2, t(beta))) + 1e-100 \\
\hspace*{0.27 in} Lambda[which(Lambda[,1] >= 0.5),2] <- 0 \\
\hspace*{0.27 in} LL <- sum(dbern(Data$z, Lambda[,1], log=TRUE), \\
\hspace*{0.62 in} dpois(Data$y, Lambda[,2], log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=rpois(nrow(Lambda), Lambda[,2]), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}
\subsection{Initial Values}
\code{Initial.Values <- GIV(Model, MyData, n=10000)}

\bibliography{References}

\end{document}
